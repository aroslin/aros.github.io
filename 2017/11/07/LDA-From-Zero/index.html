<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">






  <meta name="keywords" content="Machine Learning,Statistics," />





  <link rel="alternate" href="/atom.xml" title="Aroslin" type="application/atom+xml" />






<meta name="description" content="去年初学LDA，看完了Rickjin老师的《LDA数学八卦》，觉得不是很懂，查阅了很多资料之后才对LDA有了更深入地认识。一直想加入自己的理解后更简单的讲述这个模型，今日补上。 注：文章的行文思路与大多数公式参考了Rickjin《LDA数学八卦》。有兴趣更深入了解的同学可以在看完本篇后继续阅读。  LDA(Latent Dirichlet allocation)是主题模型的一种，最早是由Blei">
<meta name="keywords" content="Machine Learning,Statistics">
<meta property="og:type" content="article">
<meta property="og:title" content="从零开始学LDA（Latent Dirichlet allocation）">
<meta property="og:url" content="http://yoursite.com/2017/11/07/LDA-From-Zero/index.html">
<meta property="og:site_name" content="Aroslin">
<meta property="og:description" content="去年初学LDA，看完了Rickjin老师的《LDA数学八卦》，觉得不是很懂，查阅了很多资料之后才对LDA有了更深入地认识。一直想加入自己的理解后更简单的讲述这个模型，今日补上。 注：文章的行文思路与大多数公式参考了Rickjin《LDA数学八卦》。有兴趣更深入了解的同学可以在看完本篇后继续阅读。  LDA(Latent Dirichlet allocation)是主题模型的一种，最早是由Blei">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2017/11/07/LDA-From-Zero/1.jpg">
<meta property="og:image" content="http://yoursite.com/2017/11/07/LDA-From-Zero/2.png">
<meta property="og:image" content="http://yoursite.com/2017/11/07/LDA-From-Zero/3.png">
<meta property="og:image" content="http://yoursite.com/2017/11/07/LDA-From-Zero/4.png">
<meta property="og:image" content="http://yoursite.com/2017/11/07/LDA-From-Zero/5.jpg">
<meta property="og:image" content="http://yoursite.com/2017/11/07/LDA-From-Zero/6.png">
<meta property="og:image" content="http://yoursite.com/2017/11/07/LDA-From-Zero/7.png">
<meta property="og:image" content="http://yoursite.com/2017/11/07/LDA-From-Zero/8.png">
<meta property="og:image" content="http://yoursite.com/2017/11/07/LDA-From-Zero/10.png">
<meta property="og:image" content="http://yoursite.com/2017/11/07/LDA-From-Zero/9.png">
<meta property="og:image" content="http://yoursite.com/2017/11/07/LDA-From-Zero/11.png">
<meta property="og:image" content="http://yoursite.com/2017/11/07/LDA-From-Zero/12.png">
<meta property="og:updated_time" content="2017-12-04T13:28:30.949Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="从零开始学LDA（Latent Dirichlet allocation）">
<meta name="twitter:description" content="去年初学LDA，看完了Rickjin老师的《LDA数学八卦》，觉得不是很懂，查阅了很多资料之后才对LDA有了更深入地认识。一直想加入自己的理解后更简单的讲述这个模型，今日补上。 注：文章的行文思路与大多数公式参考了Rickjin《LDA数学八卦》。有兴趣更深入了解的同学可以在看完本篇后继续阅读。  LDA(Latent Dirichlet allocation)是主题模型的一种，最早是由Blei">
<meta name="twitter:image" content="http://yoursite.com/2017/11/07/LDA-From-Zero/1.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/11/07/LDA-From-Zero/"/>





  <title>从零开始学LDA（Latent Dirichlet allocation） | Aroslin</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Aroslin</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Life is always hard.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-something">
          <a href="/somthing/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-transgender"></i> <br />
            
            有料
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/07/LDA-From-Zero/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Aros Lin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aroslin">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">从零开始学LDA（Latent Dirichlet allocation）</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-07T19:17:10+08:00">
                2017-11-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习模型/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习模型</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          
		  
          
          
             <span id="/2017/11/07/LDA-From-Zero/" class="leancloud_visitors" data-flag-title="从零开始学LDA（Latent Dirichlet allocation）">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">热度&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
				 <span>℃</span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p>去年初学LDA，看完了Rickjin老师的《LDA数学八卦》，觉得不是很懂，查阅了很多资料之后才对LDA有了更深入地认识。一直想加入自己的理解后更简单的讲述这个模型，今日补上。</p>
<p><font color="red"><strong>注：文章的行文思路与大多数公式参考了Rickjin《LDA数学八卦》。有兴趣更深入了解的同学可以在看完本篇后继续阅读。</strong></font></p>
</blockquote>
<p>LDA(Latent Dirichlet allocation)是<a href="https://zh.wikipedia.org/wiki/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B" target="_blank" rel="external">主题模型</a>的一种，最早是由Blei D M<sup>[1]</sup>等人在2003年提出的，常被用于文档的主题识别。时至今日，在大大小小的会议上仍能看到不少对LDA（或其衍生、或类似的主题模型）提出改进的文章。其自然的思路，优雅的求解和拓展性强的模型结构，实在是非常适合作为对无监督学习一个入门模型来学习和改进。</p>
<p>本文主要介绍LDA背后的相关数学知识和模型的构建与求解思路，力求只有少量统计知识的初学者也能看懂。文中略去了大量的公式推导细节，如果希望深入的了解，请根据需要查找相关的资料。</p>
<a id="more"></a>
<h2 id="从扔硬币说起"><a class="header-anchor" href="#从扔硬币说起">¶</a>从扔硬币说起</h2>
<p>介绍模型之前，我们先来简单回顾一些知识点。本章主要讲解二项分布，贝叶斯理论，beta分布，multi分布和Dirichlet分布。如果你熟悉这些知识点，可以直接跳过这一章。</p>
<h3 id="二项分布"><a class="header-anchor" href="#二项分布">¶</a>二项分布</h3>
<blockquote>
<p>二项分布（英语：Binomial distribution）是n个独立的是/非试验中成功的次数的离散概率分布，其中每次试验的成功概率为p。这样的单次成功/失败试验又称为伯努利试验。实际上，当n = 1时，二项分布就是伯努利分布。二项分布是显著性差异的二项试验的基础。</p>
<p>摘自<a href="https://zh.wikipedia.org/wiki/%E4%BA%8C%E9%A0%85%E5%88%86%E4%BD%88" target="_blank" rel="external">二项分布维基百科</a></p>
</blockquote>
<p>扔一枚硬币，一共扔N次，其中k次都是正面朝上的概率是多少？
这个概率服从二项分布(Bernoulli distribution)
$$P(k|N,p)=C^k_N\cdot p^k\cdot (1-p)^{N-k}$$
其中p是硬币扔一次正面朝上的概率。真实场景中，我们面对的问题往往是，我想知道这个公式中的p是多少。一个我们熟悉且自然的思路是，把这个硬币扔N次，得到的结果中k次朝上的那么可以估计</p>
<p>$$p=\frac{k}{N}$$</p>
<p>如果你学过概率论，那么你就知道这是对二项分布中参数p的一个<a href="https://zh.wikipedia.org/zh-hans/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1" target="_blank" rel="external">极大似然估计</a>。</p>
<h3 id="贝叶斯估计"><a class="header-anchor" href="#贝叶斯估计">¶</a>贝叶斯估计</h3>
<p>当我们像上面去估计p值的时候，其实已经默认了一个假设，就是p是一个定值，它在每次实验中都是相同的。p是一个客观存的量，只是我们不知道它是多少。这样的想法很容易理解，但有时候会带来一些问题。当我们扔了10次硬币，10次都朝上的时候（尽管概率很小，但这一现象也会发生），用上面的方法估计，p=1。这一结论和我们的常识不符，当全部出现正面朝上这种小概率事件时，我么可能会倾向认为p值比0.5大，但没有大到等于1。</p>
<p>这时候，有些人就想，或许p并不是一个定值，而是服从某一种概率分布，对p的一个较好的估计是这个概率分布的<a href="https://zh.wikipedia.org/zh-hans/%E6%9C%9F%E6%9C%9B%E5%80%BC" target="_blank" rel="external">期望</a>，那么即使发生了每次都正面朝上的情况，只要这个概率分布不是始终等于1，就不会出现违背常识的情况了。</p>
<p>怎么理解？这就好比说，你扔出的硬币在落下的一瞬间之前，在你无法感知的空间中散落成无数的硬币，这些硬币的密度不尽相同，也就是落下时的p是不同的，但这些p是的有约束的，都是服从某一概率分布。在降落的一瞬间，有且仅有一枚硬币投射到你感知到的硬币上，也就是这一次抛硬币对应的p。由于无数硬币的p值并不是都等于1，所以即使出现扔硬币多次全部正面朝上，也可以理解成这些硬币中p值较大的硬币比较多，而不会是所有硬币的p都等于1。</p>
<img src="/2017/11/07/LDA-From-Zero/1.jpg" title="Figure 1. 无限硬币">
<p>如果你觉得上面这略显中二的说法也不太好理解的话😑，我们就直接看知识点好了。根据伯努利试验中p值是否固定，划分出了两个学派——频率学派和贝叶斯学派。这两个学派的思路差异主要体现在：</p>
<blockquote>
<ul>
<li>频率派把需要推断的参数θ看做是固定的未知常数，即概率虽然是未知的，但是确定的一个值，同时，样本X是随机的，所以频率派重点研究样本空间，大部分的概率计算都是针对样本X的分布；</li>
<li>贝叶斯派的观点则截然相反，他们认为参数是随机变量，而样本X 是固定的，由于样本是固定的，所以他们重点研究的是参数的分布。</li>
</ul>
</blockquote>
<p>在贝叶斯学派看来，所有的参数都不是固定值，而是服从一个概率分布，而对这个概率分布的估计有一个经典的模式：</p>
<p>先验分布𝜋(𝜃)+样本信息Χ⇒后验分布𝜋(𝜃|𝑥)</p>
<p>上述思考模式意味着，新观察到的样本信息将修正人们以前对事物的认知。换言之，在得到新的样本信息之前，人们对的认知是先验分布$𝜋(𝜃)$，在得到新的样本信息Χ后，人们对的认知为$𝜋(𝜃|𝑥)$。而这里求解$𝜋(𝜃|𝑥)$最重要依据就是贝叶斯公式（条件概率公式）
$$P(𝜃,𝑋)=P(𝜃│𝑋)∙P(𝑋)=P(𝑋|𝜃)∙P(𝜃)$$</p>
<p>举个例子来说明两个学派分析的思路差异好了。加入我们扔一枚硬币，连续扔10次，有6次出现了正面朝上，那么扔一次正面朝上的概率是多少？</p>
<p>按照频率学派的观点，
$$p=\frac{6}{10}=0.6$$</p>
<p>按照贝叶斯学派，首先p有一个先验分布，也就是你对p的一个经验估计，为了计算简单和表达的方便，咱们这里取一个离散分布,令$𝜃=p=0.4,0.5,0.6$，且</p>
<blockquote>
<p>$P(𝜃=0.4)=0.1$</p>
<p>$P(𝜃=0.5)=0.8$</p>
<p>$P(𝜃=0.6)=0.1$</p>
</blockquote>
<p>先验分布的期望
$$E(𝜃)=0.4\cdot0.1+0.5\cdot0.8+0.6\cdot0.1=0.5$$</p>
<p>根据样本信息，10次中有6次朝上，由贝叶斯公式变形可以得到，在该样本条件下，$𝜃=6$的概率为</p>
<p>$$𝑝(𝜃=0.6│𝑋=6)=\frac{𝑝(𝑋=6|𝜃=0.6)\cdot𝑝(𝜃=0.6)}{𝑝(𝑋=6)} $$</p>
<p>其中：</p>
<ul>
<li>$𝜃$就是抛一次硬币是正面的概率，X是正面的次数，在本例中就是6；</li>
<li>$𝑝(𝜃=0.6│𝑋=6)$就是在6次正面的情况下，$𝜃=0.6$的概率，即后验概率；</li>
<li>$𝑝(𝑋=6|𝜃=0.6)$是𝜃=0.6时可以抛出6次正面的概率，即样本信息，这里求法按二项分布来求；</li>
<li>$𝑝(𝜃=0.6)$就是𝜃的先验分布，此例子中$𝑝(𝜃=0.6)=0.1$；</li>
<li>$𝑝(𝑋)$称为Normalizer，或者叫做marginal probability，是一个定值，$𝜃$分布如果是离散的，就是$𝑝(𝑋|𝜃)\cdot 𝑝(𝜃)$在所有𝜃情况下的和；如果$𝜃$是连续的，$𝑝(𝑋)=\int𝑝(𝑋|𝜃)\cdot 𝑝(𝜃)𝑑𝜃$。</li>
</ul>
<p>我们就可以分别求出在样本信息为10次实验6次正面的情况下，后验分布</p>
<blockquote>
<p>$P(𝜃=0.4|X=6)=0.056$</p>
<p>$P(𝜃=0.5|X=6)=0.819$</p>
<p>$P(𝜃=0.6|X=6)=0.125$</p>
</blockquote>
<p>后验分布的期望
$$E(𝜃|X=6)=0.4\cdot0.056+0.5\cdot0.819+0.6\cdot0.125=0.5069$$</p>
<p>可以看到，后验分布的概率变得比0.5大了，而且并没有大很多。由于这里先验分布取的很特殊，哪怕出现无穷多次都是正面朝上的情况，后验分布的期望也只会逼近0.6。</p>
<h3 id="beta分布与共轭"><a class="header-anchor" href="#beta分布与共轭">¶</a>Beta分布与共轭</h3>
<p>上面的例子中，我们选取了一个离散分布作为先验分布，可以看出这个离散分布限定了p只能取三个值，得到的后验分布期望所在的区间也十分有限，选取这个分布并不太合适。数学家们给出了一个很漂亮的分布——<a href="https://zh.wikipedia.org/zh-hans/%CE%92%E5%88%86%E5%B8%83" target="_blank" rel="external">Beta分布</a>，来作为二项分布的先验分布。为什么选取Beta分布呢？主要是因为两点：</p>
<blockquote>
<ul>
<li>Beta分布形态多变，可以满足多种情况下的分布。</li>
<li>Beta分布做先验分布，样本信息服从二项分布时，两种分布符合良好的特性可以简化计算。</li>
</ul>
</blockquote>
<p>为了更好的理解Beat分布与二项分布的关系，我们先来看一下Beta分布的表达式。
$$p(\theta)=Beta(\theta|\alpha,\beta)=\frac{\theta^{\alpha-1}\cdot (1-\theta)^{\beta-1}}{B(\alpha,\beta)},$$
$$B(\alpha,\beta)=\int^1_0 t^{\alpha-1} (1-t)^{\beta-1}dt$$</p>
<!---
其中$B(\alpha,\beta)$也可以写成

$$B(\alpha,\beta)=\frac{\Gamma (\alpha)\cdot \Gamma (\beta)}{\Gamma (\alpha+\beta)},$$
$$\Gamma (n)=\int^\infty_0t^{n-1}e^{-t}dt=(n-1)!$$

两种写法之间的等价性就不在这里证明了。根据下面一种写法我们知道

$$B(\alpha,\beta)=\frac{(\alpha-1)!\cdot (\beta-1)!}{(\alpha+\beta-2)!}$$
-->
<p>公式中$\alpha$和$\beta$是Beta分布的两个超参数，通过调整$\alpha$和$\beta$的大小可以调整Beta分布的形态，如下图所示。</p>
<img src="/2017/11/07/LDA-From-Zero/2.png" title="Figure 2. Beta分布">
<p>由图像我们不难看出，改变$\alpha$和$\beta$可以得到丰富的Beta分布的形式，得到不同的期望与分布的均匀程度，可以满足很多的分析场景。这是我们提到的选取Beta分布作为先验分布的第一点原因。</p>
<p>那么，它与二项分布的关系呢？我么不妨来求一下先验分布是Beta分布，样本服从二项分布的情况下，后验分布是什么样子的。如果先验分布是Beta分布</p>
<p>$$p(\theta)=Beta(\theta|\alpha,\beta)=\frac{\theta^{\alpha-1}\cdot (1-\theta)^{\beta-1}}{B(\alpha,\beta)}$$</p>
<p>样本服从二项分布</p>
<p>$$P(X|\theta)=C^k_N\cdot \theta^k\cdot (1-\theta)^{N-k}$$</p>
<p>那么根据贝叶斯公式，后验分布满足</p>
<span>$$\begin{align} 
p(\theta|X) &amp; =\frac{p(X|\theta)\cdot p(\theta)}{p(X)}\\\\
&amp; =\frac{p(X|\theta)\cdot p(\theta)}{\int p(X|\theta)\cdot p(\theta)d\theta}\\\\
&amp; =\frac{(C^k_N\cdot \theta^k\cdot (1-\theta)^{N-k})\cdot(\frac{\theta^{\alpha-1}\cdot (1-\theta)^{\beta-1}}{B(\alpha,\beta)})}{\int (C^k_N\cdot \theta^k\cdot (1-\theta)^{N-k})\cdot (\frac{\theta^{\alpha-1}\cdot (1-\theta)^{\beta-1}}{B(\alpha,\beta)})d\theta}\\\\
&amp; =\frac{\frac{C^k_N}{B(\alpha,\beta)}\cdot \theta^{\alpha+k-1}\cdot (1-\theta)^{\beta+N-k-1}}{\frac{C^k_N}{B(\alpha,\beta)}\cdot \int \theta^{\alpha+k-1}\cdot (1-\theta)^{\beta+N-k-1}d\theta}\\\\
&amp; =\frac {\theta^{\alpha+k-1}\cdot (1-\theta)^{\beta+N-k-1}}{B(\theta|\alpha+k,\beta+(N-k))}\\\\
&amp;= Beta(\theta|\alpha+k,\beta+(N-k))
\end{align}$$</span><!-- Has MathJax -->
<p>得到的仍然是一个Beta分布！也就是说，在先验分布是Beta分布的情况下，如果样本服从二项分布，那么后验分布也是Beta分布。在贝叶斯估计中，将这种根据样本求得的后验分布与先验分布的分布形式一致时，样本服从的分布与先验分布共称为<a href="https://zh.wikipedia.org/zh-hans/%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C" target="_blank" rel="external"><strong>共轭分布</strong></a>。将上面的式子我们简化的记为</p>
<p>$$Beta(\theta|\alpha,\beta)+BinomCount(k,N-k)=Beta(\theta|\alpha+k,\beta+N-k)$$</p>
<p>共轭是一个非常好的特性。在之前的先验分布是离散分布的例子中，我们需要把每一步都求解出来才能知道后验分布的期望是多少，但是如果先验分布是Beta分布，我们只需要会求Beta分布的期望，就能很快的得到后验分布的期望了。</p>
<p>事实上，$Beta(\theta|\alpha,\beta)$的期望非常好求（这里就不推导公式了）</p>
<p>$$E(Beta(\theta|\alpha,\beta))=\frac{\alpha}{\alpha+\beta}$$</p>
<p>等下，这个结果是不是有点眼熟？如果我们另$\alpha=k,\beta=N-k$，在扔硬币的试验中，Beta的期望，也就是对p值的估计就变成了</p>
<p>$$E(Beta(\theta|k,N-k))=\frac{k}{N}$$</p>
<p>和频率学派对p值得估计统一起来了！从这里我们可以看出超参数$\alpha,\beta$的物理意义实际上就是扔硬币实验中正面朝上负面朝上的次数。根据我们的经验，我们认为p值应该是0.5，那么我们可以设先验Beta分布中$\alpha=\beta=1$，十次实验全部正面朝上，后验Beta分布的期望，也就是对p值的估计$E(\theta)=11/12$，一个接近1但不等于1的数。甚至，如果你对p值等于0.5非常有自信，你可以先验Beta分布中$\alpha=\beta=100$，那么十次实验全部正面朝上的话，$E(\theta)=101/102$，对p值的估计仍在0.5左右。记住这一点，对理解共轭的特性和Beta分布的含义有较大的帮助。</p>
<blockquote>
<p>Beta分布是分布的分布，它与二项分布共轭。在Beta分布中，超参数$\alpha,\beta$物理含义是伯努利试验中两种结果发生的次数，它们比值决定了Beta分布的期望，他们的大小决定了样本对后验分布的影响程度。</p>
</blockquote>
<h3 id="多项分布与dirichlet共轭"><a class="header-anchor" href="#多项分布与dirichlet共轭">¶</a>多项分布与Dirichlet共轭</h3>
<p>好的，下面我们来把二项分布推广吧。想象硬币就是一个“二面体”，它的每一面朝上的事件都会发生，而且发生的概率并不相同，对二项分布</p>
<p>$$P(k|N,p)=C^k_N\cdot p^k\cdot (1-p)^{N-k}$$
令$x_1=k$，$x_2=N-k$，$p_1=p$，$p_2=1-p$，$\vec{x}=(x_1.x_2)$，$\vec{p}=(p_1,p_2)$那么上式可以改写为</p>
<p>$$p(\vec{x}|N,\vec{p})=\frac{N!}{x_1!\cdot x_2!}\cdot p^{x_1}_1\cdot p^{x_2}_2$$</p>
<p>这里式子的形式已经很清楚了。如果我们抛一个k面的凸多面体，一共抛N次，每个面朝上的次数为$\vec{x}$，每个面朝上的概率为$\vec{p}$,那么对应的概率就是<a href="https://en.wikipedia.org/wiki/Multinomial_distribution" target="_blank" rel="external">多项分布</a></p>
<p>$$p(\vec{x}|N,\vec{p})=\frac{N!}{\prod_i^ kx_i}\cdot \coprod_{i}^ {k}p ^{x_i}_i$$</p>
<img src="/2017/11/07/LDA-From-Zero/3.png" title="Figure 3. 多面体">
<p>与多项分布共轭的分布就是<a href="https://zh.wikipedia.org/zh-hans/%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83" target="_blank" rel="external">Dirchlet分布</a></p>
<p>$$p(\vec{x})=Dir(\vec{x}|\vec{\alpha})=\frac{1}{\Delta (\vec{\alpha})}\coprod_{i}^ {k}x_i ^{\alpha_i-1}$$
$$\Delta (\vec{\alpha})=\int\coprod_{i} ^ {k}x ^{\alpha-1}_id \vec {x}$$</p>
<p>Dirchlet分布与多项分布的共轭也满足我们之前说的漂亮的性质</p>
<p>$$Dir(\vec{x}|\vec{\alpha})+MultCount(\vec{m})=Dir(\vec{x}|\vec{\alpha}+\vec{m})$$</p>
<p>而Dirchlet分布的期望等于</p>
<p>$$E(\vec{p})=(\frac{\alpha_1}{\sum_{k}^{i}\alpha_i},\frac{\alpha_2}{\sum_{k} ^ {i}\alpha_i}, \ldots ,\frac{\alpha_N}{\sum_{k} ^{i}\alpha_i})$$</p>
<p>多项分布与Dirihlet分布是整个LDA模型构建的基础。了解它们的特性对于理解和改进LDA模型会有很大的帮助。至于LDA模型到底是什么，我们下节继续。</p>
<h2 id="latent-dirichlet-allocation"><a class="header-anchor" href="#latent-dirichlet-allocation">¶</a>Latent Dirichlet allocation</h2>
<p>OK，正餐来了。那么什么是LDA呢？让我们从Unigram Model开始说起。</p>
<h3 id="unigram-model"><a class="header-anchor" href="#unigram-model">¶</a>Unigram Model</h3>
<p>一篇文档是由许多词语构成的，事实上，抛开语法结构不谈，如果一篇文章是由一些没有顺序的词语构成，我们仍能从一定程度理解文档在探讨的主题。就好像那句有趣的话一样</p>
<blockquote>
<p>研表究明，汉字序顺并不定一影阅响读</p>
</blockquote>
<p>那么我们不妨将文档看做是一些词的集合，那么最简单的生成文档的方式就是，先选定这篇文档的词语个数N，然后选N个词出来。这一过程就好像是我们有一个巨大的多面体，它有字典里的词那么多面，每一面对应一个词。当我们要创作一篇有N个词的文档时，我们就把这个多面体扔N次，然后文档就生成了。</p>
<p>是的，上面这个简单的文档生成模型就叫做Unigram Model。它所对应的一篇文档的生成概率就是一个多项分布，根据词语的数量和总词数就可以估计每个词出现的概率。</p>
<img src="/2017/11/07/LDA-From-Zero/4.png" title="Figure 4. Unigram Model">
<p>Unigram model中文档生成的过程如下</p>
<blockquote>
<hr>
<p><strong>Unigram model文档生成过程</strong></p>
<hr>
<p>假设：只有一个多面体，这个多面体有N面，每个面代表一个词；每个面的朝上的概率不尽相同。</p>
<hr>
<p>过程：</p>
<p>1.选定文档的词语数量M；</p>
<p>2.抛掷多面体，记录朝上的面对应的词；</p>
<p>3.重复步骤共M次，直到所有词都生成。</p>
<hr>
</blockquote>
<p>由生成过程我们知道，当一篇文档每一个词确定下来之后得到$\vec{w}=(w_1,\cdots,w_V)$，则该文档的生成概率为</p>
<p>$$p(\vec{w})=p(w_1)\cdots p(w_V)$$</p>
<p>如果语料库中有M篇文档，那么语料库$W=(\vec{w}_1)\cdots \vec{w}_M$生成的概率为</p>
<p>$$p(W)=p(\vec{w}_1)\cdots p(\vec{w}_M)$$</p>
<p>假如语料库中的无重复的词语数量是N，每一个词在所有文档中出现的次数为$n_i$，那么$\vec{n}=(n_1,\cdots,n_N)$正好是一个多项分布，整个语料库生成的概率为
$$p(W)=p(\vec{n})=Mult(\vec{n}|\vec{p},N)=\coprod _{i=1} ^{N} p_i ^{n_i}$$</p>
<p>这里，对每一个参数$p_k$的一个估计值就是</p>
<p>$$\hat{p_i}=\frac{n_i}{N}$$</p>
<p>当然，别忘记了贝叶斯估计，如果给这个多面体每个面朝上的概率一个先验分布$Dir(\vec{p}| \vec{\alpha})$，那么就可以轻松地根据样本信息 $\vec{n}$ 来估计后验分布$Dir(\vec{p}| \vec{\alpha}+\vec{n})$了。它的概率图模型如下图所示。</p>
<img src="/2017/11/07/LDA-From-Zero/5.jpg" title="Figure 5. Unigram Model概率图模型">
<blockquote>
<p>这里稍微提一下概率图模型。我没系统学过，面试的时候老是被问，但是总也画不标准。后来看了<a href="http://www.cnblogs.com/arachis/p/LDA.html" target="_blank" rel="external">这一篇</a>，大概了解了一些基本规则。</p>
<p>这种记图的方式叫做plate notation，我们会用到符号包括</p>
<ul>
<li>单圆圈表示隐变量</li>
<li>双圆圈表示观察到的变量</li>
<li>箭头表示采样</li>
<li>相同参数采样得到的独立同分布的变量放在一个方框里，并在方框中标注变量数量</li>
</ul>
</blockquote>
<p>这里，我们不难根据之前Normalizer的定义，得出先验分布是超参数为$\vec{alpha}$的Dirichlet分布的Unigram Model，文档生成概率如下（这个公式比较重要，我们后面会用到，记为<span id="jump">公式3.1</span>。）</p>
<span>$$\begin{align} 
p(\vec{W}|\vec{\alpha})&amp; =\int p(\vec{W}|\vec{p}) \cdot p(\vec{p}|\vec{\alpha})d\vec{p}\\\\
&amp;=\int \prod_{i=1}^{N}p_i^{n_i}\cdot Dir(\vec{p}|\vec{\alpha}))d\vec{p} \\\\
&amp;=\int \prod_{i=1}^{N}p_i^{n_i} \cdot \frac{1}{\Delta (\vec{\alpha})} \prod_{i=1}^{N}p_i^{\alpha_i-1}d\vec{p} \\\\
&amp;=\frac{1}{\Delta (\vec{\alpha})}\int \prod_{i=1}^{N}p_i^{n_i+\alpha_i-1}\vec{p}\\\\
&amp;=\frac{\Delta(\vec{n}+\vec{\alpha})}{\Delta (\vec{\alpha})}
\end{align}$$</span><!-- Has MathJax -->
<h3 id="主题模型"><a class="header-anchor" href="#主题模型">¶</a>主题模型</h3>
<p>Unigram Model够简单，但不太符合我们日常的写作习惯。写作的人在写文档时，往往会先挑选一些主题。比如我在写这篇博客的时候，我想写一些关于机器学习，关于统计，关于工程实现方面的东西，那么我的这个多面体恐怕“煎饼果子”，“热干面”这样的词出现的概率就得很小，可当我写一些美食相关的博客时，这样一个多面体或许又显得有些不够用。</p>
<p>主题模型很好地解决了这一问题，相比Unigram Model，它添加了主题层，让文档生成的方式更加自然合理。一篇文章往往由多个主题构成，一个主题可以由多个与之相关的词语来描述。简单来说，当我们现在要创作一篇文当时，我们并不是只有一个大的多面体了，而是有多个“主题多面体”。在计算机主题相关的多面体上，内存、硬盘、复杂度、编程这些词出现的概率会更大；而在音乐相关的主题上。莫扎特、钢琴、二分音符、C大调这样的词出现的概率会更大。</p>
<p>当然，这时我们生成一篇文档，就需要两种多面体。第一种多面体有一个，它有K个面，每个面代表一个主题；第二种多面体有K个，每一个有W革面，每一个代表一个词。当我们生成一篇有N个词文档时，对每一个词，我们先扔第一种多面体，选定一个主题，然后扔对应的第二种多面体，得到最终的词。</p>
<p>这样直观的想法最初由Hoffmm在199年给出的PLSA（Probabilistic Latent
Semantic Analysis）模型中进行了明确的数学化。PLSA在对文档主题建模时，除了词袋模型的大前提外，主要有三点假设：</p>
<blockquote>
<ul>
<li>一篇文章可以由多个主题构成</li>
<li>每一个主题由一组词的概率分布来表示</li>
<li>一篇文章中的每一个具体的词都来自于一个固定的主题</li>
</ul>
</blockquote>
<p>对于PLSA模型而言一篇文档生成的方式如下</p>
<blockquote>
<hr>
<p><strong>PLSA文档生成过程</strong></p>
<hr>
<p>假设：有两类多面体分别是</p>
<ul>
<li><strong>doc-topic多面体</strong>，每一个doc-topic多面体有K个面，K代表主题个数，每个面对应一个主题编号。</li>
<li><strong>topic-word多面体</strong>，一共有K个，每个topic-word多面体有V个面，每个面对应一个词。</li>
</ul>
<hr>
<p>过程：</p>
<p>1.选定文档的词语数量N；</p>
<p>2.抛掷doc-topic多面体，记录朝上的面对应的主题k；</p>
<p>3.抛掷k对应的topic-word多面体，记录朝上的面对应的词v</p>
<p>3.重复步骤2,3, N次，直到所有词都生成。</p>
<hr>
</blockquote>
<img src="/2017/11/07/LDA-From-Zero/6.png" title="Figure 6. PLSA文档生成过程">
<p>每篇文档生成的概率和整个语料库生成的概率就可以由多个多项分布的联合分布求出来，具体公式这里略去，我们直接开始讲LDA。</p>
<h3 id="lda文本主题建模"><a class="header-anchor" href="#lda文本主题建模">¶</a>LDA文本主题建模</h3>
<p>好了，终于到LDA了。</p>
<p>细心的你一定会想，unigram model中多面体的概率能用贝叶斯估计，那PLSA是否也能用贝叶斯估计能。当然！如果把PLSA中doc-topic多面体和topic-word多面体每一面朝上的概率加上Drichlet先验分布，就得了LDA模型！简单来说，可以理解为LDA就是把PLSA中的概率估计改造成了一个贝叶斯估计，选用的先验分布是Drichlet分布。（下面这个图我个人觉得把$Dir(\vec{\beta)}$）指向右边的topic-word多面体会更好一些。)</p>
<img src="/2017/11/07/LDA-From-Zero/7.png" title="Figure 7. LDA文档生成过程">
<p>每一篇文章的主题分布服从Dirichlet-Multi共轭结构，这样的结构一共有M个；每一个主题的词分布也符合Dirichlet-Multi共轭结构，这样的结构一共有K个。LDA建模的本质么就是这个M+K个共轭结构和他们之间的关系，LDA模型的求解也就是求这M+K个共轭结构构成的联合分布的期望。</p>
<p>如果觉得这段话不好理解，也没关系，下面我们开始列公式，我们可以直接从公式中看出LDA模型的本质。</p>
<p>为了更好的讲解公式，统一用法，减少歧义，我们现在把即将用到的公式中的每一个变量和参数列在下面，供读者查阅。</p>
<blockquote>
<ul>
<li>K:主题个数</li>
<li>V：语料库中所有的、无重复的词语个数</li>
<li>M: 语料库中文档数量</li>
<li>$\vec{\alpha}$：doc-topic多面体的先验分布的超参数，维数为K。</li>
<li>$\vec{\beta}$：topic-word多面体的先验分布的超参数，维数为V。</li>
<li>$\vec{\theta}_m$： 第m篇文档的主题分布，即这篇文档的doc-topic多面体投掷时出现每一个topic的概率，维度为K。</li>
<li>$\vec{\varphi}_k$： 第k个主题的词分布，即这个主题的topic-word多面体投掷时出现每一个word的概率，维度为V</li>
<li>$N_m$： 语料库中第m篇文档的词数量</li>
<li>$\vec{z}$：整个语料库的每一个词的主题构成的向量</li>
<li>$\vec{z}_m$：第m篇文档的每一个词的主题构成的向量，维度为N_m</li>
<li>$z_{m,n}$：第m篇文档的第n个词的主题</li>
<li>$w_{m,n}$：第m篇文档的第n个词的具体词语</li>
<li>$\vec{n}_m=(n_m ^1,\cdots,n_m ^K)$：$n_m ^K$表示在语料库中，第m篇文档中属于主题k的词语的个数。</li>
<li>$\vec{w}_k=(v_k ^1,\cdots,v_k ^V)$：第k个主题下每个词的个数。$v_k ^t$表示在语料库中，第k个主题包含的某一个具体的词语V的个数。</li>
</ul>
<blockquote>
<ul>
<li>$\vec{w}=(\vec{w}_1,\cdots,\vec{w}_k)$：所有主题下的每个词的个数。</li>
</ul>
</blockquote>
</blockquote>
<p>先上概率图。</p>
<img src="/2017/11/07/LDA-From-Zero/8.png" title="Figure 8. LDA概率图">
<p>由概率图和文档生成过程我们可以将LDA中每一个词生成的过程分解为两个大的物理过程，分别是</p>
<p>1.生成每一个词对应的主题，$\vec{\alpha} \rightarrow \vec{\theta_m} \rightarrow z_{m,n}$。在这个过程中，首先根据超参数$\vec{\alpha}$采样得到第m篇文档的主题分布$\vec{\theta_m}$，相当于的到了一个doc-topic骰子；然后投掷这个骰子，得到第n个词对应的topic编号$z_{m,n}$。这里M篇文档对应M个Dirichlet-Multi共轭结构。</p>
<p>2.根据主题生成具体的词，$\vec{\beta} \rightarrow \vec{\varphi_k} \rightarrow w_{m,n}$。在这个过程中，根据超参数$\vec{\beta}$采样得到第k个主题的词分布$\vec{\varphi_k}$，相当于的到了一个编号是$k=z_{m,n}$的topic-doc骰子；然后投掷这个骰子，得到第n个词对应的词$w_{m,n}$。这里K个主题对应K个Dirichlet-Multi共轭结构。</p>
<p>需要注意的是，在这两个过程中，每一个Dirichlet-Multi共轭结构都是独立的。这也就意味着，不仅可以一次就把$w_{m,n}$具体的词语生成出来，也可以先生成所有的$z_{m,n}$,再生成所有的$w_{m,n}$，这两个过程是完全等价的。根据这一特性，我们可以把LDA的文档生成过程定义为如下形式。</p>
<blockquote>
<hr>
<p><strong>LDA文档生成过程</strong></p>
<hr>
<p>假设：有两类多面体分别是</p>
<ul>
<li><strong>doc-topic多面体</strong>，一共有M个，每个doc-topic多面体有K个面，每个面对应一个主题编号。这个多面体每个面朝上的概率分布服从超参数为$\vec{\alpha}$的Dirichlet分布。</li>
<li><strong>topic-word多面体</strong>，一共有K个，每个topic-word多面体有V个面，每个面对应一个词。这个多面体每个面朝上的概率分布服从超参数为$\vec{\beta}$的Dirichlet分布。</li>
</ul>
<hr>
<p>过程：</p>
<ol>
<li>
<p>选定每一篇文档的词语数量$N_m$；</p>
</li>
<li>
<p>对第m篇文档，根据超参数$\vec{\alpha}$采样的到对应的doc-topic多面体。</p>
</li>
<li>
<p>对第m篇文档的第n个词，投掷doc-topic多面体，得到该词的主题$z_{m,n}$。</p>
</li>
<li>
<p>重复步骤3共$N_m$次，得到第m篇文档的每一个词的主题编号$\vec{z_m}$。</p>
</li>
<li>
<p>重复步骤2-4共M次，得到每一篇文档的的每一个词的主题编号。（过程1结束）</p>
</li>
<li>
<p>对第k个主题，根据超参数$\vec{\beta}$采样的到对应的topic-word多面体。</p>
</li>
<li>
<p>对第m篇文档的第n个词，根据主题编号选择对应的topic-word多面体，投掷该多面体，得到对应的词$w_{m,n}$。</p>
</li>
<li>
<p>重复步骤7，直到所有词都得到具体的词。（过程2结束）</p>
</li>
</ol>
<hr>
</blockquote>
<p>从这一过程就可以很清晰的看出，LDA模型求解的过程实际上就是求这M+K个Dirichlet-Multi共轭结构下文档生成概率的联合分布了。那么Dirichlet-Multi共轭结构如何如何求解呢？</p>
<p>在过程1中，超参数为$\vec{\alpha}$的第m篇篇文档下每一个词的主题编号生成概率，我们可以由<a href="#jump">公式3.1</a>得到</p>
<p>$$p(\vec{z}_m|\vec{\alpha}) =\frac{\Delta(\vec{n}_m+\vec{\alpha})}{\Delta (\vec{\alpha})}$$</p>
<p>同时，我们得到隐含参数$\vec{\theta}_m$的后验分布恰好是</p>
<p>$$Dir(\vec{\theta}_m|\vec{n}_m+\vec{\alpha})$$</p>
<p>整个语料库下的所有词的主题编号的生成概率就是每一篇文档的联合分布</p>
<p>$$p(\vec{z}|\vec{\alpha}) =\prod_{m} ^{M} \frac{\Delta(\vec{n}_m+\vec{\alpha})}{\Delta (\vec{\alpha})}$$</p>
<p>同样的道理，我们可以得到过程二中，每个主题下的词生成概率为
$$p(\vec{w}_k|\vec{\beta}) =\frac{\Delta(\vec{v}_k+\vec{\beta})}{\Delta (\vec{\beta})}$$</p>
<p>同时，我们得到隐含参数$\vec{\varphi}_k$的后验分布恰好是</p>
<p>$$Dir(\vec{\varphi}_k|\vec{v}_k+\vec{\beta})$$</p>
<p>对于整个语料库的所有词的生成过程来说，每一个词具体的主题编号也是最终决定词语的先决条件，但是无论选择哪一个主题编号，并不会改变着K个主题下的词分布(<strong>这一点很重要，可以将过程1和过程2分开来看</strong>)，所以有
$$p(\vec{w}|\vec{z},\vec{\beta})=p(\vec{w}|\vec{\beta})=\prod_{k} ^{V} \frac {\Delta(\vec{v}_k+\vec{\beta})}{\Delta (\vec{\beta})}$$</p>
<p>结合上面的式子，我们最终可以得到，两个过程加在一起，LDA模型的语料库的生成概率为</p>
<p>$$p(\vec{w},\vec{z}|\vec{\alpha},\vec{\beta})=p(\vec{w},\vec{z}|\vec{\beta})\cdotp(\vec{z}|\vec{\alpha})=\prod_{k} ^{V} \frac {\Delta(\vec{v}<em>k+\vec{\beta})}{\Delta (\vec{\beta})} \cdot \prod</em>{m} ^{M} \frac{\Delta(\vec{n}_m+\vec{\alpha})}{\Delta (\vec{\alpha})}$$</p>
<p>到这里，整个LDA模型的文档生成过程就介绍完了。LDA模型在实际应用中，就是要估计隐含变量$\vec{\theta}_m$和$\vec{\varphi}_k$的值，也就是要求出doc-topic多面体和topic-word多面体的每个面朝上的概率大小，由此我们可以去分析文档的主题分布和主题的词分布。那么，这两个概率大小究竟该怎么求呢，我们继续往下探讨。</p>
<h2 id="lda模型的求解"><a class="header-anchor" href="#lda模型的求解">¶</a>LDA模型的求解</h2>
<p>OK，模型的基本构成我们了解完了，接下来就要看模型的求解了。对于LDA模型来说，最基本的目标有两个：</p>
<ul>
<li>Training。根据一个已有的语料库，我们需要这个语料库用LDA方式生成过程中的参数$\vec{\theta}_m$和$\vec{\varphi}_k$，由此可以把生成这个语料库的LDA模型完全确定下来。</li>
<li>Inference。对于一篇不在这个语料库中的文档，我们能够根据已经确定的LDA模型来计算这篇文档的主题分布。</li>
</ul>
<p>当然，要先进行Training，然后才能进行Inference。Training的过程实际上就是对于上一节中我们得到的$p(\vec{w},\vec{z}|\vec{\alpha},\vec{\beta})$分布求期望，由于$p(\vec{w})$是我们观测到的已知变量，实际上就是要求分布$p(\vec{z}|\vec{w}$的期望，也就是每一个词对应的最可能的主题编号。</p>
<p>如何求这个期望是一个难题。早期的PLSA和LDA的求解都是基于最大期望（Expectation Maximization）算法做的，要对$p(\vec{w},\vec{z}|\vec{\alpha},\vec{\beta})$直接积分求解，这一过程的时间复杂度和空间复杂度都比较高，好处是可以得到精确解。在工程上和较新的一些关于主题模型的研究中，则往往采用Gibbs Sampling的方式求解。这种方式得到的是近似解，但是求解速度快，工程实现也比较简单。下面，我们就讲解如何通过Gibbs Sampling的方式求解LDA模型。</p>
<h3 id="采样"><a class="header-anchor" href="#采样">¶</a>采样</h3>
<p>Sampling，就是采样。理解Gibbs Sampling的基础，是要明白采样的目的和原理。**所谓采样，是指获得符合某一概率分布的一些列样本，根据大数定律，当这些样本足够多时，样本的期望会逐渐趋近于概率分布的期望。**也就是说，采样的目的就是通过样本期望去估计分布的期望。</p>
<p>举个例子，有一个每个面大小不相同的骰子，现在想知道每个面朝上的概率大小是多少，怎么做？最先想到的最简单的方法，就是采样。不断投掷这个骰子，记录下每次投掷朝上的面，然后计算这些面出现的次数占总次数比，就可以大致的估计每个面朝上的概率。</p>
<p>对LDA求解也是一样，直接求解期望很麻烦，那么我们不妨通过采样得到一系列符合联合分布的样本，计算样本的期望就可以估计分布的期望。如何得到符合联合分布的样本呢，这就要引入另一组数学知识——马氏链和蒙特卡洛方法。</p>
<h3 id="马氏链"><a class="header-anchor" href="#马氏链">¶</a>马氏链</h3>
<p>马氏链（Markov Chain）的数学定义很简单，
$$P(X_{t+1}=x|X_t,X_{t-1},\cdots)=P(X_{t+1}=x|X_t\cdots)$$
即在一个状态转移链中，如果下一时刻状态转移的概率只和当前状态有关，那么这个转移链就是一个马氏链。</p>
<p>用一个例子来说明马氏链和马氏链产生的过程。（这个例子我实在《LDA数学八卦》里看到的，不过网上还想用烂了）社会学家经常把人按其经济状况分成3类：下层(lower-class)、中层(middle-class)、上层(upper-class)，我们用1,2,3 分别代表这三个阶层。社会学家们发现决定一个人的收入阶层的最重要的因素就是其父母的收入阶层。如果一个人的收入属于下层类别，那么他的孩子属于下层收入的概率是 0.65, 属于中层收入的概率是 0.28, 属于上层收入的概率是 0.07。事实上，从父代到子代，收入阶层的变化的转移概率如下。</p>
<img src="/2017/11/07/LDA-From-Zero/10.png" title="Figure 9. 转移概率表">
<img src="/2017/11/07/LDA-From-Zero/9.png" title="Figure 10. 转移概率图">
<p>我们用矩阵的形式表示，状态转移矩阵记为</p>
<p>$$P=\begin{bmatrix}
0.65 &amp; 0.28 &amp; 0.07\\
0.15 &amp; 0.67 &amp; 0.18\\
0.12 &amp; 0.36 &amp; 0.52
\end{bmatrix}$$</p>
<p>我们假设当前这一代下、中、上层人口的比例是$\pi_0=[0.21,0.68,0.11]$，那么下一代的比例分布式$\pi_0=\pi_1\cdot P$，以此类推，可以计算前n代人中下、中、上层人口比例为</p>
<img src="/2017/11/07/LDA-From-Zero/11.png">
<p>从第7代开始，这个比例就不变了。这是偶然吗？换一个初始概率$\pi_0=[0.75, 0.15, 0.1]]$再试一次，前n代人中下、中、上层人口比例为</p>
<img src="/2017/11/07/LDA-From-Zero/12.png">
<p>第9代人的时候, 分布又收敛了。奇特的是，两次给定不同的初始概率分布，最终都收敛到概率分布$\pi=[0.286,0.489,0.225]$，也就是说收敛的行为和初始概率分布 $\pi_0$ 无关。这说明这个收敛行为主要是由概率转移矩阵P决定的。</p>
<blockquote>
<p>[1]Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation[J]. Journal of machine Learning research, 2003, 3(Jan): 993-1022.]</p>
</blockquote>

      
    </div>
    
    
    
	
	
	  <div>
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------End. Thank u  for reading.-------------</div>
    
</div>
	  </div>
	

    
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          
            <a href="/tags/Statistics/" rel="tag"># Statistics</a>
          
        </div>
      

      
        <div>
          <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>如果您喜欢这篇文章，请给作者一点支持~</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.png" alt="Aros Lin 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="Aros Lin 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

        </div>
      


      
      
        <div class="post-widgets">
        
          <div class="wp_rating">
            <div id="wpac-rating"></div>
          </div>
        

        

        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/11/07/今天开始写博客/" rel="next" title="今天开始写博客">
                <i class="fa fa-chevron-left"></i> 今天开始写博客
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/11/07/Self-Control-Pressure/" rel="prev" title="《自控力-和压力做朋友》读书笔记">
                《自控力-和压力做朋友》读书笔记 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        
  <div class="bdsharebuttonbox">
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
    <a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a>
    <a href="#" class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
    <a href="#" class="bds_tieba" data-cmd="tieba" title="分享到百度贴吧"></a>
    <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
    <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
    <a href="#" class="bds_more" data-cmd="more"></a>
    <a class="bds_count" data-cmd="count"></a>
  </div>
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "2",
        "bdMiniList": false,
        "bdPic": ""
      },
      "share": {
        "bdSize": "16",
        "bdStyle": "0"
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src=src='/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script>

      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zMTc2My84MzI3"></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="Aros Lin" />
            
              <p class="site-author-name" itemprop="name">Aros Lin</p>
              <p class="site-description motion-element" itemprop="description">Machine Learning</br>Data Mining</br>Psychology</br>Personal Site</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/aroslin" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="http://weibo.com/aroslin" target="_blank" title="微博">
                    
                      <i class="fa fa-fw fa-weibo"></i>微博</a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#从扔硬币说起"><span class="nav-number">1.</span> <span class="nav-text">¶从扔硬币说起</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#二项分布"><span class="nav-number">1.1.</span> <span class="nav-text">¶二项分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#贝叶斯估计"><span class="nav-number">1.2.</span> <span class="nav-text">¶贝叶斯估计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#beta分布与共轭"><span class="nav-number">1.3.</span> <span class="nav-text">¶Beta分布与共轭</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多项分布与dirichlet共轭"><span class="nav-number">1.4.</span> <span class="nav-text">¶多项分布与Dirichlet共轭</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#latent-dirichlet-allocation"><span class="nav-number">2.</span> <span class="nav-text">¶Latent Dirichlet allocation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#unigram-model"><span class="nav-number">2.1.</span> <span class="nav-text">¶Unigram Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#主题模型"><span class="nav-number">2.2.</span> <span class="nav-text">¶主题模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lda文本主题建模"><span class="nav-number">2.3.</span> <span class="nav-text">¶LDA文本主题建模</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lda模型的求解"><span class="nav-number">3.</span> <span class="nav-text">¶LDA模型的求解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#采样"><span class="nav-number">3.1.</span> <span class="nav-text">¶采样</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#马氏链"><span class="nav-number">3.2.</span> <span class="nav-text">¶马氏链</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-arrows-alt"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Aros Lin</span>

  
</div>




  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.3</div>



        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 本站访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>



<script>
  	var _mtac = {};
  	(function() {
  		var mta = document.createElement("script");
  		mta.src = "https://pingjs.qq.com/h5/stats.js?v2.0.4";
  		mta.setAttribute("name", "MTAH5");
  		mta.setAttribute("sid", "500546792");

  		var s = document.getElementsByTagName("script")[0];
  		s.parentNode.insertBefore(mta, s);
  	})();
</script>






        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  









<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="//unpkg.com/valine/dist/Valine.min.js"></script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("SWNkymbB0682pkUfd3Xz8Tiy-gzGzoHsz", "BEUXhaWBblGCa0ICtt7iF2Ou");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  
  <script type="text/javascript">
  wpac_init = window.wpac_init || [];
  wpac_init.push({widget: 'Rating', id: 7962,
    el: 'wpac-rating',
    color: 'fc6423'
  });
  (function() {
    if ('WIDGETPACK_LOADED' in window) return;
    WIDGETPACK_LOADED = true;
    var mc = document.createElement('script');
    mc.type = 'text/javascript';
    mc.async = true;
    mc.src = '//embed.widgetpack.com/widget.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling);
  })();
  </script>


  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
