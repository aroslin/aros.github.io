<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Topic-driven Multi-type Citation Network Analysis]]></title>
    <url>%2F2017%2F11%2F10%2Fpaper5%2F</url>
    <content type="text"><![CDATA[作者：Yang Z, Hong L, Davison B D. 年份：2010 期刊：Adaptivity, Personalization and Fusion of Heterogeneous Information. 研究内容：Focus on the task of ranking authors.以往的研究已经使用了content-based approaches或者citation network link analyses,但两者的结合却很少。本文提出一种新的概率模型，模型结合了基于内容的方法和多种引用网络（文章之间，作者之间，隶属关系，出版场所），并且提出了heterogeneous PageRank random surfer model来反映不同特征的重要性。 本文贡献： Proposing a novel probabilistic model which combines content-based analysis with a multi-type citation network, integrating relationships of authors, papers, affiliations and publishing venues in one model. This model can be extended to include more types of social factors. Proposing a heterogeneous PageRank random surfer model compared with the original uniform PageRank model, to reflect the impact among different factors. Introducing topical link analysis into citation network analysis. In particular, Topical PageRank [18] is adopted for citation link analysis. A comparative study using ACM digital library data on various PageRank extensions as well as different complexity of citation networks. 说实话，他号称的Combine就是把俩结果用一个权值加起来，总有一种上当受骗的感觉…🙃🙃🙃 ¶主要方法： ¶构建网络图 ¶4-T graph version-1 四个Factor，四副sub-graph。 Author Graph GAu。 节点是作者。如果两个作者共同写过一篇文章或者一个引用过另一个，两个作者间存在连线。单线，无权。（下同） Paper Graph GP。 节点是文章，连线是引用。 Affiliation Graph GAf。 节点是机构，如果两个机构下至少有一对作者有连线，两机构连线。 Venue Graph GV。节点是出版社，如果两个出版社下至少有一对文章有连线，两出版社连线。 子网间的连线：根据各自的关系，有一度连接的都连起来。 作者连线发表文章 作者连线属于机构 作者连线发表文章出版社 文章连线作者机构 文章连线出版社 机构连接下属作者发表文章对应的出版社。 ¶4-T graph version-2 框架1存在冗余。设计框架2，子图和图内连接方式相同，作者只和文章与机构连线，文章之和作者和出版物连线。 ¶Topical PageRank求解 整张图看作一个大的网络，使用Pagerank进行排序。不同于均匀的PageRank的是，节点间的传播概率是不同的。相同类型的两个节点间的传播概率相同。 基于Topical PageRank来实现content-based analysis。每一个节点对应一个Page，包括代表这个节点的T个主题的分布组成的主题向量和代表这个节点在每个主题上的重要程度的authority向量。random surfer model，游走求解。 ¶Combine content-based approach with citation network Okapi BM25：In information retrieval, Okapi BM25 (BM stands for Best Matching) is a ranking function used by search engines to rank matching documents according to their relevance to a given search query. content-based approach就是用Okapi BM25算一个结果，citation network就是前面的Topical PageRank，Combine就是把他俩用一个权值加起来。 这就是你说的Combine？你在逗我？🙂 三种方式来评价结果和query的匹配程度（你上面不是说不用于检索吗？） In the first approach, we collected all the PC members in the related conferences for each research area during 2008 and 2009. In the second approach, we collected all the ACM fellows, ACM distinguished and senior members provided from the ACM website. We utilized human judgements to generate relevant lists in the third approach. In our evaluation system, the top ten and twenty returned authors by various ranking algorithms were retrieved and mixed together. ¶实验结果 评价指标：NDCG（网页中用于衡量网页排序质量的指标） 越后面介绍的方法指标更好。 参数的比较。 node2vec: Scalable Feature Learning for Networks]]></content>
      <categories>
        <category>文献阅读</category>
        <category>Technology foresight</category>
      </categories>
      <tags>
        <tag>Technology foresight</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Social Influence Analysis in Large-scale Networks]]></title>
    <url>%2F2017%2F11%2F10%2Fpaper4%2F</url>
    <content type="text"><![CDATA[作者：Jie Tang 年份：2009 期刊：KDD 研究内容：区分不同angle(topic)上的社会影响，量化影响的大小。 propose Topical Affinity Propagation (TAP) to model the topic-level social influence on large networks. ¶主要思想 在主题层面（topic level）上利用亲和力传播（affinity propagation）来进行社会影响鉴定。 TAP provides topical influence graphs that quantitatively measure the influence on a fine-grain level; The influence graphs from TAP can be used to support other applications such as finding representative nodes or constructing the influential subgraphs; An efficient distributed learning algorithm is developed for TAP based on the Map-Reduce framework in order to scale to real large networks. ¶数据 网络和节点下的主题分布。一个作者共现网络，一个引用网络，一个film-director-actor-writer网络 ¶结论 相较于文本相似度有两个优势1.可以分析来年各个节点互相影响的差异。2.可以计算整个网络中影响力最强的节点。 借鉴意义：可对社交网络中用户节点转化成向量，然后计算用户节点与相连节点的相似性，作为亲密度度量的手段。 Social Influence Analysis in Large-scale Networks]]></content>
      <categories>
        <category>文献阅读</category>
        <category>Social Networks</category>
      </categories>
      <tags>
        <tag>Social Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Identifying potentially disruptive trends by means of keyword network analysis]]></title>
    <url>%2F2017%2F11%2F10%2Fpaper3%2F</url>
    <content type="text"><![CDATA[作者：Fefie Dotsika, Andrew Watkins 年份：2017 期刊：Technological Forecasting &amp; Social Change 研究内容：通过词共现生成词网络，一系列的词语代表一个技术点。由词网路各种中心性的对比变化分析技术的破坏性趋势。 ¶关键术语含义 有价值技术：In an ever-changing technological landscape where innovation is a crucial driver for economic growth and survival, it is desirable to be able to predict which technologies, when established, have the potential to revolutionise an industry, create new markets, and increase accessibility and affordability. Disruptive innovation定义：Disruptive innovation is defined as the process of transforming a product or service that historically has been accessible at the top of a market access (i.e. for a high price or specialised skill-set) to become accessible to a new and larger population of consumers at the bottom of that market Maturing trends were found to share influential common topics identified by high degree, betweenness and closeness centrality scores.Niche and potentiallyemerging trends within groupswere detected by means of eccentricity and farness metrics. ¶使用数据 WOS文献数据 Forrester,Frost &amp; Sullivan, Gartner, IDC and Ovum.五种Business reports ¶分析方法 ¶1.分析各个领域的数量分布。 结论：没有明显证据表明学术出版物总是超前于商业出版物。 ¶2.网络结构与特征分析。 网络的节点是词，连线表示两个词在同一个出版物中共现的次数总和，使用UCNET等工具制作。 网络结构指标，包括network size，density，diameter，average degree等。 聚类与子网络指标，包括coefficient，Erdös number，average embeddedness，modularity等。 结论： 所有网络呈现low density和high clustering coefficients，与随机网络差异明显 Positive modularity values Erdös number is low ¶3.节点位置分析（找出已经是disruptive 的技术） Degree centrality，指标越高，语义重要性越中心。 Eigenvector centrality，扩展了Degree centrality中心性的概念，与上类似。 Betweenness centrality，起到桥梁作用的次数越多，在信息流中更有影响力。 Closeness centrality，与网络中其他所有点的距离和。 Eccentricity，距离最远点的距离。 提取degree、eigenvector、betweenness 、closeness四项中最高的两个词作为main keyword，通过可视化验证了这些词确实处于central的位置。 根据不同的中心性组合得到不同的disruptive技术类型。 如High degree- Low betweenness，表示Popular mature keyword. ¶4.成熟前位置分析（找出可能成为disruptive 的技术） 有high closeness 和 low degree 的技术最有成为disruptive 的技术的趋势。 列举Twitter`s popularity等一系列网络例子来证明这种趋势的存在。 借鉴意义： 网络形成后的趋势分析方法。 例证方式。 Identifying potentially disruptive trends by means of keyword network analysis]]></content>
      <categories>
        <category>文献阅读</category>
        <category>Technology foresight</category>
      </categories>
      <tags>
        <tag>Technology foresight</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Survey on Social Community Detection]]></title>
    <url>%2F2017%2F11%2F10%2Fpaper2%2F</url>
    <content type="text"><![CDATA[作者：Michel Plantie, Michel Crampes 年份：2013 期刊：Social media retrieval. Springer London 研究内容：对以往的Social Community Detection研究进行了总结，得出三种常见的分析方法。 The first approach considers the social network as a graph and then analyzes its structure with graph properties and algorithms built around the graph structure. The second approach associates the social network with a hypergraph and analyzes its structure through hypergraph properties and algorithms based on hypergraph structures. The third approach uses the properties of concept lattices in order to analyze the social network structure in association with hypergraph properties and algorithms based on Galois lattices and hypergraph structures. 总结很详细，不过没读完。 ¶关键对象说明 图(Graph)：node和edge构成，edge只连接两个nodes。 超图(Hypergraph)：node和hyperedges构成，hyperedges可以连接多个nodes，一个hyperedges内包含的就是一个community。 Galois点阵（Galois lattice）Individuals sharing the same subset of properties define a community. row是属性，column是对象，画出一个图，包含先沟通的子集的对象被认为是一个community。 Survey on Social Community Detection]]></content>
      <categories>
        <category>文献阅读</category>
        <category>Social Networks</category>
      </categories>
      <tags>
        <tag>Social Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo Next如何置顶文章]]></title>
    <url>%2F2017%2F11%2F10%2Ftop-post%2F</url>
    <content type="text"><![CDATA[¶安装hexo-math 参考Netcan_Space博客 安装hexo-generator-index-pin-top，然后在Front-matter中加上top: true即可. 在\themes\next\layout\_macropost.swig下修改修改相应的效果。 Next主题自带Frongt-matter中使用Sticky: true的方式添加指定效果，但是在这篇博文写作的时候该方法只会将文章置顶到某一个页面的最上方。如果你的首页博客是一页最多显示10篇，而从时间上你要指定的文章排在第15篇，那么设定sticky会导致你的文章在/page/2中被置顶。 本博客使用的置顶效果是在post-sticky-flag和post-title-link之间加入 12345&#123;% if post.top &gt; 0 %&#125; &lt;span class=&quot;post-top-flag&quot; title=&quot;top&quot;&gt; &lt;font color=&quot;red&quot;&gt;[置顶]&lt;/font&gt; &lt;/span&gt;&#123;% endif %&#125;]]></content>
      <categories>
        <category>问题梳理</category>
      </categories>
      <tags>
        <tag>Problem</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Node2vec: Scalable Feature Learning for Networks]]></title>
    <url>%2F2017%2F11%2F10%2Fpaper1%2F</url>
    <content type="text"><![CDATA[作者：Aditya Grover，Jure Leskovec 年份：2016 期刊：KDD 研究内容：Network Embedding。网络的特征学习和向量化表达，在multi-label classification和link prediction两个方面得到了有效性验证。 七月份精读过，不过当时没有记录太多。是一片适合作为random surfer model的入门之作。 主要方法： ¶Rrandom walk neighborhood sampling 获得圈子训练集。 当从节点t移动到节点v时，v移动到下一个节点x的概率由节点t到节点x的距离决定。 ¶Skip-gram训练得到的Embedding模型。 训练时，输入时初始的node u，输出的random walk采样到的neighbor nodes。类似于word2vec 中的输入时一个词，输出是上下文的词进行训练。 ¶结果展示 可以根据选择不同的步长和概率参数得到不同的结果，如发现社交圈子（上图），或者发现在网络结构中处在类似地位的节点（下图）。 借鉴意义：可对社交网络中用户节点转化成向量，然后计算用户节点与相连节点的相似性，作为亲密度度量的手段。 node2vec: Scalable Feature Learning for Networks]]></content>
      <categories>
        <category>文献阅读</category>
        <category>Social Networks</category>
      </categories>
      <tags>
        <tag>Social Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo写博客编辑公式相关问题]]></title>
    <url>%2F2017%2F11%2F08%2Fhexo-mathjax%2F</url>
    <content type="text"><![CDATA[¶安装hexo-math hexo-math的github有详细的教程，主要就是两点。 一是安装 npm install hexo-math --save 二是在站点“_config.yml”中配置 1234567891011math: engine: 'mathjax' # or 'katex' mathjax: src: custom_mathjax_source config: # MathJax config katex: css: custom_css_source js: custom_js_source # not used config: # KaTeX config 值得一提的是，如果你是用的是Next主题，那么你还需要在\themes\next\下的“_config.yml”中配置启用mathjax，否则是不会生效的。（我这里弄了好久，还以为各种装错了，最是还是看了参考链接才找到问题） mathjax: enable: true]]></content>
      <categories>
        <category>问题梳理</category>
      </categories>
      <tags>
        <tag>Problem</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《自控力-和压力做朋友》读书笔记]]></title>
    <url>%2F2017%2F11%2F07%2FSelf-Control-Pressure%2F</url>
    <content type="text"><![CDATA[《自控力-与压力做朋友》（The Upside of Stress） 作者：[美] 凯里·麦格尼格尔 Kelly McGonigal 北京联合出版社2016年3月第1版 末尾附思维导图下载链接 To do something special, you just have to believe it`s special. 人们为了减少吸烟，就在烟盒上画着吸烟者千疮百孔的肺，期望能警醒人们减少吸烟。可惜的是，人们看到这些吸烟者的肺，压力变得更大，有什么是比抽烟更能缓解压力的呢？于是人们抽了更多的烟，又给自己带来了更多的压力。 《自控力-与压力做朋友》Kelly McGonigal的第二本关于自控力的书。（JD上买错了才发现还有第一本…） 整本书的中心思想是：改变对压力的看法，会使你更健康和幸福。文中通过大量的社会学实验作为论据来证明论点。内容的展开都是从实验的角度出发，探讨面对压力时不同的态度导致的不同的生理性反映，干预和对比来说明实验的合理性。大量的例证让论证看起来很丰富，但缺少体系的结构设计和略显重复的实验内容让人看起来也有些疲惫。 但看结论的话，有点像鸡汤。不过书中除了大量实验佐证之外，还提供了很多帮助你改善思维习惯的小练习，许多实验的设计也可以作为调整的参考，值得一读。 ¶第一部分 重新思考压力 ¶第一章——改变我们的思维模式：什么是压力？压力真的都是负担吗？ 过去人们总在说，压力是有害的，要缓解甚至消除压力。 但压力真的都是有害的吗？你是否在面对面对期末考试的压力时更高效的学习？是否在完全没有压力的假期中感到无聊和疲倦？是否在压力过后，感觉自己得到了成长？ 或许让我们难以承受的不是压力本身，而是我们对待压力的态度。 ¶第二章——身处困境时，压力是可以依靠的资源，而非要消灭的敌人 比赛开始前，运动员们血脉喷张，心跳加速，身体新成代谢加快，他们告诉自己，我很兴奋，我想要大干一场。 比赛开始前，学生们血脉喷张，心跳加速，身体新成代谢加快，他们告诉自己，我很紧张，我得找个地方冷静一下。 其实压力可以给我们带来动力，让我们更集中，以更好的状态面对难题。 ¶第三章——压力和意义成正比：有意义，意味着有压力 寒假每天躺在床上，什么也不干，玩玩手机，吃了睡，睡了吃，这样的生活多好啊~ 其实一点也不好，每天反而很累，而且觉得无趣。 丢失了压力的生活，也丢失了意义 ¶第二部分 转化压力 ¶第四章——全身心投入：拥抱焦虑能帮助你更好的应对挑战 试着让自己觉得兴奋而不是紧张。 拥抱压力，你能获得勇气。 ¶第五章——内在联结：压力能使人更具关怀性，提升抗挫力 感到压力的时候，我们会倾向于和人交流我们的压力，来缓解自己的情绪；同样的他人和我们交流他们的压力时，如果我们有过类似的压力，就能带去更多的关怀。 怀着宏大的目标，崇高的使命感也能让人更有勇气面对压力。 Life is always hard. Everybody knows. ¶第六章——幸福成长：痛苦使你坚强，即使痛苦正当下，未来尚模糊 经历过中等苦难的人，更容易成长，变得有毅力，身体更加康，幸福感也更高。不是说要支持去伤害自己，而是学会把痛苦变成自己的资源，让自己更强大。 Everything that kills me, makes me feel alive. **结语：**作者上一本书《自控力》，讲的就是如何减少拖延，然而新书还是拖了很久才出版，可见哪怕写了这么多，作者自己不是立马就成了完人。思维不是那么容易变得，但人总成长。至少我今日写下了这篇博客，我在记录，我在改变。 [自控力思维导图下载](http://aroslin.github.io/downloads/The Upside of Stress.xmind)]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>Psychology</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始学LDA（Latent Dirichlet allocation）]]></title>
    <url>%2F2017%2F11%2F07%2FLDA-From-Zero%2F</url>
    <content type="text"><![CDATA[去年初学LDA，看完了Rickjin老师的《LDA数学八卦》，觉得不是很懂，查阅了很多资料之后才对LDA有了更深入地认识。一直想加入自己的理解后更简单的讲述这个模型，今日补上。 注：文章的行文思路与大多数公式参考了Rickjin《LDA数学八卦》。有兴趣更深入了解的同学可以在看完本篇后继续阅读。 LDA(Latent Dirichlet allocation)是主题模型主题模型的一种，最早是由Blei D M[1]等人在2003年提出的，常被用于文档的主题识别。时至今日，在大大小小的会议上仍能看到不少对LDA（或其衍生、或类似的主题模型）提出改进的文章。其自然的思路，优雅的求解和拓展性强的模型结构，实在是非常适合作为对无监督学习一个入门模型来学习和改进。 本文主要介绍LDA背后的相关数学知识和模型的构建与求解思路，力求只有少量统计知识的初学者也能看懂。文中略去了大量的公式推导细节，如果希望深入的了解，请根据需要查找相关的资料。 ¶从扔硬币说起 介绍模型之前，我们先来简单回顾一些知识点。本章主要讲解二项分布，贝叶斯理论，beta分布，multi分布和Dirichlet分布。如果你熟悉这些知识点，可以直接跳过这一章。 ¶二项分布 二项分布（英语：Binomial distribution）是n个独立的是/非试验中成功的次数的离散概率分布，其中每次试验的成功概率为p。这样的单次成功/失败试验又称为伯努利试验。实际上，当n = 1时，二项分布就是伯努利分布。二项分布是显著性差异的二项试验的基础。 摘自二项分布维基百科 扔一枚硬币，一共扔N次，其中k次都是正面朝上的概率是多少？ 这个概率服从二项分布(Bernoulli distribution) $$P(k|N,p)=C^k_N\cdot p^k\cdot (1-p)^{N-k}$$ 其中p是硬币扔一次正面朝上的概率。真实场景中，我们面对的问题往往是，我想知道这个公式中的p是多少。一个我们熟悉且自然的思路是，把这个硬币扔N次，得到的结果中k次朝上的那么可以估计 $$p=\frac{k}{N}$$ 如果你学过概率论，那么你就知道这是对二项分布中参数p的一个[极大似然估计。](https://zh.wikipedia.org/zh-hans/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1) ¶贝叶斯估计 当我们像上面去估计p值的时候，其实已经默认了一个假设，就是p是一个定值，它在每次实验中都是相同的。p是一个客观存的量，只是我们不知道它是多少。这样的想法很容易理解，但有时候会带来一些问题。当我们扔了10次硬币，10次都朝上的时候（尽管概率很小，但这一现象也会发生），用上面的方法估计，p=1。这一结论和我们的常识不符，当全部出现正面朝上这种小概率事件时，我么可能会倾向认为p值比0.5大，但没有大到等于1。 这时候，有些人就想，或许p并不是一个定值，而是服从某一种概率分布，对p的一个较好的估计是这个概率分布的期望，那么即使发生了每次都正面朝上的情况，只要这个概率分布不是始终等于1，就不会出现违背常识的情况了。 怎么理解？这就好比说，你扔出的硬币在落下的一瞬间之前，在你无法感知的空间中散落成无数的硬币，这些硬币的密度不尽相同，也就是落下时的p是不同的，但这些p是的有约束的，都是服从某一概率分布。在降落的一瞬间，有且仅有一枚硬币投射到你感知到的硬币上，也就是这一次抛硬币对应的p。由于无数硬币的p值并不是都等于1，所以即使出现扔硬币多次全部正面朝上，也可以理解成这些硬币中p值较大的硬币比较多，而不会是所有硬币的p都等于1。 如果你觉得上面这略显中二的说法也不太好理解的话😑，我们就直接看知识点好了。根据伯努利试验中p值是否固定，划分出了两个学派——频率学派和贝叶斯学派。这两个学派的思路差异主要体现在： 频率派把需要推断的参数θ看做是固定的未知常数，即概率虽然是未知的，但是确定的一个值，同时，样本X是随机的，所以频率派重点研究样本空间，大部分的概率计算都是针对样本X的分布； 贝叶斯派的观点则截然相反，他们认为参数是随机变量，而样本X 是固定的，由于样本是固定的，所以他们重点研究的是参数的分布。 在贝叶斯学派看来，所有的参数都不是固定值，而是服从一个概率分布，而对这个概率分布的估计有一个经典的模式： 先验分布𝜋(𝜃)+样本信息Χ⇒后验分布𝜋(𝜃|𝑥) 上述思考模式意味着，新观察到的样本信息将修正人们以前对事物的认知。换言之，在得到新的样本信息之前，人们对的认知是先验分布𝜋(𝜃)，在得到新的样本信息Χ后，人们对的认知为𝜋(𝜃|𝑥)。 举个例子来说明两个学派分析的思路差异好了。加入我们扔一枚硬币，连续扔10次，有6次出现了正面朝上，那么扔一次正面朝上的概率是多少？ 按照频率学派的观点， $$p=\frac{6}{10}=0.6$$ 按照贝叶斯学派，首先p有一个先验分布，也就是你对p的一个经验估计，为了计算简单和表达的方便，咱们这里取一个离散分布,令$𝜃=p=0.4,0.5,0.6$，且 $P(𝜃=0.4)=0.1$ $P(𝜃=0.5)=0.8$ $P(𝜃=0.6)=0.1$ 先验分布的期望 $$E(𝜃)=0.4\cdot0.1+0.5\cdot0.8+0.6\cdot0.1=0.5$$ 根据样本信息，10次中有6次朝上，由条件概率公式 $$P(𝜃,𝑋)=P(𝜃│𝑋)∙P(𝑋)=P(𝑋|𝜃)∙P(𝜃)$$ 变形得到，在该样本条件下，$𝜃=6$的概率为 $$𝑝(𝜃=0.6│𝑋=6)=\frac{𝑝(𝑋=6|𝜃=0.6)\cdot𝑝(𝜃=0.6)}{𝑝(𝑋=6)} $$ 其中： $𝜃$就是抛一次硬币是正面的概率，X是正面的次数，在本例中就是6； $𝑝(𝜃=0.6│𝑋=6)$就是在6次正面的情况下，$𝜃=0.6$的概率，即后验概率； $𝑝(𝑋=6|𝜃=0.6)$是𝜃=0.6时可以抛出6次正面的概率，即样本信息，这里求法按二项分布来求； $𝑝(𝜃=0.6)$就是𝜃的先验分布，此例子中$𝑝(𝜃=0.6)=0.1$； $𝑝(𝑋)$称为Normalizer，或者叫做marginal probability，是一个定值，$𝜃$分布如果是离散的，就是$𝑝(𝑋|𝜃)\cdot 𝑝(𝜃)$在所有𝜃情况下的和；如果$𝜃$是连续的，$𝑝(𝑋)=\int𝑝(𝑋|𝜃)\cdot 𝑝(𝜃)𝑑𝜃$。 我们就可以分别求出在样本信息为10次实验6次正面的情况下，后验分布 $P(𝜃=0.4|X=6)=0.056$ $P(𝜃=0.5|X=6)=0.819$ $P(𝜃=0.6|X=6)=0.125$ 后验分布的期望 $$E(𝜃|X=6)=0.4\cdot0.056+0.5\cdot0.819+0.6\cdot0.125=0.5069$$ 可以看到，后验分布的概率变得比0.5大了，而且并没有大很多。由于这里先验分布取的很特殊，哪怕出现无穷多次都是正面朝上的情况，后验分布的期望也只会逼近0.6。 [1]Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation[J]. Journal of machine Learning research, 2003, 3(Jan): 993-1022.]]]></content>
      <categories>
        <category>机器学习模型</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[今天开始写博客]]></title>
    <url>%2F2017%2F11%2F07%2F%E4%BB%8A%E5%A4%A9%E5%BC%80%E5%A7%8B%E5%86%99%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[忙了两天，总算把个人博客搭起来了 ¶总之今天开始写博客了 这里看起来可能很奇怪，恩，因为就是一个MarkDown的写法的练习。 说实话我才敢刚开始写，还不太习惯 万事开头难。 ¶从今天开始写博客 预期每周一篇 包括但不限于以下题材 技术相关，机器学习编程啥的，某一个具体部分的详细介绍 读书笔记 文献阅读笔记 个人思考 一周至少300字吧，不算多，写个计划可能都不止300字了（笑）。]]></content>
      <categories>
        <category>Whatever</category>
        <category>日记</category>
      </categories>
      <tags>
        <tag>Whatever</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F11%2F06%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. ¶Quick Start ¶Create a new post 1$ hexo new "My New Post" More info: Writing ¶Run server 1$ hexo server More info: Server ¶Generate static files 1$ hexo generate More info: Generating ¶Deploy to remote sites 1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>Whatever</category>
      </categories>
  </entry>
</search>
