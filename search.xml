<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[《隐性逻辑》读书笔记]]></title>
    <url>%2F2017%2F11%2F19%2Fimplicit-logic%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[A novel approach to identify the major research themes and development trajectory: The case of patenting research]]></title>
    <url>%2F2017%2F11%2F15%2Fpaper6%2F</url>
    <content type="text"><![CDATA[作者：Louis Y.Y. Lu, John S. Liu 年份：2015 期刊：Technological Forecasting &amp; Social Change 研究内容：Identify the major research themes. 文章在WOS上检索了关于“patent”的论文，建立引文网络，通过edge-betweenness clustering technique和key-route main path analysis are两种方法来确立主要研究主题。提出的方法可用于发现一个领域的主要研究主题并绘制发展轨迹。数据需要有citation relationships。 这篇文章的国内外研究现状是穿插在小节中，而不是独立在某一个chapter中写的。其实我个人比较认同这种引用文献的方式，但在TF &amp; SC的其他论文中似乎比较少见。 ¶关键技术 ¶Edge-betweenness clustering 删除在不同的Groups之间edge来发现’community’。Edge-betweenness是针对边而言的，定义为“顶点对间途径该边的最短路径数量”(The edge-betweenness of an edge is defined as ‘the number of shortest paths between pairs of vertices that run along it’)。 本文是直接用的Csardi, G., Nepusz, T., 2006. The igraph software package for complex network research. Int. J. Complex Syst. 1695开发的软件实现Edge-betweenness clustering。 算法的大致步骤如下： First,calculate the betweenness for all edges in the network; second, remove the edge with the highest betweenness; third, recalculate the betweenness for all edges affected by the removal; fourth, repeat from step 2 until no edge remains. ¶Key-route main path analysis 寻找有高遍历权重（traversal weight）的边top significant link，从头结点和尾结点展开搜索，搜索的links和这条link构成一个Path。所有Path的集合叫做Key-route main path。这里解释不是很详细，不过作者本身也只是用了别人的工具。 本文使用Pajek软件直接生成主路径的。Batageij, V., Mrvar, A., 1998. Pajek — program for large network analysis. Connections 21 (2), 47–57. ¶数据 1971-2013在WOS上与Patent相关的论文。完成后进行了简单的统计。 Author statistics.The authors are ranked by g-index and h-index. Journal statistics。top 20 influential journals that have published the most research papers in patenting according to their g-index and hindex. ¶结果 通过edge-betweenness clustering得到major research groups 通过word cloud判断group topic 通过key-route main path analysis来找寻gruops之间的关联和整体的发展轨迹。 ¶Clustering in patenting 一个整体统计加上每一个Gruop的词云，本来就只有12页的论文基本上都是图和表。😂😂😂 八类分别是： citation network analysis patent law patent valuation academic patenting gene patenting patent policy patent protection technology analysis Table 1.Statistics of 8 clusters Cluster 1 2 3 4 5 6 7 8 Active years 1983–2013 1977–2013 1986–2013 1978–2013 1991–2013 1985–2013 1989–2013 1994–2013 No. of papers 609 410 399 248 192 171 154 138 Papers /year 19.64 11.08 14.25 6.89 8.73 5.90 6.16 4.6 ¶Overall development 节点是论文。 后面的group需要根据之前聚类的结果人为标注。 ¶借鉴意义 我个人是觉得把国内外研究现状穿插在具体的方法选择部分可能更好。 聚类方式本身没有太大的创新，但是可以考虑之后也利用一些引文信息作为聚类的辅助判断标准。 自动生成Main Path或许是一个应用点？ A novel approach to identify the major research themes and development trajectory: The case of patenting research]]></content>
      <categories>
        <category>文献阅读</category>
        <category>Technology foresight</category>
      </categories>
      <tags>
        <tag>Technology foresight</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Topic-driven Multi-type Citation Network Analysis]]></title>
    <url>%2F2017%2F11%2F10%2Fpaper5%2F</url>
    <content type="text"><![CDATA[作者：Yang Z, Hong L, Davison B D. 年份：2010 期刊：Adaptivity, Personalization and Fusion of Heterogeneous Information. 研究内容：Focus on the task of ranking authors.以往的研究已经使用了content-based approaches或者citation network link analyses,但两者的结合却很少。本文提出一种新的概率模型，模型结合了基于内容的方法和多种引用网络（文章之间，作者之间，隶属关系，出版场所），并且提出了heterogeneous PageRank random surfer model来反映不同特征的重要性。 本文贡献： Proposing a novel probabilistic model which combines content-based analysis with a multi-type citation network, integrating relationships of authors, papers, affiliations and publishing venues in one model. This model can be extended to include more types of social factors. Proposing a heterogeneous PageRank random surfer model compared with the original uniform PageRank model, to reflect the impact among different factors. Introducing topical link analysis into citation network analysis. In particular, Topical PageRank [18] is adopted for citation link analysis. A comparative study using ACM digital library data on various PageRank extensions as well as different complexity of citation networks. 说实话，他号称的Combine就是把俩结果用一个权值加起来，总有一种上当受骗的感觉…🙃🙃🙃 ¶主要方法： ¶构建网络图 ¶4-T graph version-1 四个Factor，四副sub-graph。 Author Graph GAu。 节点是作者。如果两个作者共同写过一篇文章或者一个引用过另一个，两个作者间存在连线。单线，无权。（下同） Paper Graph GP。 节点是文章，连线是引用。 Affiliation Graph GAf。 节点是机构，如果两个机构下至少有一对作者有连线，两机构连线。 Venue Graph GV。节点是出版社，如果两个出版社下至少有一对文章有连线，两出版社连线。 子网间的连线：根据各自的关系，有一度连接的都连起来。 作者连线发表文章 作者连线属于机构 作者连线发表文章出版社 文章连线作者机构 文章连线出版社 机构连接下属作者发表文章对应的出版社。 ¶4-T graph version-2 框架1存在冗余。设计框架2，子图和图内连接方式相同，作者只和文章与机构连线，文章之和作者和出版物连线。 ¶Topical PageRank求解 整张图看作一个大的网络，使用Pagerank进行排序。不同于均匀的PageRank的是，节点间的传播概率是不同的。相同类型的两个节点间的传播概率相同。 基于Topical PageRank来实现content-based analysis。每一个节点对应一个Page，包括代表这个节点的T个主题的分布组成的主题向量和代表这个节点在每个主题上的重要程度的authority向量。random surfer model，游走求解。 ¶Combine content-based approach with citation network Okapi BM25：In information retrieval, Okapi BM25 (BM stands for Best Matching) is a ranking function used by search engines to rank matching documents according to their relevance to a given search query. content-based approach就是用Okapi BM25算一个结果，citation network就是前面的Topical PageRank，Combine就是把他俩用一个权值加起来。 这就是你说的Combine？你在逗我？🙂 三种方式来评价结果和query的匹配程度（你上面不是说不用于检索吗？） In the first approach, we collected all the PC members in the related conferences for each research area during 2008 and 2009. In the second approach, we collected all the ACM fellows, ACM distinguished and senior members provided from the ACM website. We utilized human judgements to generate relevant lists in the third approach. In our evaluation system, the top ten and twenty returned authors by various ranking algorithms were retrieved and mixed together. ¶实验结果 评价指标：NDCG（网页中用于衡量网页排序质量的指标） 越后面介绍的方法指标更好。 参数的比较。 node2vec: Scalable Feature Learning for Networks]]></content>
      <categories>
        <category>文献阅读</category>
        <category>Technology foresight</category>
      </categories>
      <tags>
        <tag>Technology foresight</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Social Influence Analysis in Large-scale Networks]]></title>
    <url>%2F2017%2F11%2F10%2Fpaper4%2F</url>
    <content type="text"><![CDATA[作者：Jie Tang 年份：2009 期刊：KDD 研究内容：区分不同angle(topic)上的社会影响，量化影响的大小。 propose Topical Affinity Propagation (TAP) to model the topic-level social influence on large networks. ¶主要思想 在主题层面（topic level）上利用亲和力传播（affinity propagation）来进行社会影响鉴定。 TAP provides topical influence graphs that quantitatively measure the influence on a fine-grain level; The influence graphs from TAP can be used to support other applications such as finding representative nodes or constructing the influential subgraphs; An efficient distributed learning algorithm is developed for TAP based on the Map-Reduce framework in order to scale to real large networks. ¶数据 网络和节点下的主题分布。一个作者共现网络，一个引用网络，一个film-director-actor-writer网络 ¶结论 相较于文本相似度有两个优势1.可以分析来年各个节点互相影响的差异。2.可以计算整个网络中影响力最强的节点。 借鉴意义：可对社交网络中用户节点转化成向量，然后计算用户节点与相连节点的相似性，作为亲密度度量的手段。 Social Influence Analysis in Large-scale Networks]]></content>
      <categories>
        <category>文献阅读</category>
        <category>Social Networks</category>
      </categories>
      <tags>
        <tag>Social Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Identifying potentially disruptive trends by means of keyword network analysis]]></title>
    <url>%2F2017%2F11%2F10%2Fpaper3%2F</url>
    <content type="text"><![CDATA[作者：Fefie Dotsika, Andrew Watkins 年份：2017 期刊：Technological Forecasting &amp; Social Change 研究内容：通过词共现生成词网络，一系列的词语代表一个技术点。由词网路各种中心性的对比变化分析技术的破坏性趋势。 ¶关键术语含义 有价值技术：In an ever-changing technological landscape where innovation is a crucial driver for economic growth and survival, it is desirable to be able to predict which technologies, when established, have the potential to revolutionise an industry, create new markets, and increase accessibility and affordability. Disruptive innovation定义：Disruptive innovation is defined as the process of transforming a product or service that historically has been accessible at the top of a market access (i.e. for a high price or specialised skill-set) to become accessible to a new and larger population of consumers at the bottom of that market Maturing trends were found to share influential common topics identified by high degree, betweenness and closeness centrality scores.Niche and potentiallyemerging trends within groupswere detected by means of eccentricity and farness metrics. ¶使用数据 WOS文献数据 Forrester,Frost &amp; Sullivan, Gartner, IDC and Ovum.五种Business reports ¶分析方法 ¶1.分析各个领域的数量分布。 结论：没有明显证据表明学术出版物总是超前于商业出版物。 ¶2.网络结构与特征分析。 网络的节点是词，连线表示两个词在同一个出版物中共现的次数总和，使用UCNET等工具制作。 网络结构指标，包括network size，density，diameter，average degree等。 聚类与子网络指标，包括coefficient，Erdös number，average embeddedness，modularity等。 结论： 所有网络呈现low density和high clustering coefficients，与随机网络差异明显 Positive modularity values Erdös number is low ¶3.节点位置分析（找出已经是disruptive 的技术） Degree centrality，指标越高，语义重要性越中心。 Eigenvector centrality，扩展了Degree centrality中心性的概念，与上类似。 Betweenness centrality，起到桥梁作用的次数越多，在信息流中更有影响力。 Closeness centrality，与网络中其他所有点的距离和。 Eccentricity，距离最远点的距离。 提取degree、eigenvector、betweenness 、closeness四项中最高的两个词作为main keyword，通过可视化验证了这些词确实处于central的位置。 根据不同的中心性组合得到不同的disruptive技术类型。 如High degree- Low betweenness，表示Popular mature keyword. ¶4.成熟前位置分析（找出可能成为disruptive 的技术） 有high closeness 和 low degree 的技术最有成为disruptive 的技术的趋势。 列举Twitter`s popularity等一系列网络例子来证明这种趋势的存在。 借鉴意义： 网络形成后的趋势分析方法。 例证方式。 Identifying potentially disruptive trends by means of keyword network analysis]]></content>
      <categories>
        <category>文献阅读</category>
        <category>Technology foresight</category>
      </categories>
      <tags>
        <tag>Technology foresight</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Survey on Social Community Detection]]></title>
    <url>%2F2017%2F11%2F10%2Fpaper2%2F</url>
    <content type="text"><![CDATA[作者：Michel Plantie, Michel Crampes 年份：2013 期刊：Social media retrieval. Springer London 研究内容：对以往的Social Community Detection研究进行了总结，得出三种常见的分析方法。 The first approach considers the social network as a graph and then analyzes its structure with graph properties and algorithms built around the graph structure. The second approach associates the social network with a hypergraph and analyzes its structure through hypergraph properties and algorithms based on hypergraph structures. The third approach uses the properties of concept lattices in order to analyze the social network structure in association with hypergraph properties and algorithms based on Galois lattices and hypergraph structures. 总结很详细，不过没读完。 ¶关键对象说明 图(Graph)：node和edge构成，edge只连接两个nodes。 超图(Hypergraph)：node和hyperedges构成，hyperedges可以连接多个nodes，一个hyperedges内包含的就是一个community。 Galois点阵（Galois lattice）Individuals sharing the same subset of properties define a community. row是属性，column是对象，画出一个图，包含先沟通的子集的对象被认为是一个community。 Survey on Social Community Detection]]></content>
      <categories>
        <category>文献阅读</category>
        <category>Social Networks</category>
      </categories>
      <tags>
        <tag>Social Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Node2vec: Scalable Feature Learning for Networks]]></title>
    <url>%2F2017%2F11%2F10%2Fpaper1%2F</url>
    <content type="text"><![CDATA[作者：Aditya Grover，Jure Leskovec 年份：2016 期刊：KDD 研究内容：Network Embedding。网络的特征学习和向量化表达，在multi-label classification和link prediction两个方面得到了有效性验证。 七月份精读过，不过当时没有记录太多。是一片适合作为random surfer model的入门之作。 主要方法： ¶Rrandom walk neighborhood sampling 获得圈子训练集。 当从节点t移动到节点v时，v移动到下一个节点x的概率由节点t到节点x的距离决定。 ¶Skip-gram训练得到的Embedding模型。 训练时，输入时初始的node u，输出的random walk采样到的neighbor nodes。类似于word2vec 中的输入时一个词，输出是上下文的词进行训练。 ¶结果展示 可以根据选择不同的步长和概率参数得到不同的结果，如发现社交圈子（上图），或者发现在网络结构中处在类似地位的节点（下图）。 借鉴意义：可对社交网络中用户节点转化成向量，然后计算用户节点与相连节点的相似性，作为亲密度度量的手段。 node2vec: Scalable Feature Learning for Networks]]></content>
      <categories>
        <category>文献阅读</category>
        <category>Social Networks</category>
      </categories>
      <tags>
        <tag>Social Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo Next如何置顶文章]]></title>
    <url>%2F2017%2F11%2F10%2Ftop-post%2F</url>
    <content type="text"><![CDATA[¶安装hexo-math 参考Netcan_Space博客 安装hexo-generator-index-pin-top，然后在Front-matter中加上top: true即可. 在\themes\next\layout\_macropost.swig下修改修改相应的效果。 Next主题自带Frongt-matter中使用Sticky: true的方式添加指定效果，但是在这篇博文写作的时候该方法只会将文章置顶到某一个页面的最上方。如果你的首页博客是一页最多显示10篇，而从时间上你要指定的文章排在第15篇，那么设定sticky会导致你的文章在/page/2中被置顶。 本博客使用的置顶效果是在post-sticky-flag和post-title-link之间加入 12345&#123;% if post.top &gt; 0 %&#125; &lt;span class=&quot;post-top-flag&quot; title=&quot;top&quot;&gt; &lt;font color=&quot;red&quot;&gt;[置顶]&lt;/font&gt; &lt;/span&gt;&#123;% endif %&#125;]]></content>
      <categories>
        <category>问题梳理</category>
      </categories>
      <tags>
        <tag>Problem</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo写博客编辑公式相关问题]]></title>
    <url>%2F2017%2F11%2F08%2Fhexo-mathjax%2F</url>
    <content type="text"><![CDATA[¶安装hexo-math hexo-math的github有详细的教程，主要就是两点。 一是安装 npm install hexo-math --save 二是在站点_config.yml中配置 1234567891011math: engine: 'mathjax' # or 'katex' mathjax: src: custom_mathjax_source config: # MathJax config katex: css: custom_css_source js: custom_js_source # not used config: # KaTeX config 值得一提的是，如果你是用的是Next主题，那么你还需要在\themes\next\下的_config.yml中配置启用mathjax，否则是不会生效的。（我这里弄了好久，还以为各种装错了，最是还是看了参考链接才找到问题） mathjax: enable: true ¶多行公式 如果使用的是hexo-math,写多行公式要使用 {% math %} \begin{align} \end{align} {% endmath %} 的方式。Latex语法\\换行，&amp;打在等号前进行对齐。特别需要注意的是，换行时要写\\\\，因为\\会被认为是转义字符\。 如 {% math %} \begin{align} y &= x_1+x_2 \\\\ &=1+2 \\\\ &=3 \end{align} {% endmath %} 的显示效果是 $$\begin{align} y &amp; = x_1+x_2 \\\\ &amp; =1+2 \\\\ &amp; =3 \end{align}$$]]></content>
      <categories>
        <category>问题梳理</category>
      </categories>
      <tags>
        <tag>Problem</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《自控力-和压力做朋友》读书笔记]]></title>
    <url>%2F2017%2F11%2F07%2FSelf-Control-Pressure%2F</url>
    <content type="text"><![CDATA[《自控力-与压力做朋友》（The Upside of Stress） 作者：[美] 凯里·麦格尼格尔 Kelly McGonigal 北京联合出版社2016年3月第1版 末尾附思维导图下载链接 To do something special, you just have to believe it`s special. 人们为了减少吸烟，就在烟盒上画着吸烟者千疮百孔的肺，期望能警醒人们减少吸烟。可惜的是，人们看到这些吸烟者的肺，压力变得更大，有什么是比抽烟更能缓解压力的呢？于是人们抽了更多的烟，又给自己带来了更多的压力。 《自控力-与压力做朋友》Kelly McGonigal的第二本关于自控力的书。（JD上买错了才发现还有第一本…） 整本书的中心思想是：改变对压力的看法，会使你更健康和幸福。文中通过大量的社会学实验作为论据来证明论点。内容的展开都是从实验的角度出发，探讨面对压力时不同的态度导致的不同的生理性反映，干预和对比来说明实验的合理性。大量的例证让论证看起来很丰富，但缺少体系的结构设计和略显重复的实验内容让人看起来也有些疲惫。 但看结论的话，有点像鸡汤。不过书中除了大量实验佐证之外，还提供了很多帮助你改善思维习惯的小练习，许多实验的设计也可以作为调整的参考，值得一读。 ¶第一部分 重新思考压力 ¶第一章——改变我们的思维模式：什么是压力？压力真的都是负担吗？ 过去人们总在说，压力是有害的，要缓解甚至消除压力。 但压力真的都是有害的吗？你是否在面对面对期末考试的压力时更高效的学习？是否在完全没有压力的假期中感到无聊和疲倦？是否在压力过后，感觉自己得到了成长？ 或许让我们难以承受的不是压力本身，而是我们对待压力的态度。 ¶第二章——身处困境时，压力是可以依靠的资源，而非要消灭的敌人 比赛开始前，运动员们血脉喷张，心跳加速，身体新成代谢加快，他们告诉自己，我很兴奋，我想要大干一场。 比赛开始前，学生们血脉喷张，心跳加速，身体新成代谢加快，他们告诉自己，我很紧张，我得找个地方冷静一下。 其实压力可以给我们带来动力，让我们更集中，以更好的状态面对难题。 ¶第三章——压力和意义成正比：有意义，意味着有压力 寒假每天躺在床上，什么也不干，玩玩手机，吃了睡，睡了吃，这样的生活多好啊~ 其实一点也不好，每天反而很累，而且觉得无趣。 丢失了压力的生活，也丢失了意义 ¶第二部分 转化压力 ¶第四章——全身心投入：拥抱焦虑能帮助你更好的应对挑战 试着让自己觉得兴奋而不是紧张。 拥抱压力，你能获得勇气。 ¶第五章——内在联结：压力能使人更具关怀性，提升抗挫力 感到压力的时候，我们会倾向于和人交流我们的压力，来缓解自己的情绪；同样的他人和我们交流他们的压力时，如果我们有过类似的压力，就能带去更多的关怀。 怀着宏大的目标，崇高的使命感也能让人更有勇气面对压力。 Life is always hard. Everybody knows. ¶第六章——幸福成长：痛苦使你坚强，即使痛苦正当下，未来尚模糊 经历过中等苦难的人，更容易成长，变得有毅力，身体更加康，幸福感也更高。不是说要支持去伤害自己，而是学会把痛苦变成自己的资源，让自己更强大。 Everything that kills me, makes me feel alive. **结语：**作者上一本书《自控力》，讲的就是如何减少拖延，然而新书还是拖了很久才出版，可见哪怕写了这么多，作者自己不是立马就成了完人。思维不是那么容易变得，但人总成长。至少我今日写下了这篇博客，我在记录，我在改变。 [自控力思维导图下载](http://aroslin.github.io/downloads/The Upside of Stress.xmind)]]></content>
      <categories>
        <category>阅读笔记</category>
      </categories>
      <tags>
        <tag>Psychology</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始学LDA（Latent Dirichlet allocation）]]></title>
    <url>%2F2017%2F11%2F07%2FLDA-From-Zero%2F</url>
    <content type="text"><![CDATA[去年初学LDA，看完了Rickjin老师的《LDA数学八卦》，觉得不是很懂，查阅了很多资料之后才对LDA有了更深入地认识。一直想加入自己的理解后更简单的讲述这个模型，今日补上。 注：文章的行文思路与大多数公式参考了Rickjin《LDA数学八卦》。有兴趣更深入了解的同学可以在看完本篇后继续阅读。 LDA(Latent Dirichlet allocation)是主题模型的一种，最早是由Blei D M[1]等人在2003年提出的，常被用于文档的主题识别。时至今日，在大大小小的会议上仍能看到不少对LDA（或其衍生、或类似的主题模型）提出改进的文章。其自然的思路，优雅的求解和拓展性强的模型结构，实在是非常适合作为对无监督学习一个入门模型来学习和改进。 本文主要介绍LDA背后的相关数学知识和模型的构建与求解思路，力求只有少量统计知识的初学者也能看懂。文中略去了大量的公式推导细节，如果希望深入的了解，请根据需要查找相关的资料。 ¶从扔硬币说起 介绍模型之前，我们先来简单回顾一些知识点。本章主要讲解二项分布，贝叶斯理论，beta分布，multi分布和Dirichlet分布。如果你熟悉这些知识点，可以直接跳过这一章。 ¶二项分布 二项分布（英语：Binomial distribution）是n个独立的是/非试验中成功的次数的离散概率分布，其中每次试验的成功概率为p。这样的单次成功/失败试验又称为伯努利试验。实际上，当n = 1时，二项分布就是伯努利分布。二项分布是显著性差异的二项试验的基础。 摘自二项分布维基百科 扔一枚硬币，一共扔N次，其中k次都是正面朝上的概率是多少？ 这个概率服从二项分布(Bernoulli distribution) $$P(k|N,p)=C^k_N\cdot p^k\cdot (1-p)^{N-k}$$ 其中p是硬币扔一次正面朝上的概率。真实场景中，我们面对的问题往往是，我想知道这个公式中的p是多少。一个我们熟悉且自然的思路是，把这个硬币扔N次，得到的结果中k次朝上的那么可以估计 $$p=\frac{k}{N}$$ 如果你学过概率论，那么你就知道这是对二项分布中参数p的一个极大似然估计。 ¶贝叶斯估计 当我们像上面去估计p值的时候，其实已经默认了一个假设，就是p是一个定值，它在每次实验中都是相同的。p是一个客观存的量，只是我们不知道它是多少。这样的想法很容易理解，但有时候会带来一些问题。当我们扔了10次硬币，10次都朝上的时候（尽管概率很小，但这一现象也会发生），用上面的方法估计，p=1。这一结论和我们的常识不符，当全部出现正面朝上这种小概率事件时，我么可能会倾向认为p值比0.5大，但没有大到等于1。 这时候，有些人就想，或许p并不是一个定值，而是服从某一种概率分布，对p的一个较好的估计是这个概率分布的期望，那么即使发生了每次都正面朝上的情况，只要这个概率分布不是始终等于1，就不会出现违背常识的情况了。 怎么理解？这就好比说，你扔出的硬币在落下的一瞬间之前，在你无法感知的空间中散落成无数的硬币，这些硬币的密度不尽相同，也就是落下时的p是不同的，但这些p是的有约束的，都是服从某一概率分布。在降落的一瞬间，有且仅有一枚硬币投射到你感知到的硬币上，也就是这一次抛硬币对应的p。由于无数硬币的p值并不是都等于1，所以即使出现扔硬币多次全部正面朝上，也可以理解成这些硬币中p值较大的硬币比较多，而不会是所有硬币的p都等于1。 如果你觉得上面这略显中二的说法也不太好理解的话😑，我们就直接看知识点好了。根据伯努利试验中p值是否固定，划分出了两个学派——频率学派和贝叶斯学派。这两个学派的思路差异主要体现在： 频率派把需要推断的参数θ看做是固定的未知常数，即概率虽然是未知的，但是确定的一个值，同时，样本X是随机的，所以频率派重点研究样本空间，大部分的概率计算都是针对样本X的分布； 贝叶斯派的观点则截然相反，他们认为参数是随机变量，而样本X 是固定的，由于样本是固定的，所以他们重点研究的是参数的分布。 在贝叶斯学派看来，所有的参数都不是固定值，而是服从一个概率分布，而对这个概率分布的估计有一个经典的模式： 先验分布𝜋(𝜃)+样本信息Χ⇒后验分布𝜋(𝜃|𝑥) 上述思考模式意味着，新观察到的样本信息将修正人们以前对事物的认知。换言之，在得到新的样本信息之前，人们对的认知是先验分布$𝜋(𝜃)$，在得到新的样本信息Χ后，人们对的认知为$𝜋(𝜃|𝑥)$。而这里求解$𝜋(𝜃|𝑥)$最重要依据就是贝叶斯公式（条件概率公式） $$P(𝜃,𝑋)=P(𝜃│𝑋)∙P(𝑋)=P(𝑋|𝜃)∙P(𝜃)$$ 举个例子来说明两个学派分析的思路差异好了。加入我们扔一枚硬币，连续扔10次，有6次出现了正面朝上，那么扔一次正面朝上的概率是多少？ 按照频率学派的观点， $$p=\frac{6}{10}=0.6$$ 按照贝叶斯学派，首先p有一个先验分布，也就是你对p的一个经验估计，为了计算简单和表达的方便，咱们这里取一个离散分布,令$𝜃=p=0.4,0.5,0.6$，且 $P(𝜃=0.4)=0.1$ $P(𝜃=0.5)=0.8$ $P(𝜃=0.6)=0.1$ 先验分布的期望 $$E(𝜃)=0.4\cdot0.1+0.5\cdot0.8+0.6\cdot0.1=0.5$$ 根据样本信息，10次中有6次朝上，由贝叶斯公式变形可以得到，在该样本条件下，$𝜃=6$的概率为 $$𝑝(𝜃=0.6│𝑋=6)=\frac{𝑝(𝑋=6|𝜃=0.6)\cdot𝑝(𝜃=0.6)}{𝑝(𝑋=6)} $$ 其中： $𝜃$就是抛一次硬币是正面的概率，X是正面的次数，在本例中就是6； $𝑝(𝜃=0.6│𝑋=6)$就是在6次正面的情况下，$𝜃=0.6$的概率，即后验概率； $𝑝(𝑋=6|𝜃=0.6)$是𝜃=0.6时可以抛出6次正面的概率，即样本信息，这里求法按二项分布来求； $𝑝(𝜃=0.6)$就是𝜃的先验分布，此例子中$𝑝(𝜃=0.6)=0.1$； $𝑝(𝑋)$称为Normalizer，或者叫做marginal probability，是一个定值，$𝜃$分布如果是离散的，就是$𝑝(𝑋|𝜃)\cdot 𝑝(𝜃)$在所有𝜃情况下的和；如果$𝜃$是连续的，$𝑝(𝑋)=\int𝑝(𝑋|𝜃)\cdot 𝑝(𝜃)𝑑𝜃$。 我们就可以分别求出在样本信息为10次实验6次正面的情况下，后验分布 $P(𝜃=0.4|X=6)=0.056$ $P(𝜃=0.5|X=6)=0.819$ $P(𝜃=0.6|X=6)=0.125$ 后验分布的期望 $$E(𝜃|X=6)=0.4\cdot0.056+0.5\cdot0.819+0.6\cdot0.125=0.5069$$ 可以看到，后验分布的概率变得比0.5大了，而且并没有大很多。由于这里先验分布取的很特殊，哪怕出现无穷多次都是正面朝上的情况，后验分布的期望也只会逼近0.6。 ¶Beta分布与共轭 上面的例子中，我们选取了一个离散分布作为先验分布，可以看出这个离散分布限定了p只能取三个值，得到的后验分布期望所在的区间也十分有限，选取这个分布并不太合适。数学家们给出了一个很漂亮的分布——Beta分布，来作为二项分布的先验分布。为什么选取Beta分布呢？主要是因为两点： Beta分布形态多变，可以满足多种情况下的分布。 Beta分布做先验分布，样本信息服从二项分布时，两种分布符合良好的特性可以简化计算。 为了更好的理解Beat分布与二项分布的关系，我们先来看一下Beta分布的表达式。 $$p(\theta)=Beta(\theta|\alpha,\beta)=\frac{\theta^{\alpha-1}\cdot (1-\theta)^{\beta-1}}{B(\alpha,\beta)},$$ $$B(\alpha,\beta)=\int^1_0 t^{\alpha-1} (1-t)^{\beta-1}dt$$ 公式中$\alpha$和$\beta$是Beta分布的两个超参数，通过调整$\alpha$和$\beta$的大小可以调整Beta分布的形态，如下图所示。 由图像我们不难看出，改变$\alpha$和$\beta$可以得到丰富的Beta分布的形式，得到不同的期望与分布的均匀程度，可以满足很多的分析场景。这是我们提到的选取Beta分布作为先验分布的第一点原因。 那么，它与二项分布的关系呢？我么不妨来求一下先验分布是Beta分布，样本服从二项分布的情况下，后验分布是什么样子的。如果先验分布是Beta分布 $$p(\theta)=Beta(\theta|\alpha,\beta)=\frac{\theta^{\alpha-1}\cdot (1-\theta)^{\beta-1}}{B(\alpha,\beta)}$$ 样本服从二项分布 $$P(X|\theta)=C^k_N\cdot \theta^k\cdot (1-\theta)^{N-k}$$ 那么根据贝叶斯公式，后验分布满足 $$\begin{align} p(\theta|X) &amp; =\frac{p(X|\theta)\cdot p(\theta)}{p(X)}\\\\ &amp; =\frac{p(X|\theta)\cdot p(\theta)}{\int p(X|\theta)\cdot p(\theta)d\theta}\\\\ &amp; =\frac{(C^k_N\cdot \theta^k\cdot (1-\theta)^{N-k})\cdot(\frac{\theta^{\alpha-1}\cdot (1-\theta)^{\beta-1}}{B(\alpha,\beta)})}{\int (C^k_N\cdot \theta^k\cdot (1-\theta)^{N-k})\cdot (\frac{\theta^{\alpha-1}\cdot (1-\theta)^{\beta-1}}{B(\alpha,\beta)})d\theta}\\\\ &amp; =\frac{\frac{C^k_N}{B(\alpha,\beta)}\cdot \theta^{\alpha+k-1}\cdot (1-\theta)^{\beta+N-k-1}}{\frac{C^k_N}{B(\alpha,\beta)}\cdot \int \theta^{\alpha+k-1}\cdot (1-\theta)^{\beta+N-k-1}d\theta}\\\\ &amp; =\frac {\theta^{\alpha+k-1}\cdot (1-\theta)^{\beta+N-k-1}}{B(\theta|\alpha+k,\beta+(N-k))}\\\\ &amp;= Beta(\theta|\alpha+k,\beta+(N-k)) \end{align}$$ 得到的仍然是一个Beta分布！也就是说，在先验分布是Beta分布的情况下，如果样本服从二项分布，那么后验分布也是Beta分布。在贝叶斯估计中，将这种根据样本求得的后验分布与先验分布的分布形式一致时，样本服从的分布与先验分布共称为共轭分布。将上面的式子我们简化的记为 $$Beta(\theta|\alpha,\beta)+BinomCount(k,N-k)=Beta(\theta|\alpha+k,\beta+N-k)$$ 共轭是一个非常好的特性。在之前的先验分布是离散分布的例子中，我们需要把每一步都求解出来才能知道后验分布的期望是多少，但是如果先验分布是Beta分布，我们只需要会求Beta分布的期望，就能很快的得到后验分布的期望了。 事实上，$Beta(\theta|\alpha,\beta)$的期望非常好求（这里就不推导公式了） $$E(Beta(\theta|\alpha,\beta))=\frac{\alpha}{\alpha+\beta}$$ 等下，这个结果是不是有点眼熟？如果我们另$\alpha=k,\beta=N-k$，在扔硬币的试验中，Beta的期望，也就是对p值的估计就变成了 $$E(Beta(\theta|k,N-k))=\frac{k}{N}$$ 和频率学派对p值得估计统一起来了！从这里我们可以看出超参数$\alpha,\beta$的物理意义实际上就是扔硬币实验中正面朝上负面朝上的次数。根据我们的经验，我们认为p值应该是0.5，那么我们可以设先验Beta分布中$\alpha=\beta=1$，十次实验全部正面朝上，后验Beta分布的期望，也就是对p值的估计$E(\theta)=11/12$，一个接近1但不等于1的数。甚至，如果你对p值等于0.5非常有自信，你可以先验Beta分布中$\alpha=\beta=100$，那么十次实验全部正面朝上的话，$E(\theta)=101/102$，对p值的估计仍在0.5左右。记住这一点，对理解共轭的特性和Beta分布的含义有较大的帮助。 Beta分布是分布的分布，它与二项分布共轭。在Beta分布中，超参数$\alpha,\beta$物理含义是伯努利试验中两种结果发生的次数，它们比值决定了Beta分布的期望，他们的大小决定了样本对后验分布的影响程度。 ¶多项分布与Dirichlet共轭 好的，下面我们来把二项分布推广吧。想象硬币就是一个“二面体”，它的每一面朝上的事件都会发生，而且发生的概率并不相同，对二项分布 $$P(k|N,p)=C^k_N\cdot p^k\cdot (1-p)^{N-k}$$ 令$x_1=k$，$x_2=N-k$，$p_1=p$，$p_2=1-p$，$\vec{x}=(x_1.x_2)$，$\vec{p}=(p_1,p_2)$那么上式可以改写为 $$p(\vec{x}|N,\vec{p})=\frac{N!}{x_1!\cdot x_2!}\cdot p^{x_1}_1\cdot p^{x_2}_2$$ 这里式子的形式已经很清楚了。如果我们抛一个k面的凸多面体，一共抛N次，每个面朝上的次数为$\vec{x}$，每个面朝上的概率为$\vec{p}$,那么对应的概率就是多项分布 $$p(\vec{x}|N,\vec{p})=\frac{N!}{\prod_i^ kx_i}\cdot \coprod_{i}^ {k}p ^{x_i}_i$$ 与多项分布共轭的分布就是Dirchlet分布 $$p(\vec{x})=Dir(\vec{x}|\vec{\alpha})=\frac{1}{\Delta (\vec{\alpha})}\coprod_{i}^ {k}x_i ^{\alpha_i-1} \Delta (\vec{\alpha})=\int\coprod_{i} ^ {k}x ^{\alpha-1}_id \vec {x}$$ Dirchlet分布与多项分布的共轭也满足我们之前说的漂亮的性质 $$Dir(\vec{x}|\vec{\alpha})+MultCount(\vec{m})=Dir(\vec{x}|\vec{\alpha}+\vec{m})$$ 而Dirchlet分布的期望等于 $$E(\vec{p})=(\frac{\alpha_1}{\sum_{k}^{i}\alpha_i},\frac{\alpha_2}{\sum_{k} ^ {i}\alpha_i}, \ldots ,\frac{\alpha_N}{\sum_{k} ^{i}\alpha_i})$$ 多项分布与Dirihlet分布是整个LDA模型构建的基础。了解它们的特性对于理解和改进LDA模型会有很大的帮助。至于LDA模型到底是什么，我们下节继续。 ¶Latent Dirichlet allocation OK，正餐来了。那么什么是LDA呢？让我们从Unigram Model开始说起。 ¶Unigram Model 一篇文档是由许多词语构成的，事实上，抛开语法结构不谈，如果一篇文章是由一些没有顺序的词语构成，我们仍能从一定程度理解文档在探讨的主题。就好像那句有趣的话一样 研表究明，汉字序顺并不定一影阅响读 那么我们不妨将文档看做是一些词的集合，那么最简单的生成文档的方式就是，先选定这篇文档的词语个数N，然后选N个词出来。这一过程就好像是我们有一个巨大的多面体，它有字典里的词那么多面，每一面对应一个词。当我们要创作一篇有N个词的文档时，我们就把这个多面体扔N次，然后文档就生成了。 是的，上面这个简单的文档生成模型就叫做Unigram Model。它所对应的一篇文档的生成概率就是一个多项分布，根据词语的数量和总词数就可以估计每个词出现的概率。 Unigram model中文档生成的过程如下 Unigram model文档生成过程 假设：只有一个多面体，这个多面体有N面，每个面代表一个词；每个面的朝上的概率不尽相同。 过程： 1.选定文档的词语数量N； 2.抛掷多面体，记录朝上的面对应的词； 3.重复步骤2， N次，直到所有词都生成。 当然，别忘记了贝叶斯估计，如果给这个多面体每个面朝上的概率一个先验分布$Dir(\vec{p}| \vec{\alpha})$，那么就可以轻松地根据样本信息 $\vec{n}$ 来估计后验分布$Dir(\vec{p}| \vec{\alpha}+\vec{n})$了。它的概率图模型如下图所示。 这里稍微提一下概率图模型。我没系统学过，面试的时候老是被问，但是总也画不标准。后来看了这一篇，大概了解了一些基本规则。 这种记图的方式叫做plate notation，我们会用到符号包括 单圆圈表示隐变量 双圆圈表示观察到的变量 箭头表示采样 相同参数采样得到的独立同分布的变量放在一个方框里，并在方框中标注变量数量 ¶主题模型 Unigram Model够简单，但不太符合我们日常的写作习惯。写作的人在写文档时，往往会先挑选一些主题。比如我在写这篇博客的时候，我想写一些关于机器学习，关于统计，关于工程实现方面的东西，那么我的这个多面体恐怕“煎饼果子”，“热干面”这样的词出现的概率就得很小，可当我写一些美食相关的博客时，这样一个多面体或许又显得有些不够用。 主题模型很好地解决了这一问题，相比Unigram Model，它添加了主题层，让文档生成的方式更加自然合理。一篇文章往往由多个主题构成，一个主题可以由多个与之相关的词语来描述。简单来说，当我们现在要创作一篇文当时，我们并不是只有一个大的多面体了，而是有多个“主题多面体”。在计算机主题相关的多面体上，内存、硬盘、复杂度、编程这些词出现的概率会更大；而在音乐相关的主题上。莫扎特、钢琴、二分音符、C大调这样的词出现的概率会更大。 当然，这时我们生成一篇文档，就需要两种多面体。第一种多面体有一个，它有K个面，每个面代表一个主题；第二种多面体有K个，每一个有W革面，每一个代表一个词。当我们生成一篇有N个词文档时，对每一个词，我们先扔第一种多面体，选定一个主题，然后扔对应的第二种多面体，得到最终的词。 这样直观的想法最初由Hoffmm在199年给出的PLSA（Probabilistic Latent Semantic Analysis）模型中进行了明确的数学化。PLSA在对文档主题建模时，除了词袋模型的大前提外，主要有三点假设： 一篇文章可以由多个主题构成 每一个主题由一组词的概率分布来表示 一篇文章中的每一个具体的词都来自于一个固定的主题 对于PLSA模型而言一篇文档生成的方式如下 PLSA文档生成过程 假设：有两类多面体分别是 doc-topic多面体，每一个doc-topic多面体有K个面，K代表主题个数，每个面对应一个主题编号。 topic-word多面体，一共有K个，每个topic-word多面体有V个面，每个面对应一个词。 过程： 1.选定文档的词语数量N； 2.抛掷doc-topic多面体，记录朝上的面对应的主题k； 3.抛掷k对应的topic-word多面体，记录朝上的面对应的词v 3.重复步骤2,3, N次，直到所有词都生成。 每篇文档生成的概率和整个语料库生成的概率就可以由多个多项分布的联合分布求出来，具体公式这里略去，我们直接开始讲LDA。 ¶LDA文本主题建模 好了，终于到LDA了。 细心的你一定会想，unigram model中多面体的概率能用贝叶斯估计，那PLSA是否也能用贝叶斯估计能。当然！如果把PLSA中doc-topic多面体和topic-word多面体每一面朝上的概率加上Drichlet先验分布，就得了LDA模型！简单来说，可以理解为LDA就是把PLSA中的概率估计改造成了一个贝叶斯估计，选用的先验分布是Drichlet分布。（下面这个图我个人觉得把$Dir(\vec{\beta)}$）指向右边的topic-word多面体会更好一些。 [1]Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation[J]. Journal of machine Learning research, 2003, 3(Jan): 993-1022.]]]></content>
      <categories>
        <category>机器学习模型</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[今天开始写博客]]></title>
    <url>%2F2017%2F11%2F07%2F%E4%BB%8A%E5%A4%A9%E5%BC%80%E5%A7%8B%E5%86%99%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[忙了两天，总算把个人博客搭起来了 ¶总之今天开始写博客了 这里看起来可能很奇怪，恩，因为就是一个MarkDown的写法的练习。 说实话我才敢刚开始写，还不太习惯 万事开头难。 ¶从今天开始写博客 预期每周一篇 包括但不限于以下题材 技术相关，机器学习编程啥的，某一个具体部分的详细介绍 读书笔记 文献阅读笔记 个人思考 一周至少300字吧，不算多，写个计划可能都不止300字了（笑）。]]></content>
      <categories>
        <category>Whatever</category>
        <category>日记</category>
      </categories>
      <tags>
        <tag>Whatever</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F11%2F06%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. ¶Quick Start ¶Create a new post 1$ hexo new "My New Post" More info: Writing ¶Run server 1$ hexo server More info: Server ¶Generate static files 1$ hexo generate More info: Generating ¶Deploy to remote sites 1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>Whatever</category>
      </categories>
  </entry>
</search>
