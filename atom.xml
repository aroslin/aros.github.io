<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Aroslin</title>
  
  <subtitle>Life is always hard.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-11-19T12:35:57.787Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Aros Lin</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>《隐性逻辑》读书笔记</title>
    <link href="http://yoursite.com/2017/11/19/implicit-logic/"/>
    <id>http://yoursite.com/2017/11/19/implicit-logic/</id>
    <published>2017-11-19T12:29:33.000Z</published>
    <updated>2017-11-19T12:35:57.787Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>A novel approach to identify the major research themes and development trajectory: The case of patenting research</title>
    <link href="http://yoursite.com/2017/11/15/paper6/"/>
    <id>http://yoursite.com/2017/11/15/paper6/</id>
    <published>2017-11-15T02:11:06.000Z</published>
    <updated>2017-11-15T07:30:41.889Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>作者：Louis Y.Y. Lu, John S. Liu</p><p>年份：2015</p><p>期刊：Technological Forecasting &amp; Social Change</p></blockquote><p>研究内容：<strong>Identify the major research themes</strong>.文章在WOS上检索了关于“patent”的论文，建立引文网络，通过<strong>edge-betweenness clustering technique</strong>和<strong>key-route main path analysis are</strong>两种方法来确立主要研究主题。提出的方法可用于发现一个领域的主要研究主题并绘制发展轨迹。数据需要有citation relationships。</p><p>这篇文章的国内外研究现状是穿插在小节中，而不是独立在某一个chapter中写的。其实我个人比较认同这种引用文献的方式，但在TF &amp; SC的其他论文中似乎比较少见。</p><a id="more"></a><h2 id="关键技术"><a class="header-anchor" href="#关键技术">¶</a>关键技术</h2><h3 id="edge-betweenness-clustering"><a class="header-anchor" href="#edge-betweenness-clustering">¶</a>Edge-betweenness clustering</h3><p>删除在不同的Groups之间edge来发现’community’。Edge-betweenness是针对边而言的，定义为“顶点对间途径该边的最短路径数量”(The edge-betweenness of an edgeis defined as ‘the number of shortest paths between pairs of verticesthat run along it’)。</p><p>本文是直接用的<a href="http://www.interjournal.org/manuscript_abstract.php?361100992" target="_blank" rel="external">Csardi, G., Nepusz, T., 2006. The igraph software package for complex network research.Int. J. Complex Syst. 1695</a>开发的软件实现Edge-betweenness clustering。</p><p>算法的大致步骤如下：</p><blockquote><ul><li>First,calculate the betweenness for all edges in the network;</li><li>second, remove the edge with the highest betweenness;</li><li>third, recalculate the betweenness for all edges affected by the removal;</li><li>fourth, repeat from step 2 until no edge remains.</li></ul></blockquote><h3 id="key-route-main-path-analysis"><a class="header-anchor" href="#key-route-main-path-analysis">¶</a>Key-route main path analysis</h3><p>寻找有高遍历权重（traversal weight）的边top significant link，从头结点和尾结点展开搜索，搜索的links和这条link构成一个Path。所有Path的集合叫做Key-route main path。这里解释不是很详细，不过作者本身也只是用了别人的工具。</p><p>本文使用Pajek软件直接生成主路径的。<a href="http://www.academia.edu/download/31188195/pajek.pdf" target="_blank" rel="external">Batageij, V., Mrvar, A., 1998. Pajek — program for large network analysis. Connections 21(2), 47–57.</a></p><h2 id="数据"><a class="header-anchor" href="#数据">¶</a>数据</h2><p>1971-2013在WOS上与Patent相关的论文。完成后进行了简单的统计。</p><blockquote><ul><li>Author statistics.The authors are ranked by g-index and h-index.</li><li>Journal statistics。top 20 influential journals that have published the most research papers in patenting according to their g-index and hindex.</li></ul></blockquote><h2 id="结果"><a class="header-anchor" href="#结果">¶</a>结果</h2><ul><li>通过edge-betweenness clustering得到major research groups</li><li>通过word cloud判断group topic</li><li>通过key-route main path analysis来找寻gruops之间的关联和整体的发展轨迹。</li></ul><h3 id="clustering-in-patenting"><a class="header-anchor" href="#clustering-in-patenting">¶</a>Clustering in patenting</h3><p>一个整体统计加上每一个Gruop的词云，本来就只有12页的论文基本上都是图和表。😂😂😂 八类分别是：</p><ul><li>citation network analysis</li><li>patent law</li><li>patent valuation</li><li>academic patenting</li><li>gene patenting</li><li>patent policy</li><li>patent protection</li><li>technology analysis</li></ul><center>Table 1.Statistics of 8 clusters</center><table><thead><tr><th>Cluster</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th></tr></thead><tbody><tr><td>Active years</td><td>1983–2013</td><td>1977–2013</td><td>1986–2013</td><td>1978–2013</td><td>1991–2013</td><td>1985–2013</td><td>1989–2013</td><td>1994–2013</td></tr><tr><td>No. of papers</td><td>609</td><td>410</td><td>399</td><td>248</td><td>192</td><td>171</td><td>154</td><td>138</td></tr><tr><td>Papers /year</td><td>19.64</td><td>11.08</td><td>14.25</td><td>6.89</td><td>8.73</td><td>5.90</td><td>6.16</td><td>4.6</td></tr></tbody></table><img src="/2017/11/15/paper6/1.png" title="Gene patenting word cloud"><h3 id="overall-development"><a class="header-anchor" href="#overall-development">¶</a>Overall development</h3><p>节点是论文。</p><img src="/2017/11/15/paper6/2.png" title="Overall development trajectory"><p>后面的group需要根据之前聚类的结果人为标注。</p><img src="/2017/11/15/paper6/3.png" title="八个Group之间的关系"><h2 id="借鉴意义"><a class="header-anchor" href="#借鉴意义">¶</a>借鉴意义</h2><ul><li>我个人是觉得把国内外研究现状穿插在具体的方法选择部分可能更好。</li><li>聚类方式本身没有太大的创新，但是可以考虑之后也利用一些引文信息作为聚类的辅助判断标准。</li><li>自动生成Main Path或许是一个应用点？</li></ul><blockquote><p><a href="http://www.sciencedirect.com/science/article/pii/S0040162515003133" target="_blank" rel="external">A novel approach to identify the major research themes and development trajectory: The case of patenting research</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;作者：Louis Y.Y. Lu, John S. Liu&lt;/p&gt;
&lt;p&gt;年份：2015&lt;/p&gt;
&lt;p&gt;期刊：Technological Forecasting &amp;amp; Social Change&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;研究内容：&lt;strong&gt;Identify the major research themes&lt;/strong&gt;.
文章在WOS上检索了关于“patent”的论文，建立引文网络，通过&lt;strong&gt;edge-betweenness clustering technique&lt;/strong&gt;和&lt;strong&gt;key-route main path analysis are&lt;/strong&gt;两种方法来确立主要研究主题。提出的方法可用于发现一个领域的主要研究主题并绘制发展轨迹。数据需要有citation relationships。&lt;/p&gt;
&lt;p&gt;这篇文章的国内外研究现状是穿插在小节中，而不是独立在某一个chapter中写的。其实我个人比较认同这种引用文献的方式，但在TF &amp;amp; SC的其他论文中似乎比较少见。&lt;/p&gt;
    
    </summary>
    
      <category term="文献阅读" scheme="http://yoursite.com/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/"/>
    
      <category term="Technology foresight" scheme="http://yoursite.com/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/Technology-foresight/"/>
    
    
      <category term="Technology foresight" scheme="http://yoursite.com/tags/Technology-foresight/"/>
    
  </entry>
  
  <entry>
    <title>Topic-driven Multi-type Citation Network Analysis</title>
    <link href="http://yoursite.com/2017/11/10/paper5/"/>
    <id>http://yoursite.com/2017/11/10/paper5/</id>
    <published>2017-11-10T08:20:46.000Z</published>
    <updated>2017-11-10T11:50:46.561Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>作者：Yang Z, Hong L, Davison B D.</p><p>年份：2010</p><p>期刊：Adaptivity, Personalization and Fusion of Heterogeneous Information.</p></blockquote><p>研究内容：Focus on the task of ranking authors.以往的研究已经使用了content-based approaches或者citation network link analyses,但两者的结合却很少。本文提出一种新的概率模型，模型结合了基于内容的方法和多种引用网络（文章之间，作者之间，隶属关系，出版场所），并且提出了heterogeneous PageRank random surfer model来反映不同特征的重要性。</p><p>本文贡献：</p><ul><li>Proposing a novel probabilistic model which combines content-based analysis with a multi-type citation network, integrating relationships of authors, papers, affiliations and publishing venues in one model. This model can be extended to include more types of social factors.</li><li>Proposing a heterogeneous PageRank random surfer model compared with the original uniform PageRank model, to reflect the impact among different factors.</li><li>Introducing topical link analysis into citation network analysis. In particular, Topical PageRank [18] is adopted for citation link analysis.</li><li>A comparative study using ACM digital library data on various PageRank extensions as well as different complexity of citation networks.</li></ul><p>说实话，他号称的Combine就是把俩结果用一个权值加起来，总有一种上当受骗的感觉…🙃🙃🙃</p><a id="more"></a><h2 id="主要方法："><a class="header-anchor" href="#主要方法：">¶</a>主要方法：</h2><h3 id="构建网络图"><a class="header-anchor" href="#构建网络图">¶</a>构建网络图</h3><h4 id="4-t-graph-version-1"><a class="header-anchor" href="#4-t-graph-version-1">¶</a>4-T graph version-1</h4><p>四个Factor，四副sub-graph。</p><ul><li>Author Graph GAu。 节点是作者。如果两个作者共同写过一篇文章或者一个引用过另一个，两个作者间存在连线。单线，无权。（下同）</li><li>Paper Graph GP。 节点是文章，连线是引用。</li><li>Affiliation Graph GAf。 节点是机构，如果两个机构下至少有一对作者有连线，两机构连线。</li><li>Venue Graph GV。节点是出版社，如果两个出版社下至少有一对文章有连线，两出版社连线。</li></ul><p>子网间的连线：根据各自的关系，有一度连接的都连起来。</p><ul><li>作者连线发表文章</li><li>作者连线属于机构</li><li>作者连线发表文章出版社</li><li>文章连线作者机构</li><li>文章连线出版社</li><li>机构连接下属作者发表文章对应的出版社。</li></ul><img src="/2017/11/10/paper5/1.png" title="4-T graph version-1"><h4 id="4-t-graph-version-2"><a class="header-anchor" href="#4-t-graph-version-2">¶</a>4-T graph version-2</h4><p>框架1存在冗余。设计框架2，子图和图内连接方式相同，作者只和文章与机构连线，文章之和作者和出版物连线。</p><img src="/2017/11/10/paper5/2.png" title="4-T graph version-2"><h3 id="topical-pagerank求解"><a class="header-anchor" href="#topical-pagerank求解">¶</a>Topical PageRank求解</h3><p>整张图看作一个大的网络，使用Pagerank进行排序。不同于均匀的PageRank的是，节点间的传播概率是不同的。相同类型的两个节点间的传播概率相同。</p><p>基于Topical PageRank来实现content-based analysis。每一个节点对应一个Page，包括代表这个节点的T个主题的分布组成的主题向量和代表这个节点在每个主题上的重要程度的authority向量。random surfer model，游走求解。</p><img src="/2017/11/10/paper5/3.png" title="4-T Rank"><h3 id="combine-content-based-approach-with-citation-network"><a class="header-anchor" href="#combine-content-based-approach-with-citation-network">¶</a>Combine content-based approach with citation network</h3><blockquote><p>Okapi BM25：In information retrieval, Okapi BM25 (BM stands for Best Matching) is a ranking function used by search engines to rank matching documents according to their relevance to a given search query.</p></blockquote><p>content-based approach就是用Okapi BM25算一个结果，citation network就是前面的Topical PageRank，Combine就是把他俩用一个权值加起来。</p><p>这就是你说的Combine？你在逗我？🙂</p><img src="/2017/11/10/paper5/4.png" title="Combine？🙂"><p>三种方式来评价结果和query的匹配程度（你上面不是说不用于检索吗？）</p><ul><li>In the first approach, we collected all the PC members in the related conferences for each research area during 2008 and 2009.</li><li>In the second approach, we collected all the ACM fellows, ACM distinguished and senior members provided from the ACM website.</li><li>We utilized human judgements to generate relevant lists in the third approach. In our evaluation system, the top ten and twenty returned authors by various ranking algorithms were retrieved and mixed together.</li></ul><h3 id="实验结果"><a class="header-anchor" href="#实验结果">¶</a>实验结果</h3><p><strong>评价指标</strong>：NDCG（网页中用于衡量网页排序质量的指标）</p><p>越后面介绍的方法指标更好。</p><img src="/2017/11/10/paper5/5.png" title="评价指标"><p>参数的比较。</p><img src="/2017/11/10/paper5/6.png" title="参数比较"><blockquote><p><a href="https://dl.acm.org/citation.cfm?id=1937062" target="_blank" rel="external">node2vec: Scalable Feature Learning for Networks</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;作者：Yang Z, Hong L, Davison B D.&lt;/p&gt;
&lt;p&gt;年份：2010&lt;/p&gt;
&lt;p&gt;期刊：Adaptivity, Personalization and Fusion of Heterogeneous Information.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;研究内容：Focus on the task of ranking authors.以往的研究已经使用了content-based approaches或者citation network link analyses,但两者的结合却很少。本文提出一种新的概率模型，模型结合了基于内容的方法和多种引用网络（文章之间，作者之间，隶属关系，出版场所），并且提出了heterogeneous PageRank random surfer model来反映不同特征的重要性。&lt;/p&gt;
&lt;p&gt;本文贡献：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proposing a novel probabilistic model which combines content-based analysis with a multi-type citation network, integrating relationships of authors, papers, affiliations and publishing venues in one model. This model can be extended to include more types of social factors.&lt;/li&gt;
&lt;li&gt;Proposing a heterogeneous PageRank random surfer model compared with the original uniform PageRank model, to reflect the impact among different factors.&lt;/li&gt;
&lt;li&gt;Introducing topical link analysis into citation network analysis. In particular, Topical PageRank [18] is adopted for citation link analysis.&lt;/li&gt;
&lt;li&gt;A comparative study using ACM digital library data on various PageRank extensions as well as different complexity of citation networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;说实话，他号称的Combine就是把俩结果用一个权值加起来，总有一种上当受骗的感觉…🙃🙃🙃&lt;/p&gt;
    
    </summary>
    
      <category term="文献阅读" scheme="http://yoursite.com/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/"/>
    
      <category term="Technology foresight" scheme="http://yoursite.com/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/Technology-foresight/"/>
    
    
      <category term="Technology foresight" scheme="http://yoursite.com/tags/Technology-foresight/"/>
    
  </entry>
  
  <entry>
    <title>Social Influence Analysis in Large-scale Networks</title>
    <link href="http://yoursite.com/2017/11/10/paper4/"/>
    <id>http://yoursite.com/2017/11/10/paper4/</id>
    <published>2017-11-10T08:11:10.000Z</published>
    <updated>2017-11-10T08:20:20.169Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>作者：Jie Tang</p><p>年份：2009</p><p>期刊：KDD</p></blockquote><p>研究内容：区分不同angle(topic)上的社会影响，量化影响的大小。propose Topical Affinity Propagation (TAP) to model the topic-level social influence onlarge networks.</p><a id="more"></a><h2 id="主要思想"><a class="header-anchor" href="#主要思想">¶</a>主要思想</h2><p>在主题层面（topic level）上利用亲和力传播（affinity propagation）来进行社会影响鉴定。</p><ul><li>TAP provides topical influence graphs that quantitatively measure the influence on a fine-grain level;</li><li>The influence graphs from TAP can be used to support other applications such as finding representative nodes or constructing the influential subgraphs;</li><li>An efficient distributed learning algorithm is developed for TAP based on the Map-Reduce framework in order to scale to real large networks.</li></ul><h2 id="数据"><a class="header-anchor" href="#数据">¶</a>数据</h2><p>网络和节点下的主题分布。一个作者共现网络，一个引用网络，一个film-director-actor-writer网络</p><h2 id="结论"><a class="header-anchor" href="#结论">¶</a>结论</h2><p>相较于文本相似度有两个优势1.可以分析来年各个节点互相影响的差异。2.可以计算整个网络中影响力最强的节点。</p><p><strong>借鉴意义</strong>：可对社交网络中用户节点转化成向量，然后计算用户节点与相连节点的相似性，作为亲密度度量的手段。</p><blockquote><p><a href="https://dl.acm.org/citation.cfm?id=1557108" target="_blank" rel="external">Social Influence Analysis in Large-scale Networks</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;作者：Jie Tang&lt;/p&gt;
&lt;p&gt;年份：2009&lt;/p&gt;
&lt;p&gt;期刊：KDD&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;研究内容：区分不同angle(topic)上的社会影响，量化影响的大小。
propose Topical Affinity Propagation (TAP) to model the topic-level social influence on
large networks.&lt;/p&gt;
    
    </summary>
    
      <category term="文献阅读" scheme="http://yoursite.com/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/"/>
    
      <category term="Social Networks" scheme="http://yoursite.com/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/Social-Networks/"/>
    
    
      <category term="Social Networks" scheme="http://yoursite.com/tags/Social-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Identifying potentially disruptive trends by means of keyword network analysis</title>
    <link href="http://yoursite.com/2017/11/10/paper3/"/>
    <id>http://yoursite.com/2017/11/10/paper3/</id>
    <published>2017-11-10T07:55:08.000Z</published>
    <updated>2017-11-10T08:33:07.988Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>作者：Fefie Dotsika, Andrew Watkins</p><p>年份：2017</p><p>期刊：Technological Forecasting &amp; Social Change</p></blockquote><p>研究内容：通过词共现生成词网络，一系列的词语代表一个技术点。由词网路各种中心性的对比变化分析技术的破坏性趋势。</p><a id="more"></a><h2 id="关键术语含义"><a class="header-anchor" href="#关键术语含义">¶</a>关键术语含义</h2><p><strong>有价值技术</strong>：In an ever-changing technological landscape where innovation is a crucial driver for economic growth and survival, it is desirable to be able to predict which technologies, when established, have the potential to revolutionise an industry, create new markets, and increase accessibility and affordability.</p><p><strong>Disruptive innovation定义</strong>：Disruptive innovation is defined as the process of transforming a product or service that historically has been accessible at the top of a market access (i.e. for a high price or specialised skill-set) to become accessible to a new and larger population of consumers at the bottom of that market</p><p>Maturing trends were found to share influential common topics identified by high degree, betweenness and closeness centrality scores.Niche and potentiallyemerging trends within groupswere detected by means of eccentricity and farness metrics.</p><h2 id="使用数据"><a class="header-anchor" href="#使用数据">¶</a>使用数据</h2><ul><li>WOS文献数据</li><li>Forrester,Frost &amp; Sullivan, Gartner, IDC and Ovum.五种Business reports</li></ul><h2 id="分析方法"><a class="header-anchor" href="#分析方法">¶</a>分析方法</h2><h3 id="1-分析各个领域的数量分布"><a class="header-anchor" href="#1-分析各个领域的数量分布">¶</a>1.分析各个领域的数量分布。</h3><p>结论：没有明显证据表明学术出版物总是超前于商业出版物。</p><h3 id="2-网络结构与特征分析"><a class="header-anchor" href="#2-网络结构与特征分析">¶</a>2.网络结构与特征分析。</h3><p>网络的节点是词，连线表示两个词在同一个出版物中共现的次数总和，使用UCNET等工具制作。</p><ul><li>网络结构指标，包括network size，density，diameter，average degree等。</li><li>聚类与子网络指标，包括coefficient，Erdös number，average embeddedness，modularity等。结论：</li><li>所有网络呈现low density和high clustering coefficients，与随机网络差异明显</li><li>Positive modularity values</li><li>Erdös number is low</li></ul><h3 id="3-节点位置分析-找出已经是disruptive-的技术"><a class="header-anchor" href="#3-节点位置分析-找出已经是disruptive-的技术">¶</a>3.节点位置分析（找出已经是disruptive 的技术）</h3><ul><li>Degree centrality，指标越高，语义重要性越中心。</li><li>Eigenvector centrality，扩展了Degree centrality中心性的概念，与上类似。</li><li>Betweenness centrality，起到桥梁作用的次数越多，在信息流中更有影响力。</li><li>Closeness centrality，与网络中其他所有点的距离和。</li><li>Eccentricity，距离最远点的距离。提取degree、eigenvector、betweenness 、closeness四项中最高的两个词作为main keyword，通过可视化验证了这些词确实处于central的位置。</li></ul><p><strong>根据不同的中心性组合得到不同的disruptive技术类型。如High degree- Low betweenness，表示Popular mature keyword.</strong></p><h3 id="4-成熟前位置分析-找出可能成为disruptive-的技术"><a class="header-anchor" href="#4-成熟前位置分析-找出可能成为disruptive-的技术">¶</a>4.成熟前位置分析（找出可能成为disruptive 的技术）</h3><p>有high closeness 和 low degree 的技术最有成为disruptive 的技术的趋势。</p><p>列举Twitter`s popularity等一系列网络例子来证明这种趋势的存在。</p><p><strong>借鉴意义：</strong></p><ul><li>网络形成后的趋势分析方法。</li><li>例证方式。</li></ul><p><a href="http://www.sciencedirect.com/science/article/pii/S0040162517303517" target="_blank" rel="external">Identifying potentially disruptive trends by means of keyword network analysis</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;作者：Fefie Dotsika, Andrew Watkins&lt;/p&gt;
&lt;p&gt;年份：2017&lt;/p&gt;
&lt;p&gt;期刊：Technological Forecasting &amp;amp; Social Change&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;研究内容：通过词共现生成词网络，一系列的词语代表一个技术点。由词网路各种中心性的对比变化分析技术的破坏性趋势。&lt;/p&gt;
    
    </summary>
    
      <category term="文献阅读" scheme="http://yoursite.com/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/"/>
    
      <category term="Technology foresight" scheme="http://yoursite.com/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/Technology-foresight/"/>
    
    
      <category term="Technology foresight" scheme="http://yoursite.com/tags/Technology-foresight/"/>
    
  </entry>
  
  <entry>
    <title>Survey on Social Community Detection</title>
    <link href="http://yoursite.com/2017/11/10/paper2/"/>
    <id>http://yoursite.com/2017/11/10/paper2/</id>
    <published>2017-11-10T07:42:01.000Z</published>
    <updated>2017-11-10T07:54:58.202Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>作者：Michel Plantie, Michel Crampes</p><p>年份：2013</p><p>期刊：Social media retrieval. Springer London</p></blockquote><p>研究内容：对以往的Social Community Detection研究进行了总结，得出三种常见的分析方法。</p><ul><li>The first approach considers the social network as a graph and then analyzes its structure with graph properties and algorithms built around the graph structure.</li><li>The second approach associates the social network with a hypergraph and analyzes its structure through hypergraph properties and algorithms based on hypergraph structures.</li><li>The third approach uses the properties of concept lattices in order to analyze the social network structure in association with hypergraph properties and algorithms based on Galois lattices and hypergraph structures.</li></ul><p>总结很详细，不过没读完。</p><a id="more"></a><h2 id="关键对象说明"><a class="header-anchor" href="#关键对象说明">¶</a>关键对象说明</h2><ul><li>图(Graph)：node和edge构成，edge只连接两个nodes。</li></ul><img src="/2017/11/10/paper2/1.png" title="图"><ul><li>超图(Hypergraph)：node和hyperedges构成，hyperedges可以连接多个nodes，一个hyperedges内包含的就是一个community。</li></ul><img src="/2017/11/10/paper2/2.png" title="超图"><ul><li>Galois点阵（Galois lattice）Individuals sharing the same subset of properties define a community. row是属性，column是对象，画出一个图，包含先沟通的子集的对象被认为是一个community。</li></ul><img src="/2017/11/10/paper2/3.png" title="忘了是啥"><img src="/2017/11/10/paper2/4.png" title="也忘了"><blockquote><p><a href="https://link.springer.com/chapter/10.1007/978-1-4471-4555-4_4" target="_blank" rel="external">Survey on Social Community Detection</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;作者：Michel Plantie, Michel Crampes&lt;/p&gt;
&lt;p&gt;年份：2013&lt;/p&gt;
&lt;p&gt;期刊：Social media retrieval. Springer London&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;研究内容：对以往的Social Community Detection研究进行了总结，得出三种常见的分析方法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first approach considers the social network as a graph and then analyzes its structure with graph properties and algorithms built around the graph structure.&lt;/li&gt;
&lt;li&gt;The second approach associates the social network with a hypergraph and analyzes its structure through hypergraph properties and algorithms based on hypergraph structures.&lt;/li&gt;
&lt;li&gt;The third approach uses the properties of concept lattices in order to analyze the social network structure in association with hypergraph properties and algorithms based on Galois lattices and hypergraph structures.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总结很详细，不过没读完。&lt;/p&gt;
    
    </summary>
    
      <category term="文献阅读" scheme="http://yoursite.com/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/"/>
    
      <category term="Social Networks" scheme="http://yoursite.com/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/Social-Networks/"/>
    
    
      <category term="Social Networks" scheme="http://yoursite.com/tags/Social-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Node2vec: Scalable Feature Learning for Networks</title>
    <link href="http://yoursite.com/2017/11/10/paper1/"/>
    <id>http://yoursite.com/2017/11/10/paper1/</id>
    <published>2017-11-10T03:52:21.000Z</published>
    <updated>2017-11-10T08:33:45.807Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>作者：Aditya Grover，Jure Leskovec</p><p>年份：2016</p><p>期刊：KDD</p></blockquote><p>研究内容：Network Embedding。网络的特征学习和向量化表达，在multi-label classification和link prediction两个方面得到了有效性验证。</p><p>七月份精读过，不过当时没有记录太多。是一片适合作为random surfer model的入门之作。</p><a id="more"></a><p>主要方法：</p><h2 id="rrandom-walk-neighborhood-sampling-获得圈子训练集"><a class="header-anchor" href="#rrandom-walk-neighborhood-sampling-获得圈子训练集">¶</a>Rrandom walk neighborhood sampling 获得圈子训练集。</h2><p>当从节点t移动到节点v时，v移动到下一个节点x的概率由节点t到节点x的距离决定。</p><img src="/2017/11/10/paper1/1.png" title="random walk概率公式"><h2 id="skip-gram训练得到的embedding模型"><a class="header-anchor" href="#skip-gram训练得到的embedding模型">¶</a>Skip-gram训练得到的Embedding模型。</h2><p>训练时，输入时初始的node u，输出的random walk采样到的neighbor nodes。类似于word2vec 中的输入时一个词，输出是上下文的词进行训练。</p><img src="/2017/11/10/paper1/2.png" title="训练过程"><h2 id="结果展示"><a class="header-anchor" href="#结果展示">¶</a>结果展示</h2><p>可以根据选择不同的步长和概率参数得到不同的结果，如发现社交圈子（上图），或者发现在网络结构中处在类似地位的节点（下图）。</p><img src="/2017/11/10/paper1/3.png" title="结果展示"><p><strong>借鉴意义</strong>：可对社交网络中用户节点转化成向量，然后计算用户节点与相连节点的相似性，作为亲密度度量的手段。</p><blockquote><p><a href="https://dl.acm.org/citation.cfm?id=2939754" target="_blank" rel="external">node2vec: Scalable Feature Learning for Networks</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;作者：Aditya Grover，Jure Leskovec&lt;/p&gt;
&lt;p&gt;年份：2016&lt;/p&gt;
&lt;p&gt;期刊：KDD&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;研究内容：Network Embedding。网络的特征学习和向量化表达，在multi-label classification和link prediction两个方面得到了有效性验证。&lt;/p&gt;
&lt;p&gt;七月份精读过，不过当时没有记录太多。是一片适合作为random surfer model的入门之作。&lt;/p&gt;
    
    </summary>
    
      <category term="文献阅读" scheme="http://yoursite.com/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/"/>
    
      <category term="Social Networks" scheme="http://yoursite.com/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/Social-Networks/"/>
    
    
      <category term="Social Networks" scheme="http://yoursite.com/tags/Social-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Hexo Next如何置顶文章</title>
    <link href="http://yoursite.com/2017/11/10/top-post/"/>
    <id>http://yoursite.com/2017/11/10/top-post/</id>
    <published>2017-11-10T03:52:21.000Z</published>
    <updated>2017-11-10T07:42:05.830Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装hexo-math"><a class="header-anchor" href="#安装hexo-math">¶</a>安装hexo-math</h2><p>参考<a href="http://www.netcan666.com/2015/11/22/%E8%A7%A3%E5%86%B3Hexo%E7%BD%AE%E9%A1%B6%E9%97%AE%E9%A2%98/" target="_blank" rel="external">Netcan_Space博客</a></p><p>安装<code>hexo-generator-index-pin-top</code>，然后在<code>Front-matter</code>中加上<code>top: true</code>即可.</p><a id="more"></a><p>在<code>\themes\next\layout\_macropost.swig</code>下修改修改相应的效果。</p><p>Next主题自带<code>Frongt-matter</code>中使用<code>Sticky: true</code>的方式添加指定效果，但是在这篇博文写作的时候该方法只会将文章置顶到某一个页面的最上方。如果你的首页博客是一页最多显示10篇，而从时间上你要指定的文章排在第15篇，那么设定sticky会导致你的文章在<code>/page/2</code>中被置顶。</p><p>本博客使用的置顶效果是在<code>post-sticky-flag</code>和<code>post-title-link</code>之间加入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if post.top &gt; 0 %&#125;</span><br><span class="line">  &lt;span class=&quot;post-top-flag&quot; title=&quot;top&quot;&gt;</span><br><span class="line">    &lt;font color=&quot;red&quot;&gt;[置顶]&lt;/font&gt;                    </span><br><span class="line">  &lt;/span&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;安装hexo-math&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#安装hexo-math&quot;&gt;¶&lt;/a&gt;安装hexo-math&lt;/h2&gt;
&lt;p&gt;参考&lt;a href=&quot;http://www.netcan666.com/2015/11/22/%E8%A7%A3%E5%86%B3Hexo%E7%BD%AE%E9%A1%B6%E9%97%AE%E9%A2%98/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Netcan_Space博客&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;安装&lt;code&gt;hexo-generator-index-pin-top&lt;/code&gt;，然后在&lt;code&gt;Front-matter&lt;/code&gt;中加上&lt;code&gt;top: true&lt;/code&gt;即可.&lt;/p&gt;
    
    </summary>
    
      <category term="问题梳理" scheme="http://yoursite.com/categories/%E9%97%AE%E9%A2%98%E6%A2%B3%E7%90%86/"/>
    
    
      <category term="Problem" scheme="http://yoursite.com/tags/Problem/"/>
    
  </entry>
  
  <entry>
    <title>Hexo写博客编辑公式相关问题</title>
    <link href="http://yoursite.com/2017/11/08/hexo-mathjax/"/>
    <id>http://yoursite.com/2017/11/08/hexo-mathjax/</id>
    <published>2017-11-08T14:33:16.000Z</published>
    <updated>2017-11-13T12:11:24.177Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装hexo-math"><a class="header-anchor" href="#安装hexo-math">¶</a>安装hexo-math</h2><p><a href="https://github.com/hexojs/hexo-math" target="_blank" rel="external">hexo-math的github</a>有详细的教程，主要就是两点。</p><p>一是安装</p><blockquote><p>npm install hexo-math --save</p></blockquote><p>二是在站点<code>_config.yml</code>中配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">math:</span><br><span class="line">  engine: <span class="string">'mathjax'</span> <span class="comment"># or 'katex'</span></span><br><span class="line">  mathjax:</span><br><span class="line">    src: custom_mathjax_source</span><br><span class="line">    config:</span><br><span class="line">      <span class="comment"># MathJax config</span></span><br><span class="line">  katex:</span><br><span class="line">    css: custom_css_source</span><br><span class="line">    js: custom_js_source <span class="comment"># not used</span></span><br><span class="line">    config:</span><br><span class="line">     <span class="comment"># KaTeX config</span></span><br></pre></td></tr></table></figure><a id="more"></a><p>值得一提的是，如果你是用的是Next主题，那么你还需要在\themes\next\下的<code>_config.yml</code>中配置启用mathjax，否则是不会生效的。（我这里弄了好久，还以为各种装错了，最是还是看了<a href="http://zjubank.com/2016/08/16/hexo-use-mathjax/" target="_blank" rel="external">参考链接</a>才找到问题）</p><pre><code>mathjax:  enable: true</code></pre><h2 id="多行公式"><a class="header-anchor" href="#多行公式">¶</a>多行公式</h2><p>如果使用的是hexo-math,写多行公式要使用</p><pre><code class="language-bash">{% math %}\begin{align} \end{align}{% endmath %}</code></pre><p>的方式。Latex语法<code>\\</code>换行，<code>&amp;</code>打在等号前进行对齐。<strong>特别需要注意的是，换行时要写<code>\\\\</code>，因为<code>\\</code>会被认为是转义字符<code>\</code></strong>。如</p><pre><code class="language-bash">{% math %}\begin{align} y &= x_1+x_2 \\\\&=1+2 \\\\&=3\end{align}{% endmath %}</code></pre><p>的显示效果是</p><span>$$\begin{align} y &amp; = x_1+x_2 \\\\&amp; =1+2 \\\\&amp; =3\end{align}$$</span><!-- Has MathJax -->]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;安装hexo-math&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#安装hexo-math&quot;&gt;¶&lt;/a&gt;安装hexo-math&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/hexojs/hexo-math&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;hexo-math的github&lt;/a&gt;有详细的教程，主要就是两点。&lt;/p&gt;
&lt;p&gt;一是安装&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;npm install hexo-math --save&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;二是在站点&lt;code&gt;_config.yml&lt;/code&gt;中配置&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;math:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  engine: &lt;span class=&quot;string&quot;&gt;&#39;mathjax&#39;&lt;/span&gt; &lt;span class=&quot;comment&quot;&gt;# or &#39;katex&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  mathjax:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    src: custom_mathjax_source&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    config:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      &lt;span class=&quot;comment&quot;&gt;# MathJax config&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  katex:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    css: custom_css_source&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    js: custom_js_source &lt;span class=&quot;comment&quot;&gt;# not used&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    config:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;     &lt;span class=&quot;comment&quot;&gt;# KaTeX config&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="问题梳理" scheme="http://yoursite.com/categories/%E9%97%AE%E9%A2%98%E6%A2%B3%E7%90%86/"/>
    
    
      <category term="Problem" scheme="http://yoursite.com/tags/Problem/"/>
    
  </entry>
  
  <entry>
    <title>《自控力-和压力做朋友》读书笔记</title>
    <link href="http://yoursite.com/2017/11/07/Self-Control-Pressure/"/>
    <id>http://yoursite.com/2017/11/07/Self-Control-Pressure/</id>
    <published>2017-11-07T12:33:16.000Z</published>
    <updated>2017-11-09T13:11:02.513Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>《自控力-与压力做朋友》（The Upside of Stress）</p><p>作者：[美] 凯里·麦格尼格尔 Kelly McGonigal</p><p>北京联合出版社2016年3月第1版</p><p>末尾附思维导图下载链接</p></blockquote><p><strong>To do something special, you just have to believe it`s special.</strong></p><p>人们为了减少吸烟，就在烟盒上画着吸烟者千疮百孔的肺，期望能警醒人们减少吸烟。可惜的是，人们看到这些吸烟者的肺，压力变得更大，有什么是比抽烟更能缓解压力的呢？于是人们抽了更多的烟，又给自己带来了更多的压力。</p><a id="more"></a><p>《自控力-与压力做朋友》Kelly McGonigal的第二本关于自控力的书。（JD上买错了才发现还有第一本…）</p><p>整本书的中心思想是：改变对压力的看法，会使你更健康和幸福。文中通过大量的社会学实验作为论据来证明论点。内容的展开都是从实验的角度出发，探讨面对压力时不同的态度导致的不同的生理性反映，干预和对比来说明实验的合理性。大量的例证让论证看起来很丰富，但缺少体系的结构设计和略显重复的实验内容让人看起来也有些疲惫。</p><p>但看结论的话，有点像鸡汤。不过书中除了大量实验佐证之外，还提供了很多帮助你改善思维习惯的小练习，许多实验的设计也可以作为调整的参考，值得一读。</p><h2 id="第一部分-重新思考压力"><a class="header-anchor" href="#第一部分-重新思考压力">¶</a>第一部分 重新思考压力</h2><h3 id="第一章-改变我们的思维模式：什么是压力？压力真的都是负担吗？"><a class="header-anchor" href="#第一章-改变我们的思维模式：什么是压力？压力真的都是负担吗？">¶</a>第一章——改变我们的思维模式：什么是压力？压力真的都是负担吗？</h3><p>过去人们总在说，压力是有害的，要缓解甚至消除压力。</p><p>但压力真的都是有害的吗？你是否在面对面对期末考试的压力时更高效的学习？是否在完全没有压力的假期中感到无聊和疲倦？是否在压力过后，感觉自己得到了成长？</p><p><strong>或许让我们难以承受的不是压力本身，而是我们对待压力的态度。</strong></p><img src="/2017/11/07/Self-Control-Pressure/firstChapter.png" title="第一章"><h3 id="第二章-身处困境时-压力是可以依靠的资源-而非要消灭的敌人"><a class="header-anchor" href="#第二章-身处困境时-压力是可以依靠的资源-而非要消灭的敌人">¶</a>第二章——身处困境时，压力是可以依靠的资源，而非要消灭的敌人</h3><p>比赛开始前，运动员们血脉喷张，心跳加速，身体新成代谢加快，他们告诉自己，我很兴奋，我想要大干一场。</p><p>比赛开始前，学生们血脉喷张，心跳加速，身体新成代谢加快，他们告诉自己，我很紧张，我得找个地方冷静一下。</p><p><strong>其实压力可以给我们带来动力，让我们更集中，以更好的状态面对难题。</strong></p><img src="/2017/11/07/Self-Control-Pressure/secondChapter.png" title="第二章"><h3 id="第三章-压力和意义成正比：有意义-意味着有压力"><a class="header-anchor" href="#第三章-压力和意义成正比：有意义-意味着有压力">¶</a>第三章——压力和意义成正比：有意义，意味着有压力</h3><p>寒假每天躺在床上，什么也不干，玩玩手机，吃了睡，睡了吃，这样的生活多好啊~</p><p>其实一点也不好，每天反而很累，而且觉得无趣。</p><p><strong>丢失了压力的生活，也丢失了意义</strong></p><img src="/2017/11/07/Self-Control-Pressure/thirdChapter.png" title="第三章"><h2 id="第二部分-转化压力"><a class="header-anchor" href="#第二部分-转化压力">¶</a>第二部分 转化压力</h2><h3 id="第四章-全身心投入：拥抱焦虑能帮助你更好的应对挑战"><a class="header-anchor" href="#第四章-全身心投入：拥抱焦虑能帮助你更好的应对挑战">¶</a>第四章——全身心投入：拥抱焦虑能帮助你更好的应对挑战</h3><p>试着让自己觉得兴奋而不是紧张。</p><p><strong>拥抱压力，你能获得勇气。</strong></p><img src="/2017/11/07/Self-Control-Pressure/fourthChapter.png" title="第四章"><h3 id="第五章-内在联结：压力能使人更具关怀性-提升抗挫力"><a class="header-anchor" href="#第五章-内在联结：压力能使人更具关怀性-提升抗挫力">¶</a>第五章——内在联结：压力能使人更具关怀性，提升抗挫力</h3><p>感到压力的时候，我们会倾向于和人交流我们的压力，来缓解自己的情绪；同样的他人和我们交流他们的压力时，如果我们有过类似的压力，就能带去更多的关怀。</p><p>怀着宏大的目标，崇高的使命感也能让人更有勇气面对压力。</p><p><strong>Life is always hard. Everybody knows.</strong></p><img src="/2017/11/07/Self-Control-Pressure/fifthhChapter.png" title="第五章"><h3 id="第六章-幸福成长：痛苦使你坚强-即使痛苦正当下-未来尚模糊"><a class="header-anchor" href="#第六章-幸福成长：痛苦使你坚强-即使痛苦正当下-未来尚模糊">¶</a>第六章——幸福成长：痛苦使你坚强，即使痛苦正当下，未来尚模糊</h3><p>经历过中等苦难的人，更容易成长，变得有毅力，身体更加康，幸福感也更高。不是说要支持去伤害自己，而是学会把痛苦变成自己的资源，让自己更强大。</p><p><strong>Everything that kills me, makes me feel alive.</strong></p><img src="/2017/11/07/Self-Control-Pressure/sixthChapter.png" title="第六章"><p>**结语：**作者上一本书《自控力》，讲的就是如何减少拖延，然而新书还是拖了很久才出版，可见哪怕写了这么多，作者自己不是立马就成了完人。思维不是那么容易变得，但人总成长。至少我今日写下了这篇博客，我在记录，我在改变。</p><p>[自控力思维导图下载](<a href="http://aroslin.github.io/downloads/The" target="_blank" rel="external">http://aroslin.github.io/downloads/The</a> Upside of Stress.xmind)</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;《自控力-与压力做朋友》（The Upside of Stress）&lt;/p&gt;
&lt;p&gt;作者：[美] 凯里·麦格尼格尔 Kelly McGonigal&lt;/p&gt;
&lt;p&gt;北京联合出版社2016年3月第1版&lt;/p&gt;
&lt;p&gt;末尾附思维导图下载链接&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;To do something special, you just have to believe it`s special.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;人们为了减少吸烟，就在烟盒上画着吸烟者千疮百孔的肺，期望能警醒人们减少吸烟。可惜的是，人们看到这些吸烟者的肺，压力变得更大，有什么是比抽烟更能缓解压力的呢？于是人们抽了更多的烟，又给自己带来了更多的压力。&lt;/p&gt;
    
    </summary>
    
      <category term="阅读笔记" scheme="http://yoursite.com/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Psychology" scheme="http://yoursite.com/tags/Psychology/"/>
    
  </entry>
  
  <entry>
    <title>从零开始学LDA（Latent Dirichlet allocation）</title>
    <link href="http://yoursite.com/2017/11/07/LDA-From-Zero/"/>
    <id>http://yoursite.com/2017/11/07/LDA-From-Zero/</id>
    <published>2017-11-07T11:17:10.000Z</published>
    <updated>2017-12-04T13:28:30.949Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>去年初学LDA，看完了Rickjin老师的《LDA数学八卦》，觉得不是很懂，查阅了很多资料之后才对LDA有了更深入地认识。一直想加入自己的理解后更简单的讲述这个模型，今日补上。</p><p><font color="red"><strong>注：文章的行文思路与大多数公式参考了Rickjin《LDA数学八卦》。有兴趣更深入了解的同学可以在看完本篇后继续阅读。</strong></font></p></blockquote><p>LDA(Latent Dirichlet allocation)是<a href="https://zh.wikipedia.org/wiki/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B" target="_blank" rel="external">主题模型</a>的一种，最早是由Blei D M<sup>[1]</sup>等人在2003年提出的，常被用于文档的主题识别。时至今日，在大大小小的会议上仍能看到不少对LDA（或其衍生、或类似的主题模型）提出改进的文章。其自然的思路，优雅的求解和拓展性强的模型结构，实在是非常适合作为对无监督学习一个入门模型来学习和改进。</p><p>本文主要介绍LDA背后的相关数学知识和模型的构建与求解思路，力求只有少量统计知识的初学者也能看懂。文中略去了大量的公式推导细节，如果希望深入的了解，请根据需要查找相关的资料。</p><a id="more"></a><h2 id="从扔硬币说起"><a class="header-anchor" href="#从扔硬币说起">¶</a>从扔硬币说起</h2><p>介绍模型之前，我们先来简单回顾一些知识点。本章主要讲解二项分布，贝叶斯理论，beta分布，multi分布和Dirichlet分布。如果你熟悉这些知识点，可以直接跳过这一章。</p><h3 id="二项分布"><a class="header-anchor" href="#二项分布">¶</a>二项分布</h3><blockquote><p>二项分布（英语：Binomial distribution）是n个独立的是/非试验中成功的次数的离散概率分布，其中每次试验的成功概率为p。这样的单次成功/失败试验又称为伯努利试验。实际上，当n = 1时，二项分布就是伯努利分布。二项分布是显著性差异的二项试验的基础。</p><p>摘自<a href="https://zh.wikipedia.org/wiki/%E4%BA%8C%E9%A0%85%E5%88%86%E4%BD%88" target="_blank" rel="external">二项分布维基百科</a></p></blockquote><p>扔一枚硬币，一共扔N次，其中k次都是正面朝上的概率是多少？这个概率服从二项分布(Bernoulli distribution)$$P(k|N,p)=C^k_N\cdot p^k\cdot (1-p)^{N-k}$$其中p是硬币扔一次正面朝上的概率。真实场景中，我们面对的问题往往是，我想知道这个公式中的p是多少。一个我们熟悉且自然的思路是，把这个硬币扔N次，得到的结果中k次朝上的那么可以估计</p><p>$$p=\frac{k}{N}$$</p><p>如果你学过概率论，那么你就知道这是对二项分布中参数p的一个<a href="https://zh.wikipedia.org/zh-hans/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1" target="_blank" rel="external">极大似然估计</a>。</p><h3 id="贝叶斯估计"><a class="header-anchor" href="#贝叶斯估计">¶</a>贝叶斯估计</h3><p>当我们像上面去估计p值的时候，其实已经默认了一个假设，就是p是一个定值，它在每次实验中都是相同的。p是一个客观存的量，只是我们不知道它是多少。这样的想法很容易理解，但有时候会带来一些问题。当我们扔了10次硬币，10次都朝上的时候（尽管概率很小，但这一现象也会发生），用上面的方法估计，p=1。这一结论和我们的常识不符，当全部出现正面朝上这种小概率事件时，我么可能会倾向认为p值比0.5大，但没有大到等于1。</p><p>这时候，有些人就想，或许p并不是一个定值，而是服从某一种概率分布，对p的一个较好的估计是这个概率分布的<a href="https://zh.wikipedia.org/zh-hans/%E6%9C%9F%E6%9C%9B%E5%80%BC" target="_blank" rel="external">期望</a>，那么即使发生了每次都正面朝上的情况，只要这个概率分布不是始终等于1，就不会出现违背常识的情况了。</p><p>怎么理解？这就好比说，你扔出的硬币在落下的一瞬间之前，在你无法感知的空间中散落成无数的硬币，这些硬币的密度不尽相同，也就是落下时的p是不同的，但这些p是的有约束的，都是服从某一概率分布。在降落的一瞬间，有且仅有一枚硬币投射到你感知到的硬币上，也就是这一次抛硬币对应的p。由于无数硬币的p值并不是都等于1，所以即使出现扔硬币多次全部正面朝上，也可以理解成这些硬币中p值较大的硬币比较多，而不会是所有硬币的p都等于1。</p><img src="/2017/11/07/LDA-From-Zero/1.jpg" title="Figure 1. 无限硬币"><p>如果你觉得上面这略显中二的说法也不太好理解的话😑，我们就直接看知识点好了。根据伯努利试验中p值是否固定，划分出了两个学派——频率学派和贝叶斯学派。这两个学派的思路差异主要体现在：</p><blockquote><ul><li>频率派把需要推断的参数θ看做是固定的未知常数，即概率虽然是未知的，但是确定的一个值，同时，样本X是随机的，所以频率派重点研究样本空间，大部分的概率计算都是针对样本X的分布；</li><li>贝叶斯派的观点则截然相反，他们认为参数是随机变量，而样本X 是固定的，由于样本是固定的，所以他们重点研究的是参数的分布。</li></ul></blockquote><p>在贝叶斯学派看来，所有的参数都不是固定值，而是服从一个概率分布，而对这个概率分布的估计有一个经典的模式：</p><p>先验分布𝜋(𝜃)+样本信息Χ⇒后验分布𝜋(𝜃|𝑥)</p><p>上述思考模式意味着，新观察到的样本信息将修正人们以前对事物的认知。换言之，在得到新的样本信息之前，人们对的认知是先验分布$𝜋(𝜃)$，在得到新的样本信息Χ后，人们对的认知为$𝜋(𝜃|𝑥)$。而这里求解$𝜋(𝜃|𝑥)$最重要依据就是贝叶斯公式（条件概率公式）$$P(𝜃,𝑋)=P(𝜃│𝑋)∙P(𝑋)=P(𝑋|𝜃)∙P(𝜃)$$</p><p>举个例子来说明两个学派分析的思路差异好了。加入我们扔一枚硬币，连续扔10次，有6次出现了正面朝上，那么扔一次正面朝上的概率是多少？</p><p>按照频率学派的观点，$$p=\frac{6}{10}=0.6$$</p><p>按照贝叶斯学派，首先p有一个先验分布，也就是你对p的一个经验估计，为了计算简单和表达的方便，咱们这里取一个离散分布,令$𝜃=p=0.4,0.5,0.6$，且</p><blockquote><p>$P(𝜃=0.4)=0.1$</p><p>$P(𝜃=0.5)=0.8$</p><p>$P(𝜃=0.6)=0.1$</p></blockquote><p>先验分布的期望$$E(𝜃)=0.4\cdot0.1+0.5\cdot0.8+0.6\cdot0.1=0.5$$</p><p>根据样本信息，10次中有6次朝上，由贝叶斯公式变形可以得到，在该样本条件下，$𝜃=6$的概率为</p><p>$$𝑝(𝜃=0.6│𝑋=6)=\frac{𝑝(𝑋=6|𝜃=0.6)\cdot𝑝(𝜃=0.6)}{𝑝(𝑋=6)} $$</p><p>其中：</p><ul><li>$𝜃$就是抛一次硬币是正面的概率，X是正面的次数，在本例中就是6；</li><li>$𝑝(𝜃=0.6│𝑋=6)$就是在6次正面的情况下，$𝜃=0.6$的概率，即后验概率；</li><li>$𝑝(𝑋=6|𝜃=0.6)$是𝜃=0.6时可以抛出6次正面的概率，即样本信息，这里求法按二项分布来求；</li><li>$𝑝(𝜃=0.6)$就是𝜃的先验分布，此例子中$𝑝(𝜃=0.6)=0.1$；</li><li>$𝑝(𝑋)$称为Normalizer，或者叫做marginal probability，是一个定值，$𝜃$分布如果是离散的，就是$𝑝(𝑋|𝜃)\cdot 𝑝(𝜃)$在所有𝜃情况下的和；如果$𝜃$是连续的，$𝑝(𝑋)=\int𝑝(𝑋|𝜃)\cdot 𝑝(𝜃)𝑑𝜃$。</li></ul><p>我们就可以分别求出在样本信息为10次实验6次正面的情况下，后验分布</p><blockquote><p>$P(𝜃=0.4|X=6)=0.056$</p><p>$P(𝜃=0.5|X=6)=0.819$</p><p>$P(𝜃=0.6|X=6)=0.125$</p></blockquote><p>后验分布的期望$$E(𝜃|X=6)=0.4\cdot0.056+0.5\cdot0.819+0.6\cdot0.125=0.5069$$</p><p>可以看到，后验分布的概率变得比0.5大了，而且并没有大很多。由于这里先验分布取的很特殊，哪怕出现无穷多次都是正面朝上的情况，后验分布的期望也只会逼近0.6。</p><h3 id="beta分布与共轭"><a class="header-anchor" href="#beta分布与共轭">¶</a>Beta分布与共轭</h3><p>上面的例子中，我们选取了一个离散分布作为先验分布，可以看出这个离散分布限定了p只能取三个值，得到的后验分布期望所在的区间也十分有限，选取这个分布并不太合适。数学家们给出了一个很漂亮的分布——<a href="https://zh.wikipedia.org/zh-hans/%CE%92%E5%88%86%E5%B8%83" target="_blank" rel="external">Beta分布</a>，来作为二项分布的先验分布。为什么选取Beta分布呢？主要是因为两点：</p><blockquote><ul><li>Beta分布形态多变，可以满足多种情况下的分布。</li><li>Beta分布做先验分布，样本信息服从二项分布时，两种分布符合良好的特性可以简化计算。</li></ul></blockquote><p>为了更好的理解Beat分布与二项分布的关系，我们先来看一下Beta分布的表达式。$$p(\theta)=Beta(\theta|\alpha,\beta)=\frac{\theta^{\alpha-1}\cdot (1-\theta)^{\beta-1}}{B(\alpha,\beta)},$$$$B(\alpha,\beta)=\int^1_0 t^{\alpha-1} (1-t)^{\beta-1}dt$$</p><!---其中$B(\alpha,\beta)$也可以写成$$B(\alpha,\beta)=\frac{\Gamma (\alpha)\cdot \Gamma (\beta)}{\Gamma (\alpha+\beta)},$$$$\Gamma (n)=\int^\infty_0t^{n-1}e^{-t}dt=(n-1)!$$两种写法之间的等价性就不在这里证明了。根据下面一种写法我们知道$$B(\alpha,\beta)=\frac{(\alpha-1)!\cdot (\beta-1)!}{(\alpha+\beta-2)!}$$--><p>公式中$\alpha$和$\beta$是Beta分布的两个超参数，通过调整$\alpha$和$\beta$的大小可以调整Beta分布的形态，如下图所示。</p><img src="/2017/11/07/LDA-From-Zero/2.png" title="Figure 2. Beta分布"><p>由图像我们不难看出，改变$\alpha$和$\beta$可以得到丰富的Beta分布的形式，得到不同的期望与分布的均匀程度，可以满足很多的分析场景。这是我们提到的选取Beta分布作为先验分布的第一点原因。</p><p>那么，它与二项分布的关系呢？我么不妨来求一下先验分布是Beta分布，样本服从二项分布的情况下，后验分布是什么样子的。如果先验分布是Beta分布</p><p>$$p(\theta)=Beta(\theta|\alpha,\beta)=\frac{\theta^{\alpha-1}\cdot (1-\theta)^{\beta-1}}{B(\alpha,\beta)}$$</p><p>样本服从二项分布</p><p>$$P(X|\theta)=C^k_N\cdot \theta^k\cdot (1-\theta)^{N-k}$$</p><p>那么根据贝叶斯公式，后验分布满足</p><span>$$\begin{align} p(\theta|X) &amp; =\frac{p(X|\theta)\cdot p(\theta)}{p(X)}\\\\&amp; =\frac{p(X|\theta)\cdot p(\theta)}{\int p(X|\theta)\cdot p(\theta)d\theta}\\\\&amp; =\frac{(C^k_N\cdot \theta^k\cdot (1-\theta)^{N-k})\cdot(\frac{\theta^{\alpha-1}\cdot (1-\theta)^{\beta-1}}{B(\alpha,\beta)})}{\int (C^k_N\cdot \theta^k\cdot (1-\theta)^{N-k})\cdot (\frac{\theta^{\alpha-1}\cdot (1-\theta)^{\beta-1}}{B(\alpha,\beta)})d\theta}\\\\&amp; =\frac{\frac{C^k_N}{B(\alpha,\beta)}\cdot \theta^{\alpha+k-1}\cdot (1-\theta)^{\beta+N-k-1}}{\frac{C^k_N}{B(\alpha,\beta)}\cdot \int \theta^{\alpha+k-1}\cdot (1-\theta)^{\beta+N-k-1}d\theta}\\\\&amp; =\frac {\theta^{\alpha+k-1}\cdot (1-\theta)^{\beta+N-k-1}}{B(\theta|\alpha+k,\beta+(N-k))}\\\\&amp;= Beta(\theta|\alpha+k,\beta+(N-k))\end{align}$$</span><!-- Has MathJax --><p>得到的仍然是一个Beta分布！也就是说，在先验分布是Beta分布的情况下，如果样本服从二项分布，那么后验分布也是Beta分布。在贝叶斯估计中，将这种根据样本求得的后验分布与先验分布的分布形式一致时，样本服从的分布与先验分布共称为<a href="https://zh.wikipedia.org/zh-hans/%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C" target="_blank" rel="external"><strong>共轭分布</strong></a>。将上面的式子我们简化的记为</p><p>$$Beta(\theta|\alpha,\beta)+BinomCount(k,N-k)=Beta(\theta|\alpha+k,\beta+N-k)$$</p><p>共轭是一个非常好的特性。在之前的先验分布是离散分布的例子中，我们需要把每一步都求解出来才能知道后验分布的期望是多少，但是如果先验分布是Beta分布，我们只需要会求Beta分布的期望，就能很快的得到后验分布的期望了。</p><p>事实上，$Beta(\theta|\alpha,\beta)$的期望非常好求（这里就不推导公式了）</p><p>$$E(Beta(\theta|\alpha,\beta))=\frac{\alpha}{\alpha+\beta}$$</p><p>等下，这个结果是不是有点眼熟？如果我们另$\alpha=k,\beta=N-k$，在扔硬币的试验中，Beta的期望，也就是对p值的估计就变成了</p><p>$$E(Beta(\theta|k,N-k))=\frac{k}{N}$$</p><p>和频率学派对p值得估计统一起来了！从这里我们可以看出超参数$\alpha,\beta$的物理意义实际上就是扔硬币实验中正面朝上负面朝上的次数。根据我们的经验，我们认为p值应该是0.5，那么我们可以设先验Beta分布中$\alpha=\beta=1$，十次实验全部正面朝上，后验Beta分布的期望，也就是对p值的估计$E(\theta)=11/12$，一个接近1但不等于1的数。甚至，如果你对p值等于0.5非常有自信，你可以先验Beta分布中$\alpha=\beta=100$，那么十次实验全部正面朝上的话，$E(\theta)=101/102$，对p值的估计仍在0.5左右。记住这一点，对理解共轭的特性和Beta分布的含义有较大的帮助。</p><blockquote><p>Beta分布是分布的分布，它与二项分布共轭。在Beta分布中，超参数$\alpha,\beta$物理含义是伯努利试验中两种结果发生的次数，它们比值决定了Beta分布的期望，他们的大小决定了样本对后验分布的影响程度。</p></blockquote><h3 id="多项分布与dirichlet共轭"><a class="header-anchor" href="#多项分布与dirichlet共轭">¶</a>多项分布与Dirichlet共轭</h3><p>好的，下面我们来把二项分布推广吧。想象硬币就是一个“二面体”，它的每一面朝上的事件都会发生，而且发生的概率并不相同，对二项分布</p><p>$$P(k|N,p)=C^k_N\cdot p^k\cdot (1-p)^{N-k}$$令$x_1=k$，$x_2=N-k$，$p_1=p$，$p_2=1-p$，$\vec{x}=(x_1.x_2)$，$\vec{p}=(p_1,p_2)$那么上式可以改写为</p><p>$$p(\vec{x}|N,\vec{p})=\frac{N!}{x_1!\cdot x_2!}\cdot p^{x_1}_1\cdot p^{x_2}_2$$</p><p>这里式子的形式已经很清楚了。如果我们抛一个k面的凸多面体，一共抛N次，每个面朝上的次数为$\vec{x}$，每个面朝上的概率为$\vec{p}$,那么对应的概率就是<a href="https://en.wikipedia.org/wiki/Multinomial_distribution" target="_blank" rel="external">多项分布</a></p><p>$$p(\vec{x}|N,\vec{p})=\frac{N!}{\prod_i^ kx_i}\cdot \coprod_{i}^ {k}p ^{x_i}_i$$</p><img src="/2017/11/07/LDA-From-Zero/3.png" title="Figure 3. 多面体"><p>与多项分布共轭的分布就是<a href="https://zh.wikipedia.org/zh-hans/%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83" target="_blank" rel="external">Dirchlet分布</a></p><p>$$p(\vec{x})=Dir(\vec{x}|\vec{\alpha})=\frac{1}{\Delta (\vec{\alpha})}\coprod_{i}^ {k}x_i ^{\alpha_i-1}$$$$\Delta (\vec{\alpha})=\int\coprod_{i} ^ {k}x ^{\alpha-1}_id \vec {x}$$</p><p>Dirchlet分布与多项分布的共轭也满足我们之前说的漂亮的性质</p><p>$$Dir(\vec{x}|\vec{\alpha})+MultCount(\vec{m})=Dir(\vec{x}|\vec{\alpha}+\vec{m})$$</p><p>而Dirchlet分布的期望等于</p><p>$$E(\vec{p})=(\frac{\alpha_1}{\sum_{k}^{i}\alpha_i},\frac{\alpha_2}{\sum_{k} ^ {i}\alpha_i}, \ldots ,\frac{\alpha_N}{\sum_{k} ^{i}\alpha_i})$$</p><p>多项分布与Dirihlet分布是整个LDA模型构建的基础。了解它们的特性对于理解和改进LDA模型会有很大的帮助。至于LDA模型到底是什么，我们下节继续。</p><h2 id="latent-dirichlet-allocation"><a class="header-anchor" href="#latent-dirichlet-allocation">¶</a>Latent Dirichlet allocation</h2><p>OK，正餐来了。那么什么是LDA呢？让我们从Unigram Model开始说起。</p><h3 id="unigram-model"><a class="header-anchor" href="#unigram-model">¶</a>Unigram Model</h3><p>一篇文档是由许多词语构成的，事实上，抛开语法结构不谈，如果一篇文章是由一些没有顺序的词语构成，我们仍能从一定程度理解文档在探讨的主题。就好像那句有趣的话一样</p><blockquote><p>研表究明，汉字序顺并不定一影阅响读</p></blockquote><p>那么我们不妨将文档看做是一些词的集合，那么最简单的生成文档的方式就是，先选定这篇文档的词语个数N，然后选N个词出来。这一过程就好像是我们有一个巨大的多面体，它有字典里的词那么多面，每一面对应一个词。当我们要创作一篇有N个词的文档时，我们就把这个多面体扔N次，然后文档就生成了。</p><p>是的，上面这个简单的文档生成模型就叫做Unigram Model。它所对应的一篇文档的生成概率就是一个多项分布，根据词语的数量和总词数就可以估计每个词出现的概率。</p><img src="/2017/11/07/LDA-From-Zero/4.png" title="Figure 4. Unigram Model"><p>Unigram model中文档生成的过程如下</p><blockquote><hr><p><strong>Unigram model文档生成过程</strong></p><hr><p>假设：只有一个多面体，这个多面体有N面，每个面代表一个词；每个面的朝上的概率不尽相同。</p><hr><p>过程：</p><p>1.选定文档的词语数量M；</p><p>2.抛掷多面体，记录朝上的面对应的词；</p><p>3.重复步骤共M次，直到所有词都生成。</p><hr></blockquote><p>由生成过程我们知道，当一篇文档每一个词确定下来之后得到$\vec{w}=(w_1,\cdots,w_V)$，则该文档的生成概率为</p><p>$$p(\vec{w})=p(w_1)\cdots p(w_V)$$</p><p>如果语料库中有M篇文档，那么语料库$W=(\vec{w}_1)\cdots \vec{w}_M$生成的概率为</p><p>$$p(W)=p(\vec{w}_1)\cdots p(\vec{w}_M)$$</p><p>假如语料库中的无重复的词语数量是N，每一个词在所有文档中出现的次数为$n_i$，那么$\vec{n}=(n_1,\cdots,n_N)$正好是一个多项分布，整个语料库生成的概率为$$p(W)=p(\vec{n})=Mult(\vec{n}|\vec{p},N)=\coprod _{i=1} ^{N} p_i ^{n_i}$$</p><p>这里，对每一个参数$p_k$的一个估计值就是</p><p>$$\hat{p_i}=\frac{n_i}{N}$$</p><p>当然，别忘记了贝叶斯估计，如果给这个多面体每个面朝上的概率一个先验分布$Dir(\vec{p}| \vec{\alpha})$，那么就可以轻松地根据样本信息 $\vec{n}$ 来估计后验分布$Dir(\vec{p}| \vec{\alpha}+\vec{n})$了。它的概率图模型如下图所示。</p><img src="/2017/11/07/LDA-From-Zero/5.jpg" title="Figure 5. Unigram Model概率图模型"><blockquote><p>这里稍微提一下概率图模型。我没系统学过，面试的时候老是被问，但是总也画不标准。后来看了<a href="http://www.cnblogs.com/arachis/p/LDA.html" target="_blank" rel="external">这一篇</a>，大概了解了一些基本规则。</p><p>这种记图的方式叫做plate notation，我们会用到符号包括</p><ul><li>单圆圈表示隐变量</li><li>双圆圈表示观察到的变量</li><li>箭头表示采样</li><li>相同参数采样得到的独立同分布的变量放在一个方框里，并在方框中标注变量数量</li></ul></blockquote><p>这里，我们不难根据之前Normalizer的定义，得出先验分布是超参数为$\vec{alpha}$的Dirichlet分布的Unigram Model，文档生成概率如下（这个公式比较重要，我们后面会用到，记为<span id="jump">公式3.1</span>。）</p><span>$$\begin{align} p(\vec{W}|\vec{\alpha})&amp; =\int p(\vec{W}|\vec{p}) \cdot p(\vec{p}|\vec{\alpha})d\vec{p}\\\\&amp;=\int \prod_{i=1}^{N}p_i^{n_i}\cdot Dir(\vec{p}|\vec{\alpha}))d\vec{p} \\\\&amp;=\int \prod_{i=1}^{N}p_i^{n_i} \cdot \frac{1}{\Delta (\vec{\alpha})} \prod_{i=1}^{N}p_i^{\alpha_i-1}d\vec{p} \\\\&amp;=\frac{1}{\Delta (\vec{\alpha})}\int \prod_{i=1}^{N}p_i^{n_i+\alpha_i-1}\vec{p}\\\\&amp;=\frac{\Delta(\vec{n}+\vec{\alpha})}{\Delta (\vec{\alpha})}\end{align}$$</span><!-- Has MathJax --><h3 id="主题模型"><a class="header-anchor" href="#主题模型">¶</a>主题模型</h3><p>Unigram Model够简单，但不太符合我们日常的写作习惯。写作的人在写文档时，往往会先挑选一些主题。比如我在写这篇博客的时候，我想写一些关于机器学习，关于统计，关于工程实现方面的东西，那么我的这个多面体恐怕“煎饼果子”，“热干面”这样的词出现的概率就得很小，可当我写一些美食相关的博客时，这样一个多面体或许又显得有些不够用。</p><p>主题模型很好地解决了这一问题，相比Unigram Model，它添加了主题层，让文档生成的方式更加自然合理。一篇文章往往由多个主题构成，一个主题可以由多个与之相关的词语来描述。简单来说，当我们现在要创作一篇文当时，我们并不是只有一个大的多面体了，而是有多个“主题多面体”。在计算机主题相关的多面体上，内存、硬盘、复杂度、编程这些词出现的概率会更大；而在音乐相关的主题上。莫扎特、钢琴、二分音符、C大调这样的词出现的概率会更大。</p><p>当然，这时我们生成一篇文档，就需要两种多面体。第一种多面体有一个，它有K个面，每个面代表一个主题；第二种多面体有K个，每一个有W革面，每一个代表一个词。当我们生成一篇有N个词文档时，对每一个词，我们先扔第一种多面体，选定一个主题，然后扔对应的第二种多面体，得到最终的词。</p><p>这样直观的想法最初由Hoffmm在199年给出的PLSA（Probabilistic LatentSemantic Analysis）模型中进行了明确的数学化。PLSA在对文档主题建模时，除了词袋模型的大前提外，主要有三点假设：</p><blockquote><ul><li>一篇文章可以由多个主题构成</li><li>每一个主题由一组词的概率分布来表示</li><li>一篇文章中的每一个具体的词都来自于一个固定的主题</li></ul></blockquote><p>对于PLSA模型而言一篇文档生成的方式如下</p><blockquote><hr><p><strong>PLSA文档生成过程</strong></p><hr><p>假设：有两类多面体分别是</p><ul><li><strong>doc-topic多面体</strong>，每一个doc-topic多面体有K个面，K代表主题个数，每个面对应一个主题编号。</li><li><strong>topic-word多面体</strong>，一共有K个，每个topic-word多面体有V个面，每个面对应一个词。</li></ul><hr><p>过程：</p><p>1.选定文档的词语数量N；</p><p>2.抛掷doc-topic多面体，记录朝上的面对应的主题k；</p><p>3.抛掷k对应的topic-word多面体，记录朝上的面对应的词v</p><p>3.重复步骤2,3, N次，直到所有词都生成。</p><hr></blockquote><img src="/2017/11/07/LDA-From-Zero/6.png" title="Figure 6. PLSA文档生成过程"><p>每篇文档生成的概率和整个语料库生成的概率就可以由多个多项分布的联合分布求出来，具体公式这里略去，我们直接开始讲LDA。</p><h3 id="lda文本主题建模"><a class="header-anchor" href="#lda文本主题建模">¶</a>LDA文本主题建模</h3><p>好了，终于到LDA了。</p><p>细心的你一定会想，unigram model中多面体的概率能用贝叶斯估计，那PLSA是否也能用贝叶斯估计能。当然！如果把PLSA中doc-topic多面体和topic-word多面体每一面朝上的概率加上Drichlet先验分布，就得了LDA模型！简单来说，可以理解为LDA就是把PLSA中的概率估计改造成了一个贝叶斯估计，选用的先验分布是Drichlet分布。（下面这个图我个人觉得把$Dir(\vec{\beta)}$）指向右边的topic-word多面体会更好一些。)</p><img src="/2017/11/07/LDA-From-Zero/7.png" title="Figure 7. LDA文档生成过程"><p>每一篇文章的主题分布服从Dirichlet-Multi共轭结构，这样的结构一共有M个；每一个主题的词分布也符合Dirichlet-Multi共轭结构，这样的结构一共有K个。LDA建模的本质么就是这个M+K个共轭结构和他们之间的关系，LDA模型的求解也就是求这M+K个共轭结构构成的联合分布的期望。</p><p>如果觉得这段话不好理解，也没关系，下面我们开始列公式，我们可以直接从公式中看出LDA模型的本质。</p><p>为了更好的讲解公式，统一用法，减少歧义，我们现在把即将用到的公式中的每一个变量和参数列在下面，供读者查阅。</p><blockquote><ul><li>K:主题个数</li><li>V：语料库中所有的、无重复的词语个数</li><li>M: 语料库中文档数量</li><li>$\vec{\alpha}$：doc-topic多面体的先验分布的超参数，维数为K。</li><li>$\vec{\beta}$：topic-word多面体的先验分布的超参数，维数为V。</li><li>$\vec{\theta}_m$： 第m篇文档的主题分布，即这篇文档的doc-topic多面体投掷时出现每一个topic的概率，维度为K。</li><li>$\vec{\varphi}_k$： 第k个主题的词分布，即这个主题的topic-word多面体投掷时出现每一个word的概率，维度为V</li><li>$N_m$： 语料库中第m篇文档的词数量</li><li>$\vec{z}$：整个语料库的每一个词的主题构成的向量</li><li>$\vec{z}_m$：第m篇文档的每一个词的主题构成的向量，维度为N_m</li><li>$z_{m,n}$：第m篇文档的第n个词的主题</li><li>$w_{m,n}$：第m篇文档的第n个词的具体词语</li><li>$\vec{n}_m=(n_m ^1,\cdots,n_m ^K)$：$n_m ^K$表示在语料库中，第m篇文档中属于主题k的词语的个数。</li><li>$\vec{w}_k=(v_k ^1,\cdots,v_k ^V)$：第k个主题下每个词的个数。$v_k ^t$表示在语料库中，第k个主题包含的某一个具体的词语V的个数。</li></ul><blockquote><ul><li>$\vec{w}=(\vec{w}_1,\cdots,\vec{w}_k)$：所有主题下的每个词的个数。</li></ul></blockquote></blockquote><p>先上概率图。</p><img src="/2017/11/07/LDA-From-Zero/8.png" title="Figure 8. LDA概率图"><p>由概率图和文档生成过程我们可以将LDA中每一个词生成的过程分解为两个大的物理过程，分别是</p><p>1.生成每一个词对应的主题，$\vec{\alpha} \rightarrow \vec{\theta_m} \rightarrow z_{m,n}$。在这个过程中，首先根据超参数$\vec{\alpha}$采样得到第m篇文档的主题分布$\vec{\theta_m}$，相当于的到了一个doc-topic骰子；然后投掷这个骰子，得到第n个词对应的topic编号$z_{m,n}$。这里M篇文档对应M个Dirichlet-Multi共轭结构。</p><p>2.根据主题生成具体的词，$\vec{\beta} \rightarrow \vec{\varphi_k} \rightarrow w_{m,n}$。在这个过程中，根据超参数$\vec{\beta}$采样得到第k个主题的词分布$\vec{\varphi_k}$，相当于的到了一个编号是$k=z_{m,n}$的topic-doc骰子；然后投掷这个骰子，得到第n个词对应的词$w_{m,n}$。这里K个主题对应K个Dirichlet-Multi共轭结构。</p><p>需要注意的是，在这两个过程中，每一个Dirichlet-Multi共轭结构都是独立的。这也就意味着，不仅可以一次就把$w_{m,n}$具体的词语生成出来，也可以先生成所有的$z_{m,n}$,再生成所有的$w_{m,n}$，这两个过程是完全等价的。根据这一特性，我们可以把LDA的文档生成过程定义为如下形式。</p><blockquote><hr><p><strong>LDA文档生成过程</strong></p><hr><p>假设：有两类多面体分别是</p><ul><li><strong>doc-topic多面体</strong>，一共有M个，每个doc-topic多面体有K个面，每个面对应一个主题编号。这个多面体每个面朝上的概率分布服从超参数为$\vec{\alpha}$的Dirichlet分布。</li><li><strong>topic-word多面体</strong>，一共有K个，每个topic-word多面体有V个面，每个面对应一个词。这个多面体每个面朝上的概率分布服从超参数为$\vec{\beta}$的Dirichlet分布。</li></ul><hr><p>过程：</p><ol><li><p>选定每一篇文档的词语数量$N_m$；</p></li><li><p>对第m篇文档，根据超参数$\vec{\alpha}$采样的到对应的doc-topic多面体。</p></li><li><p>对第m篇文档的第n个词，投掷doc-topic多面体，得到该词的主题$z_{m,n}$。</p></li><li><p>重复步骤3共$N_m$次，得到第m篇文档的每一个词的主题编号$\vec{z_m}$。</p></li><li><p>重复步骤2-4共M次，得到每一篇文档的的每一个词的主题编号。（过程1结束）</p></li><li><p>对第k个主题，根据超参数$\vec{\beta}$采样的到对应的topic-word多面体。</p></li><li><p>对第m篇文档的第n个词，根据主题编号选择对应的topic-word多面体，投掷该多面体，得到对应的词$w_{m,n}$。</p></li><li><p>重复步骤7，直到所有词都得到具体的词。（过程2结束）</p></li></ol><hr></blockquote><p>从这一过程就可以很清晰的看出，LDA模型求解的过程实际上就是求这M+K个Dirichlet-Multi共轭结构下文档生成概率的联合分布了。那么Dirichlet-Multi共轭结构如何如何求解呢？</p><p>在过程1中，超参数为$\vec{\alpha}$的第m篇篇文档下每一个词的主题编号生成概率，我们可以由<a href="#jump">公式3.1</a>得到</p><p>$$p(\vec{z}_m|\vec{\alpha}) =\frac{\Delta(\vec{n}_m+\vec{\alpha})}{\Delta (\vec{\alpha})}$$</p><p>同时，我们得到隐含参数$\vec{\theta}_m$的后验分布恰好是</p><p>$$Dir(\vec{\theta}_m|\vec{n}_m+\vec{\alpha})$$</p><p>整个语料库下的所有词的主题编号的生成概率就是每一篇文档的联合分布</p><p>$$p(\vec{z}|\vec{\alpha}) =\prod_{m} ^{M} \frac{\Delta(\vec{n}_m+\vec{\alpha})}{\Delta (\vec{\alpha})}$$</p><p>同样的道理，我们可以得到过程二中，每个主题下的词生成概率为$$p(\vec{w}_k|\vec{\beta}) =\frac{\Delta(\vec{v}_k+\vec{\beta})}{\Delta (\vec{\beta})}$$</p><p>同时，我们得到隐含参数$\vec{\varphi}_k$的后验分布恰好是</p><p>$$Dir(\vec{\varphi}_k|\vec{v}_k+\vec{\beta})$$</p><p>对于整个语料库的所有词的生成过程来说，每一个词具体的主题编号也是最终决定词语的先决条件，但是无论选择哪一个主题编号，并不会改变着K个主题下的词分布(<strong>这一点很重要，可以将过程1和过程2分开来看</strong>)，所以有$$p(\vec{w}|\vec{z},\vec{\beta})=p(\vec{w}|\vec{\beta})=\prod_{k} ^{V} \frac {\Delta(\vec{v}_k+\vec{\beta})}{\Delta (\vec{\beta})}$$</p><p>结合上面的式子，我们最终可以得到，两个过程加在一起，LDA模型的语料库的生成概率为</p><p>$$p(\vec{w},\vec{z}|\vec{\alpha},\vec{\beta})=p(\vec{w},\vec{z}|\vec{\beta})\cdotp(\vec{z}|\vec{\alpha})=\prod_{k} ^{V} \frac {\Delta(\vec{v}<em>k+\vec{\beta})}{\Delta (\vec{\beta})} \cdot \prod</em>{m} ^{M} \frac{\Delta(\vec{n}_m+\vec{\alpha})}{\Delta (\vec{\alpha})}$$</p><p>到这里，整个LDA模型的文档生成过程就介绍完了。LDA模型在实际应用中，就是要估计隐含变量$\vec{\theta}_m$和$\vec{\varphi}_k$的值，也就是要求出doc-topic多面体和topic-word多面体的每个面朝上的概率大小，由此我们可以去分析文档的主题分布和主题的词分布。那么，这两个概率大小究竟该怎么求呢，我们继续往下探讨。</p><h2 id="lda模型的求解"><a class="header-anchor" href="#lda模型的求解">¶</a>LDA模型的求解</h2><p>OK，模型的基本构成我们了解完了，接下来就要看模型的求解了。对于LDA模型来说，最基本的目标有两个：</p><ul><li>Training。根据一个已有的语料库，我们需要这个语料库用LDA方式生成过程中的参数$\vec{\theta}_m$和$\vec{\varphi}_k$，由此可以把生成这个语料库的LDA模型完全确定下来。</li><li>Inference。对于一篇不在这个语料库中的文档，我们能够根据已经确定的LDA模型来计算这篇文档的主题分布。</li></ul><p>当然，要先进行Training，然后才能进行Inference。Training的过程实际上就是对于上一节中我们得到的$p(\vec{w},\vec{z}|\vec{\alpha},\vec{\beta})$分布求期望，由于$p(\vec{w})$是我们观测到的已知变量，实际上就是要求分布$p(\vec{z}|\vec{w}$的期望，也就是每一个词对应的最可能的主题编号。</p><p>如何求这个期望是一个难题。早期的PLSA和LDA的求解都是基于最大期望（Expectation Maximization）算法做的，要对$p(\vec{w},\vec{z}|\vec{\alpha},\vec{\beta})$直接积分求解，这一过程的时间复杂度和空间复杂度都比较高，好处是可以得到精确解。在工程上和较新的一些关于主题模型的研究中，则往往采用Gibbs Sampling的方式求解。这种方式得到的是近似解，但是求解速度快，工程实现也比较简单。下面，我们就讲解如何通过Gibbs Sampling的方式求解LDA模型。</p><h3 id="采样"><a class="header-anchor" href="#采样">¶</a>采样</h3><p>Sampling，就是采样。理解Gibbs Sampling的基础，是要明白采样的目的和原理。**所谓采样，是指获得符合某一概率分布的一些列样本，根据大数定律，当这些样本足够多时，样本的期望会逐渐趋近于概率分布的期望。**也就是说，采样的目的就是通过样本期望去估计分布的期望。</p><p>举个例子，有一个每个面大小不相同的骰子，现在想知道每个面朝上的概率大小是多少，怎么做？最先想到的最简单的方法，就是采样。不断投掷这个骰子，记录下每次投掷朝上的面，然后计算这些面出现的次数占总次数比，就可以大致的估计每个面朝上的概率。</p><p>对LDA求解也是一样，直接求解期望很麻烦，那么我们不妨通过采样得到一系列符合联合分布的样本，计算样本的期望就可以估计分布的期望。如何得到符合联合分布的样本呢，这就要引入另一组数学知识——马氏链和蒙特卡洛方法。</p><h3 id="马氏链"><a class="header-anchor" href="#马氏链">¶</a>马氏链</h3><p>马氏链（Markov Chain）的数学定义很简单，$$P(X_{t+1}=x|X_t,X_{t-1},\cdots)=P(X_{t+1}=x|X_t\cdots)$$即在一个状态转移链中，如果下一时刻状态转移的概率只和当前状态有关，那么这个转移链就是一个马氏链。</p><p>用一个例子来说明马氏链和马氏链产生的过程。（这个例子我实在《LDA数学八卦》里看到的，不过网上还想用烂了）社会学家经常把人按其经济状况分成3类：下层(lower-class)、中层(middle-class)、上层(upper-class)，我们用1,2,3 分别代表这三个阶层。社会学家们发现决定一个人的收入阶层的最重要的因素就是其父母的收入阶层。如果一个人的收入属于下层类别，那么他的孩子属于下层收入的概率是 0.65, 属于中层收入的概率是 0.28, 属于上层收入的概率是 0.07。事实上，从父代到子代，收入阶层的变化的转移概率如下。</p><img src="/2017/11/07/LDA-From-Zero/10.png" title="Figure 9. 转移概率表"><img src="/2017/11/07/LDA-From-Zero/9.png" title="Figure 10. 转移概率图"><p>我们用矩阵的形式表示，状态转移矩阵记为</p><p>$$P=\begin{bmatrix}0.65 &amp; 0.28 &amp; 0.07\\0.15 &amp; 0.67 &amp; 0.18\\0.12 &amp; 0.36 &amp; 0.52\end{bmatrix}$$</p><p>我们假设当前这一代下、中、上层人口的比例是$\pi_0=[0.21,0.68,0.11]$，那么下一代的比例分布式$\pi_0=\pi_1\cdot P$，以此类推，可以计算前n代人中下、中、上层人口比例为</p><img src="/2017/11/07/LDA-From-Zero/11.png"><p>从第7代开始，这个比例就不变了。这是偶然吗？换一个初始概率$\pi_0=[0.75, 0.15, 0.1]]$再试一次，前n代人中下、中、上层人口比例为</p><img src="/2017/11/07/LDA-From-Zero/12.png"><p>第9代人的时候, 分布又收敛了。奇特的是，两次给定不同的初始概率分布，最终都收敛到概率分布$\pi=[0.286,0.489,0.225]$，也就是说收敛的行为和初始概率分布 $\pi_0$ 无关。这说明这个收敛行为主要是由概率转移矩阵P决定的。</p><blockquote><p>[1]Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation[J]. Journal of machine Learning research, 2003, 3(Jan): 993-1022.]</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;去年初学LDA，看完了Rickjin老师的《LDA数学八卦》，觉得不是很懂，查阅了很多资料之后才对LDA有了更深入地认识。一直想加入自己的理解后更简单的讲述这个模型，今日补上。&lt;/p&gt;
&lt;p&gt;&lt;font color=&quot;red&quot;&gt;&lt;strong&gt;注：文章的行文思路与大多数公式参考了Rickjin《LDA数学八卦》。有兴趣更深入了解的同学可以在看完本篇后继续阅读。&lt;/strong&gt;&lt;/font&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;LDA(Latent Dirichlet allocation)是&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;主题模型&lt;/a&gt;的一种，最早是由Blei D M&lt;sup&gt;[1]&lt;/sup&gt;等人在2003年提出的，常被用于文档的主题识别。时至今日，在大大小小的会议上仍能看到不少对LDA（或其衍生、或类似的主题模型）提出改进的文章。其自然的思路，优雅的求解和拓展性强的模型结构，实在是非常适合作为对无监督学习一个入门模型来学习和改进。&lt;/p&gt;
&lt;p&gt;本文主要介绍LDA背后的相关数学知识和模型的构建与求解思路，力求只有少量统计知识的初学者也能看懂。文中略去了大量的公式推导细节，如果希望深入的了解，请根据需要查找相关的资料。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习模型" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
      <category term="Statistics" scheme="http://yoursite.com/tags/Statistics/"/>
    
  </entry>
  
  <entry>
    <title>今天开始写博客</title>
    <link href="http://yoursite.com/2017/11/07/%E4%BB%8A%E5%A4%A9%E5%BC%80%E5%A7%8B%E5%86%99%E5%8D%9A%E5%AE%A2/"/>
    <id>http://yoursite.com/2017/11/07/今天开始写博客/</id>
    <published>2017-11-07T09:40:00.000Z</published>
    <updated>2017-11-10T07:11:33.825Z</updated>
    
    <content type="html"><![CDATA[<h1>忙了两天，总算把个人博客搭起来了</h1><h2 id="总之今天开始写博客了"><a class="header-anchor" href="#总之今天开始写博客了">¶</a>总之今天开始写博客了</h2><p>这里看起来可能很奇怪，恩，因为就是一个MarkDown的写法的练习。</p><p><em>说实话我才敢刚开始写，还不太习惯</em></p><p><strong>万事开头难。</strong></p><a id="more"></a><h3 id="从今天开始写博客"><a class="header-anchor" href="#从今天开始写博客">¶</a>从今天开始写博客</h3><blockquote><p>预期每周一篇</p><p>包括但不限于以下题材</p><ul><li><p>技术相关，机器学习编程啥的，某一个具体部分的详细介绍</p></li><li><p>读书笔记</p></li><li><p>文献阅读笔记</p></li><li><p>个人思考</p></li></ul><p>一周至少300字吧，不算多，写个计划可能都不止300字了（笑）。</p></blockquote><img src="/2017/11/07/今天开始写博客/avatar.jpg" title="随手一个图片，但愿能正常显示">]]></content>
    
    <summary type="html">
    
      &lt;h1&gt;忙了两天，总算把个人博客搭起来了&lt;/h1&gt;
&lt;h2 id=&quot;总之今天开始写博客了&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#总之今天开始写博客了&quot;&gt;¶&lt;/a&gt;总之今天开始写博客了&lt;/h2&gt;
&lt;p&gt;这里看起来可能很奇怪，恩，因为就是一个MarkDown的写法的练习。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;说实话我才敢刚开始写，还不太习惯&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;万事开头难。&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Whatever" scheme="http://yoursite.com/categories/Whatever/"/>
    
      <category term="日记" scheme="http://yoursite.com/categories/Whatever/%E6%97%A5%E8%AE%B0/"/>
    
    
      <category term="Whatever" scheme="http://yoursite.com/tags/Whatever/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2017/11/06/hello-world/"/>
    <id>http://yoursite.com/2017/11/06/hello-world/</id>
    <published>2017-11-06T14:33:16.000Z</published>
    <updated>2017-11-09T13:10:21.969Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p><a id="more"></a><h2 id="quick-start"><a class="header-anchor" href="#quick-start">¶</a>Quick Start</h2><h3 id="create-a-new-post"><a class="header-anchor" href="#create-a-new-post">¶</a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p><h3 id="run-server"><a class="header-anchor" href="#run-server">¶</a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p><h3 id="generate-static-files"><a class="header-anchor" href="#generate-static-files">¶</a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p><h3 id="deploy-to-remote-sites"><a class="header-anchor" href="#deploy-to-remote-sites">¶</a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="Whatever" scheme="http://yoursite.com/categories/Whatever/"/>
    
    
  </entry>
  
</feed>
